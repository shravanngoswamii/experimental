@string{aies1 = {1st AAAI/ACM Conference on Artificial Intelligence, Ethics and Society}}
@string{aies4 = {4th AAAI/ACM Conference on Artificial Intelligence, Ethics and Society}}
@string{aistats8 = {8th International Conference on Artificial Intelligence and Statistics}}
@string{aistats9 = {9th International Conference on Artificial Intelligence and Statistics}}
@string{aistats10 = {10th International Conference on Artificial Intelligence and Statistics}}
@string{aistats11 = {11th International Conference on Artificial Intelligence and Statistics}}
@string{aistats12 = {12th International Conference on Artificial Intelligence and Statistics}}
@string{aistats13 = {13th International Conference on Artificial Intelligence and Statistics}}
@string{aistats14 = {14th International Conference on Artificial Intelligence and Statistics}}
@string{aistats15 = {15th International Conference on Artificial Intelligence and Statistics}}
@string{aistats16 = {16th International Conference on Artificial Intelligence and Statistics}}
@string{aistats17 = {17th International Conference on Artificial Intelligence and Statistics}}
@string{aistats18 = {18th International Conference on Artificial Intelligence and Statistics}}
@string{aistats19 = {19th International Conference on Artificial Intelligence and Statistics}}
@string{aistats20 = {20th International Conference on Artificial Intelligence and Statistics}}
@string{aistats21 = {21st International Conference on Artificial Intelligence and Statistics}}
@string{aistats22 = {22nd International Conference on Artificial Intelligence and Statistics}}
@string{aistats23 = {23rd International Conference on Artificial Intelligence and Statistics}}
@string{aistats24 = {24th International Conference on Artificial Intelligence and Statistics}}
@string{aistats25 = {25th International Conference on Artificial Intelligence and Statistics}}
@string{icml20 = {20th International Conference on Machine Learning}}
@string{icml21 = {21st International Conference on Machine Learning}}
@string{icml22 = {22nd International Conference on Machine Learning}}
@string{icml23 = {23rd International Conference on Machine Learning}}
@string{icml24 = {24th International Conference on Machine Learning}}
@string{icml25 = {25th International Conference on Machine Learning}}
@string{icml26 = {26th International Conference on Machine Learning}}
@string{icml27 = {27th International Conference on Machine Learning}}
@string{icml28 = {28th International Conference on Machine Learning}}
@string{icml29 = {29th International Conference on Machine Learning}}
@string{icml30 = {30th International Conference on Machine Learning}}
@string{icml31 = {31st International Conference on Machine Learning}}
@string{icml32 = {32nd International Conference on Machine Learning}}
@string{icml33 = {33rd International Conference on Machine Learning}}
@string{icml34 = {34th International Conference on Machine Learning}}
@string{icml35 = {35th International Conference on Machine Learning}}
@string{icml36 = {36th International Conference on Machine Learning}}
@string{icml37 = {37th International Conference on Machine Learning}}
@string{icml38 = {38th International Conference on Machine Learning}}
@string{icml39 = {39th International Conference on Machine Learning}}
@string{icml2022 = {39th International Conference on Machine Learning}}
@string{nips7  = {Advances in Neural Information Processing Systems 7}}
@string{nips8  = {Advances in Neural Information Processing Systems 8}}
@string{nips9  = {Advances in Neural Information Processing Systems 9}}
@string{nips10 = {Advances in Neural Information Processing Systems 10}}
@string{nips11 = {Advances in Neural Information Processing Systems 11}}
@string{nips12 = {Advances in Neural Information Processing Systems 12}}
@string{nips13 = {Advances in Neural Information Processing Systems 13}}
@string{nips14 = {Advances in Neural Information Processing Systems 14}}
@string{nips15 = {Advances in Neural Information Processing Systems 15}}
@string{nips16 = {Advances in Neural Information Processing Systems 16}}
@string{nips17 = {Advances in Neural Information Processing Systems 17}}
@string{nips18 = {Advances in Neural Information Processing Systems 18}}
@string{nips19 = {Advances in Neural Information Processing Systems 19}}
@string{nips20 = {Advances in Neural Information Processing Systems 20}}
@string{nips21 = {Advances in Neural Information Processing Systems 21}}
@string{nips22 = {Advances in Neural Information Processing Systems 22}}
@string{nips23 = {Advances in Neural Information Processing Systems 23}}
@string{nips24 = {Advances in Neural Information Processing Systems 24}}
@string{nips25 = {Advances in Neural Information Processing Systems 25}}
@string{nips26 = {Advances in Neural Information Processing Systems 26}}
@string{nips27 = {Advances in Neural Information Processing Systems 27}}
@string{nips28 = {Advances in Neural Information Processing Systems 28}}
@string{nips29 = {Advances in Neural Information Processing Systems 29}}
@string{nips30 = {Advances in Neural Information Processing Systems 30}}
@string{nips31 = {Advances in Neural Information Processing Systems 31}}
@string{nips32 = {Advances in Neural Information Processing Systems 32}}
@string{nips33 = {Advances in Neural Information Processing Systems 33}}
@string{nips34 = {Advances in Neural Information Processing Systems 34}}
@string{nips35 = {Advances in Neural Information Processing Systems 35}}
@string{nips36 = {Advances in Neural Information Processing Systems 36}}
@string{cogsci32 = {The Proceedings of the 32nd Annual Meeting of the Cognitive Science Society}}
@String{uai22 = {22nd Conference on Uncertainty in Artificial Intelligence}}
@String{uai27 = {27th Conference on Uncertainty in Artificial Intelligence}}
@String{uai28 = {28th Conference on Uncertainty in Artificial Intelligence}}
@String{uai29 = {29th Conference on Uncertainty in Artificial Intelligence}}
@String{uai30 = {30th Conference on Uncertainty in Artificial Intelligence}}
@String{uai31 = {31st Conference on Uncertainty in Artificial Intelligence}}
@String{uai32 = {32nd Conference on Uncertainty in Artificial Intelligence}}
@String{uai33 = {33st Conference on Uncertainty in Artificial Intelligence}}
@String{uai35 = {35th Conference on Uncertainty in Artificial Intelligence}}
@String{uai37 = {37th Conference on Uncertainty in Artificial Intelligence}}
@String{iclr04 = {4th International Conference on Learning Representations}}
@String{iclr05 = {5th International Conference on Learning Representations}}
@String{iclr06 = {6th International Conference on Learning Representations}}
@String{iclr07 = {7th International Conference on Learning Representations}}
@String{iclr08 = {8th International Conference on Learning Representations}}
@String{iclr09 = {9th International Conference on Learning Representations}}
@String{iclr10 = {10th International Conference on Learning Representations}}
@String{icra2017 = {IEEE International Conference on Robotics and Automation}}
@string{www2018 = {The Web Conference (WWW)}}
@string{aaai31 = {31st AAAI Conference on Artificial Intelligence}}
@string{aaai32 = {32nd AAAI Conference on Artificial Intelligence}}
@string{aaai33 = {33rd AAAI Conference on Artificial Intelligence}}
@string{aabi2 = {2nd Symposium on Advances in Approximate Bayesian Inference}}
@string{aabi3 = {3rd Symposium on Advances in Approximate Bayesian Inference}}
@string{acm    = {Association for Computing Machinery}}
@string{cup    = {Cambridge University Press}}
@string{curran = {Curran Associates, Inc.}}
@string{AAAI   = {AAAI Press}}
@string{mit    = {The {MIT} Press}}
@string{oup    = {Oxford University Press}}
@string{WILEY  = {John Wiley \& Sons}}
@string{rss9   = {9th International Conference on Robotics: Science \& Systems}}
@string{IECY   = {IEEE Transactions on System Science and Cybernetics}}
@string{IENN   = {IEEE Transactions on Neural Networks}}
@string{IETAC  = {IEEE Transactions on Automatic Control}}
@string{PAMI   = {IEEE Transactions on Pattern Analysis and Machine Intelligence}}
@string{ITRIT  = {IEEE Transactions on Information Theory}}
@string{tcbb   = {IEEE/ACM Transactions on Computational Biology and Bioinformatics}}
@string{jasa   = {Journal of the Americal Statistical Association}}
@string{jmlr   = {Journal of Machine Learning Research}}
@string{pmlr   = {Proceedings of Machine Learning Research}}
@string{tmlr   = {Transactions on Machine Learning Research}}
@string{jrssB  = {Journal of the Royal Statistical Society, Series B}}
@string{jcst   = {Journal of Computer Science and Technology}}
@string{nc     = {Neural Computation}}
@string{PCPS   = {Proceedings of the Cambridge Philosophical Society}}
@string{tams   = {The Annals of Mathematical Statistics}}
@string{lncs   = {Lecture Notes in Computer Science (LNCS)}}
@string{icra   = {IEEE International Conference on Robotics and Automation}}
@string{ijcai  = {International Joint Conference on Artificial Intelligence}}
@string{jcgs   = {Journal of Computational and Graphical Statistics}}
@string{staco  = {Statistics and Computing}}
@string{ejs    = {Electronic Journal of Statistics}}

@inproceedings{AbbHelGrietal11,
  cat =		 {clust},
  author =	 {Joshua Abbott and Katherine A. Heller and Zoubin
                  Ghahramani and Thomas L. Griffiths},
  title =	 {Testing a {Bayesian} Measure of Representativeness
                  Using a Large Image Database},
  booktitle =	 nips24,
  year =	 2011,
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {How do people determine which elements of a set are
                  most representative of that set? We extend an
                  existing Bayesian measure of representativeness,
                  which indicates the representativeness of a sample
                  from a distribution, to deﬁne a measure of the
                  representativeness of an item to a set. We show that
                  this measure is formally related to a machine
                  learning method known as Bayesian Sets. Building on
                  this connection, we derive an analytic expression
                  for the representativeness of objects described by a
                  sparse vector of binary features. We then apply this
                  measure to a large database of images, using it to
                  determine which images are the most representative
                  members of different sets. Comparing the resulting
                  predictions to human judgments of representativeness
                  provides a test of this measure with naturalistic
                  stimuli, and illustrates how databases that are more
                  commonly used in computer vision and machine
                  learning can be used to evaluate psychological
                  theories.}
}

@inproceedings{AdaGha09,
  cat =		 {ssl np gp},
  booktitle =	 icml26,
  title =	 {Archipelago: nonparametric {B}ayesian
                  semi-supervised learning},
  author =	 {R.~Adams and Zoubin Ghahramani},
  year =	 2009,
  address =	 {Montr\'{e}al, QC, Canada},
  month =	 {June},
  publisher =	 {Omnipress},
  pages =	 {1--8},
  editor =	 {L\'{e}on Bottou and Michael Littman},
  url =		 {.},
  abstract =	 {Semi-supervised learning (SSL), is classification
                  where additional unlabeled data can be used to
                  improve accuracy. Generative approaches are
                  appealing in this situation, as a model of the
                  data's probability density can assist in identifying
                  clusters. Nonparametric Bayesian methods, while
                  ideal in theory due to their principled motivations,
                  have been difficult to apply to SSL in practice. We
                  present a nonparametric Bayesian method that uses
                  Gaussian processes for the generative model,
                  avoiding many of the problems associated with
                  Dirichlet process mixture models. Our model is fully
                  generative and we take advantage of recent advances
                  in Markov chain Monte Carlo algorithms to provide a
                  practical inference method. Our method compares
                  favorably to competing approaches on synthetic and
                  real-world multi-class data.},
  annote =	 {This paper was awarded Honourable Mention for Best
                  Paper at ICML 2009.}
}

@inproceedings{AdaGhaJor10,
  cat =		 {np clust},
  author =	 {R. P. Adams and Zoubin Ghahramani and Michael
                  I. Jordan},
  year =	 2010,
  title =	 {Tree-Structured Stick Breaking for Hierarchical
                  Data},
  booktitle =	 nips23,
  publisher =	 mit,
  url =		 {.},
  abstract =	 {Many data are naturally modeled by an unobserved
                  hierarchical structure. In this paper we propose a
                  flexible nonparametric prior over unknown data
                  hierarchies. The approach uses nested stick-breaking
                  processes to allow for trees of unbounded width and
                  depth, where data can live at any node and are
                  infinitely exchangeable. One can view our model as
                  providing infinite mixtures where the components
                  have a dependency structure corresponding to an
                  evolutionary diffusion down a tree. By using a
                  stick-breaking approach, we can apply Markov chain
                  Monte Carlo methods based on slice sampling to
                  perform Bayesian inference and simulate from the
                  posterior distribution on trees. We apply our method
                  to hierarchical clustering of images and topic
                  modeling of text data.}
}

@inproceedings{AdaStoWil00a,
  author =	 {Adams, Nicholas J. and Storkey, Amos J. and
                  Williams, Christopher K. I. and Ghahramani, Zoubin},
  booktitle =	 {International Conference on Pattern Recognition},
  cat =		 {gm},
  pages =	 {151-154},
  title =	 {{MFDT}s: Mean Field Dynamic Trees},
  url =		 {.},
  volume =	 3,
  year =	 2000,
  abstract =	 {Tree structured belief networks are attractive for
                  image segmentation tasks. However, networks with
                  fixed architectures are not very suitable as they
                  lead to blocky artefacts, and led to the
                  introduction of dynamic trees (DTs). The Dynamic
                  trees architecture provide a prior distribution over
                  tree structures, and simulated annealing (SA) was
                  used to search for structures with high posterior
                  probability. In this paper we introduce a mean field
                  approach to inference in DTs. We find that the mean
                  field method captures the posterior better than just
                  using the maximum a posteriori solution found by SA}
}

@inproceedings{AdaWalGha10,
  cat =		 {np gm deep},
  author =	 {R.~P.~Adams and H.~Wallach and Zoubin Ghahramani},
  year =	 2010,
  month =	 {May},
  editor =	 {Yee Whye Teh and Mike Titterington},
  title =	 {Learning the Structure of Deep Sparse Graphical
                  Models},
  booktitle =	 aistats13,
  address =	 {Chia Laguna, Sardinia, Italy},
  pages =	 {1--8},
  url =		 {.},
  abstract =	 {Deep belief networks are a powerful way to model
                  complex probability distributions. However, it is
                  difficult to learn the structure of a belief
                  network, particularly one with hidden units. The
                  Indian buffet process has been used as a
                  nonparametric Bayesian prior on the structure of a
                  directed belief network with a single infinitely
                  wide hidden layer. Here, we introduce the cascading
                  Indian buffet process (CIBP), which provides a prior
                  on the structure of a layered, directed belief
                  network that is unbounded in both depth and width,
                  yet allows tractable inference. We use the CIBP
                  prior with the nonlinear Gaussian belief network
                  framework to allow each unit to vary its behavior
                  between discrete and continuous representations. We
                  use Markov chain Monte Carlo for inference in this
                  model and explore the structures learned on image
                  data.},
  annote =	 {Winner of the Best Paper Award}
}

@inproceedings{AdeGhaWell18,
  cat =		 {deep interpretability},
  title =	 {Discovering interpretable representations for both
                  deep generative and discriminative models},
  author =	 {Tameem Adel and Zoubin Ghahramani and Adrian Weller},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm Sweden},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/ICML18-Discovering.pdf},
  abstract =	 {Interpretability of representations in both deep
                  generative and discriminative models is highly
                  desirable. Current methods jointly optimize an
                  objective combining accuracy and interpretability.
                  However, this may reduce accuracy, and is not
                  applicable to already trained models. We propose two
                  interpretability frameworks. First, we provide an
                  interpretable lens for an existing model. We use a
                  generative model which takes as input the
                  representation in an existing (generative or
                  discriminative) model, weakly supervised by limited
                  side information. Applying a flexible and invertible
                  transformation to the input leads to an
                  interpretable representation with no loss in
                  accuracy. We extend the approach using an active
                  learning strategy to choose the most useful side
                  information to obtain, allowing a human to guide
                  what ``interpretable" means. Our second framework
                  relies on joint optimization for a representation
                  which is both maximally informative about the side
                  information and maximally compressive about the
                  non-interpretable data factors.  This leads to a
                  novel perspective on the relationship between
                  compression and regularization.  We also propose a
                  new interpretability evaluation metric based on our
                  framework. Empirically, we achieve state-of-the-art
                  results on three datasets using the two proposed
                  algorithms.}
}

@inproceedings{AdeValGhaWel19,
  cat =		 {deep fairness},
  title =	 {One-network Adversarial Fairness},
  author =	 {Tameem Adel and Isabel Valera and Zoubin Ghahramani
                  and Adrian Weller},
  booktitle =	 aaai33,
  year =	 2019,
  month =	 {January},
  address =	 {Hawaii},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/AAAI2019_OneNetworkAdversarialFairness.pdf},
  abstract =	 {There is currently a great expansion of the impact
                  of machine learning algorithms on our lives,
                  prompting the need for objectives other than pure
                  performance, including fairness. Fairness here means
                  that the outcome of an automated decision-making
                  system should not discriminate between subgroups
                  characterized by sensitive attributes such as gender
                  or race. Given any existing differentiable
                  classifier, we make only slight adjustments to the
                  architecture including adding a new hidden layer, in
                  order to enable the concurrent adversarial
                  optimization for fairness and accuracy. Our
                  framework provides one way to quantify the tradeoff
                  between fairness and accuracy, while also leading to
                  strong empirical performance.  }
}

@inproceedings{AdeWel19,
  cat =		 {rl gm approx},
  title =	 {{TibGM}: A Transferable and Information-Based
                  Graphical Model Approach for Reinforcement Learning},
  author =	 {Tameem Adel and Adrian Weller},
  booktitle =	 icml36,
  year =	 2019,
  month =	 {June},
  address =	 {Long Beach},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/ICML2019-TibGM.pdf},
  abstract =	 {One of the challenges to reinforcement learning (RL)
                  is scalable transferability among complex
                  tasks. Incorporating a graphical model (GM), along
                  with the rich family of related methods, as a basis
                  for RL frameworks provides potential to address
                  issues such as transferability, generalisation and
                  exploration. Here we propose a flexible GM-based RL
                  framework which leverages efficient inference
                  procedures to enhance generalisation and transfer
                  power. In our proposed transferable and
                  information-based graphical model framework ‘TibGM’,
                  we show the equivalence between our mutual
                  information-based objective in the GM, and an RL
                  consolidated objective consisting of a standard
                  reward maximisation target and a
                  generalisation/transfer objective. In settings where
                  there is a sparse or deceptive reward signal, our
                  TibGM framework is flexible enough to incorporate
                  exploration bonuses depicting intrinsic rewards. We
                  empirically verify improved performance and
                  exploration power.}
}

@inproceedings{AhnCheShiWel18,
  cat =		 {gm approx},
  title =	 {Gauged Mini-Bucket Elimination for Approximate
                  Inference},
  booktitle =	 aistats21,
  year =	 2018,
  month =	 {April},
  address =	 {Playa Blanca, Lanzarote, Canary Islands},
  author =	 {Sungsoo Ahn and Michael Chertkov and Jinwoo Shin and
                  Adrian Weller},
  abstract =	 {Computing the partition function Z of a discrete
                  graphical model is a fundamental inference
                  challenge. Since this is computationally
                  intractable, variational approximations are often
                  used in practice. Recently, so-called gauge
                  transformations were used to improve variational
                  lower bounds on Z. In this paper, we propose a new
                  gauge-variational approach, termed WMBE-G, which
                  combines gauge transformations with the weighted
                  mini-bucket elimination (WMBE) method.  WMBE-G can
                  provide both upper and lower bounds on Z, and is
                  easier to optimize than the prior gauge-variational
                  algorithm. We show that WMBE-G strictly improves the
                  earlier WMBE approximation for symmetric models
                  including Ising models with no magnetic field. Our
                  experimental results demonstrate the effectiveness
                  of WMBE-G even for generic, nonsymmetric models.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/Gauge_for_Holder_Inference.pdf}
}

@inproceedings{AhnCheWel18,
  cat =		 {gm approx},
  title =	 {Bucket renormalization for approximate inference},
  booktitle =	 icml35,
  year =	 2018,
  author =	 {Sungsoo Ahn and Michael Chertkov and Adrian Weller
                  and Jinwoo Shin},
  abstract =	 {Probabilistic graphical models are a key tool in
                  machine learning applications. Computing the
                  partition function, i.e., normalizing constant, is a
                  fundamental task of statistical inference but it is
                  generally computationally intractable, leading to
                  extensive study of approximation methods. Iterative
                  variational methods are a popular and successful
                  family of approaches. However, even state of the art
                  variational methods can return poor results or fail
                  to converge on difficult instances. In this paper,
                  we instead consider computing the partition function
                  via sequential summation over variables.  We develop
                  robust approximate algorithms by combining ideas
                  from mini-bucket elimination with tensor network and
                  renormalization group methods from statistical
                  physics. The resulting “convergence-free” methods
                  show good empirical performance on both synthetic
                  and real-world benchmark models, even for difficult
                  instances.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/ICML18-BucketRenormalization.pdf}
}

@inproceedings{AitYanObe21,
  title =	 {Deep kernel processes},
  author =	 {Laurence Aitchison and Adam X. Yang and Sebastian
                  W. Ober},
  booktitle =	 icml38,
  abstract =	 {We define deep kernel processes in which positive
                  definite Gram matrices are progressively transformed
                  by nonlinear kernel functions and by sampling from
                  (inverse) Wishart distributions. Remarkably, we find
                  that deep Gaussian processes (DGPs), Bayesian neural
                  networks (BNNs), infinite BNNs, and infinite BNNs
                  with bottlenecks can all be written as deep kernel
                  processes. For DGPs the equivalence arises because
                  the Gram matrix formed by the inner product of
                  features is Wishart distributed, and as we show,
                  standard isotropic kernels can be written entirely
                  in terms of this Gram matrix — we do not need
                  knowledge of the underlying features. We define a
                  tractable deep kernel process, the deep inverse
                  Wishart process, and give a doubly-stochastic
                  inducing-point variational inference scheme that
                  operates on the Gram matrices, not on the features,
                  as in DGPs. We show that the deep inverse Wishart
                  process gives superior performance to DGPs and
                  infinite BNNs on fully-connected baselines.},
  year =	 2021,
  cat =		 {gp approx deep},
  url =
                  {http://proceedings.mlr.press/v139/aitchison21a/aitchison21a.pdf}
}

@article{AllWenMarEtal22,
  cat =		 {deep},
  title =	 {Sparse {MoEs} meet Efficient Ensembles},
  author =	 {James Urquhart Allingham and Florian Wenzel and
                  Zelda E Mariet and Basil Mustafa and Joan Puigcerver
                  and Neil Houlsby and Ghassen Jerfel and Vincent
                  Fortuin and Balaji Lakshminarayanan and Jasper Snoek
                  and Dustin Tran and Carlos Riquelme Ruiz and
                  Rodolphe Jenatton},
  journal =	 tmlr,
  year =	 2022,
  url =		 {https://openreview.net/forum?id=i0ZM36d2qU},
  abstract =	 {Machine learning models based on the aggregated
                  outputs of submodels, either at the activation or
                  prediction levels, often exhibit strong performance
                  compared to individual models. We study the
                  interplay of two popular classes of such models:
                  ensembles of neural networks and sparse mixture of
                  experts (sparse MoEs). First, we show that the two
                  approaches have complementary features whose
                  combination is beneficial. This includes a
                  comprehensive evaluation of sparse MoEs in
                  uncertainty related benchmarks. Then, we present
                  efficient ensemble of experts (E<sup>3</sup>), a
                  scalable and simple ensemble of sparse MoEs that
                  takes the best of both classes of models, while
                  using up to 45\% fewer FLOPs than a deep ensemble.
                  Extensive experiments demonstrate the accuracy,
                  log-likelihood, few-shot learning, robustness, and
                  uncertainty improvements of E<sup>3</sup> over
                  several challenging vision Transformer-based
                  baselines. E<sup>3</sup> not only preserves its
                  efficiency while scaling to models with up to 2.7B
                  parameters, but also provides better predictive
                  performance and uncertainty estimates for larger
                  models.},
  annote =	 {<a
                  href="https://github.com/google-research/vmoe">Code</a>}
}

@article{AndGha13a,
  author =	 {Andreas, Jacob and Ghahramani, Zoubin},
  journal =	 {ACL 2013},
  cat =		 {nlp},
  pages =	 91,
  title =	 {A Generative Model of Vector Space Semantics},
  year =	 2013,
  abstract =	 {We present a novel compositional, generative model
                  for vector space representations of meaning. This
                  model reformulates earlier tensor-based approaches
                  to vector space semantics as a top-down process, and
                  provides efficient algorithms for transformation
                  from natural language to vectors and from vectors to
                  natural language. We describe procedures for
                  estimating the parameters of the model from positive
                  examples of similar phrases, and from distributional
                  representations, then use these procedures to obtain
                  similarity judgments for a set of adjective-noun
                  pairs. The model’s estimation of the similarity of
                  these pairs correlates well with human annotations,
                  demonstrating a substantial improvement over several
                  existing compositional approaches in both settings.},
  url =		 {.}
}

@article{AndSzyRasetal02,
  cat =		 {gp},
  author =	 {Irene K.~Andersen and Anna Szymkowiak and Carl
                  Edward Rasmussen and L.~G.~Hanson and
                  J.~R.~Marstrand and H.~B.~W.~Larsson and Lars Kai
                  Hansen},
  title =	 {Perfusion Quantification using {G}aussian Process
                  Deconvolution},
  url =		 {.},
  doi =		 {10.1002/mrm.10213},
  journal =	 {Magnetic Resonance in Medicine},
  volume =	 48,
  number =	 2,
  pages =	 {351--361},
  year =	 2002,
  abstract =	 {The quantification of perfusion using dynamic
                  susceptibility contrast MR imaging requires
                  deconvolution to obtain the residual
                  impulse-response function (IRF). Here, a method
                  using a Gaussian process for deconvolution, GPD, is
                  proposed. The fact that the IRF is smooth is
                  incorporated as a constraint in the method. The GPD
                  method, which automatically estimates the noise
                  level in each voxel, has the advantage that model
                  parameters are optimized automatically. The GPD is
                  compared to singular value decomposition (SVD) using
                  a common threshold for the singular values and to
                  SVD using a threshold optimized according to the
                  noise level in each voxel. The comparison is carried
                  out using artificial data as well as using data from
                  healthy volunteers. It is shown that GPD is
                  comparable to SVD variable optimized threshold when
                  determining the maximum of the IRF, which is
                  directly related to the perfusion. GPD provides a
                  better estimate of the entire IRF. As the signal to
                  noise ratio increases or the time resolution of the
                  measurements increases, GPD is shown to be superior
                  to SVD. This is also found for large distribution
                  volumes.}
}

@inproceedings{AntAllHer20,
  cat =		 {approx deep},
  author =	 {Javier Antor\'an and James Urquhart Allingham and
                  Jos\'e Miguel Hern\'andez-Lobato},
  editor =	 {Hugo Larochelle and Marc'Aurelio Ranzato and Raia
                  Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien
                  Lin},
  title =	 {Depth Uncertainty in Neural Networks},
  booktitle =	 nips33,
  year =	 2020,
  url =
                  {https://proceedings.neurips.cc/paper/2020/hash/781877bda0783aac5f1cf765c128b437-Abstract.html},
  abstract =	 {Existing methods for estimating uncertainty in deep
                  learning tend to require multiple forward passes,
                  making them unsuitable for applications where
                  computational resources are limited. To solve this,
                  we perform probabilistic reasoning over the depth of
                  neural networks. Different depths correspond to
                  subnetworks which share weights and whose
                  predictions are combined via marginalisation,
                  yielding model uncertainty. By exploiting the
                  sequential structure of feed-forward networks, we
                  are able to both evaluate our training objective and
                  make predictions with a single forward pass. We
                  validate our approach on real-world regression and
                  image classification tasks. Our approach provides
                  uncertainty calibration, robustness to dataset
                  shift, and accuracies competitive with more
                  computationally expensive baselines.},
  annote =	 {<a
                  href="https://github.com/cambridge-mlg/DUN">Code</a>}
}

@inproceedings{AntBhaAdeEtal20,
  cat =		 {interpretability},
  author =	 {Javier Antor\'an and Umang Bhatt and Tameem Adel and
                  Adrian Weller and Jos\'e Miguel Hern\'andez-Lobato},
  title =	 {Getting a {CLUE}: A Method for Explaining
                  Uncertainty Estimates},
  booktitle =	 iclr09,
  year =	 2021,
  month =	 {April},
  url =		 {https://openreview.net/forum?id=XSLF1XFq5h},
  abstract =	 {Both uncertainty estimation and interpretability are
                  important factors for trustworthy machine learning
                  systems. However, there is little work at the
                  intersection of these two areas. We address this gap
                  by proposing a novel method for interpreting
                  uncertainty estimates from differentiable
                  probabilistic models, like Bayesian Neural Networks
                  (BNNs). Our method, Counterfactual Latent
                  Uncertainty Explanations (CLUE), indicates how to
                  change an input, while keeping it on the data
                  manifold, such that a BNN becomes more confident
                  about the input's prediction. We validate CLUE
                  through 1) a novel framework for evaluating
                  counterfactual explanations of uncertainty, 2) a
                  series of ablation experiments, and 3) a user
                  study. Our experiments show that CLUE outperforms
                  baselines and enables practitioners to better
                  understand which input patterns are responsible for
                  predictive uncertainty..},
}

@inproceedings{AntJanAllEtal22,
  cat =		 {approx deep},
  author =	 {Javier Antor\'an and David Janz and James Urquhart
                  Allingham and Erik A. Daxberger and Riccardo Barbano
                  and Eric T. Nalisnick and Jos\'e Miguel
                  Hern\'andez-Lobato},
  editor =	 {Kamalika Chaudhuri and Stefanie Jegelka and Le Song
                  and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan
                  Sabato},
  title =	 {Adapting the Linearised {L}aplace Model Evidence for
                  Modern Deep Learning},
  booktitle =	 icml2022,
  series =	 {Proceedings of Machine Learning Research},
  volume =	 162,
  pages =	 {796--821},
  publisher =	 {{PMLR}},
  year =	 2022,
  url =		 {https://proceedings.mlr.press/v162/antoran22a.html},
  abstract =	 {The linearised Laplace method for estimating model
                  uncertainty has received renewed attention in the
                  Bayesian deep learning community. The method
                  provides reliable error bars and admits a
                  closed-form expression for the model evidence,
                  allowing for scalable selection of model
                  hyperparameters. In this work, we examine the
                  assumptions behind this method, particularly in
                  conjunction with model selection. We show that these
                  interact poorly with some now-standard tools of deep
                  learning--stochastic approximation methods and
                  normalisation layers--and make recommendations for
                  how to better adapt this classic method to the
                  modern setting. We provide theoretical support for
                  our recommendations and validate them empirically on
                  MLPs, classic CNNs, residual networks with and
                  without normalisation layers, generative
                  autoencoders and transformers.},
}

@inproceedings{ArnSchLar09,
  title =	 {Bayesian nonnegative matrix factorization with
                  volume prior for unmixing of hyperspectral images},
  author =	 {Morten Arngren and Mikkel N.~Schmidt and Jan Larsen},
  booktitle =	 {Machine Learning for Signal Processing, IEEE
                  Workshop on (MLSP)},
  address =	 {Grenoble, France},
  month =	 {September},
  pages =	 {1--6},
  year =	 2009,
  isbn =	 {978-1-4244-4947-7},
  doi =		 {10.1109/MLSP.2009.5306262},
  url =		 {.},
  abstract =	 {In hyperspectral image analysis the objective is to
                  unmix a set of acquired pixels into pure spectral
                  signatures (endmembers) and corresponding fractional
                  abundances. The Non-negative Matrix Factorization
                  (NMF) methods have received a lot of attention for
                  this unmixing process. Many of these NMF based
                  unmixing algorithms are based on sparsity
                  regularization encouraging pure spectral endmembers,
                  but this is not optimal for certain applications,
                  such as foods, where abundances are not sparse. The
                  pixels will theoretically lie on a simplex and hence
                  the endmembers can be estimated as the vertices of
                  the smallest enclosing simplex. In this context we
                  present a Bayesian framework employing a volume
                  constraint for the NMF algorithm, where the
                  posterior distribution is numerically sampled from
                  using a Gibbs sampling procedure. We evaluate the
                  method on synthetical and real hyperspectral data of
                  wheat kernels.},
  annote =	 {This paper was "rated among the best papers
                  submitted" to the 2009 Machine Learning for Signal
                  Processing conference.}
}

@inproceedings{ArtBurWil2021,
  title =	 {Tighter Bounds on the Log Marginal Likelihood of
                  Gaussian Process Regression Using Conjugate
                  Gradients},
  author =	 {Artemev, Artem and Burt, David R. and van der Wilk,
                  Mark},
  booktitle =	 icml38,
  abstract =	 {We propose a lower bound on the log marginal
                  likelihood of Gaussian process regression models
                  that can be computed without matrix factorisation of
                  the full kernel matrix. We show that approximate
                  maximum likelihood learning of model parameters by
                  maximising our lower bound retains many benefits of
                  the sparse variational approach while reducing the
                  bias introduced into hyperparameter learning. The
                  basis of our bound is a more careful analysis of the
                  log-determinant term appearing in the log marginal
                  likelihood, as well as using the method of conjugate
                  gradients to derive tight lower bounds on the term
                  involving a quadratic form. Our approach is a step
                  forward in unifying methods relying on lower bound
                  maximisation (e.g. variational methods) and
                  iterative approaches based on conjugate gradients
                  for training Gaussian processes. In experiments, we
                  show improved predictive performance with our model
                  for a comparable amount of training time compared to
                  other conjugate gradient based approaches.},
  year =	 2021,
  cat =		 {gp},
  url =
                  {http://proceedings.mlr.press/v139/artemev21a/artemev21a.pdf}
}

@article{AshBuiNguyenetal22,
  cat =		 {approx},
  title =	 {Partitioned Variational Inferece: A Framework for
                  Probabilistic Federated Learning},
  author =	 {Matthew Ashman and Thang D. Bui and Cuong V. Nguyen
                  and Efstratios Markou and Adrian Weller and
                  Siddharth Swaroop and Richard E. Turner},
  abstract =	 {The proliferation of computing devices has brought
                  about an opportunity to deploy machine learning
                  models on new problem domains using previously
                  inaccessible data. Traditional algorithms for
                  training such models often require data to be stored
                  on a single machine with compute performed by a
                  single node, making them unsuitable for
                  decentralised training on multiple devices. This
                  deficiency has motivated the development of
                  federated learning algorithms, which allow multiple
                  data owners to train collaboratively and use a
                  shared model whilst keeping local data
                  private. However, many of these algorithms focus on
                  obtaining point estimates of model parameters,
                  rather than probabilistic estimates capable of
                  capturing model uncertainty, which is essential in
                  many applications. Variational inference (VI) has
                  become the method of choice for fitting many modern
                  probabilistic models. In this paper we introduce
                  partitioned variational inference (PVI), a general
                  framework for performing VI in the federated
                  setting. We develop new supporting theory for PVI,
                  demonstrating a number of properties that make it an
                  attractive choice for practitioners; use PVI to
                  unify a wealth of fragmented, yet related
                  literature; and provide empirical results that
                  showcase the effectiveness of PVI in a variety of
                  federated settings.},
  url =		 {https://arxiv.org/abs/2202.12275},
  year =	 2022,
}

@article{AshSoTebetal20,
  cat =		 {gp approx},
  title =	 {Sparse {G}aussian process variational autoencoders},
  author =	 {Matthew Ashman and Jonny So and Will Tebbutt and
                  Vincent Fortuin and Michael Pearce and Richard
                  E. Turner},
  abstract =	 {Large, multi-dimensional spatio-temporal datasets
                  are omnipresent in modern science and
                  engineering. An effective framework for handling
                  such data are Gaussian process deep generative
                  models (GP-DGMs), which employ GP priors over the
                  latent variables of DGMs. Existing approaches for
                  performing inference in GP-DGMs do not support
                  sparse GP approximations based on inducing points,
                  which are essential for the computational efficiency
                  of GPs, nor do they handle missing data -- a natural
                  occurrence in many spatio-temporal datasets -- in a
                  principled manner. We address these shortcomings
                  with the development of the sparse Gaussian process
                  variational autoencoder (SGP-VAE), characterised by
                  the use of partial inference networks for
                  parameterising sparse GP approximations. Leveraging
                  the benefits of amortised variational inference, the
                  SGP-VAE enables inference in multi-output sparse GPs
                  on previously unobserved data with no additional
                  training. The SGP-VAE is evaluated in a variety of
                  experiments where it outperforms alternative
                  approaches including multi-output GPs and structured
                  VAEs.},
  url =		 {https://arxiv.org/abs/2010.10177},
  year =	 2020,
}

@article{AzaSmoTer22,
  cat =		 {gp},
  author =	 {Iskander Azangulov and Andrei Smolensky and
                  Alexander Terenin and Viacheslav Borovitskiy},
  journal =	 {arXiv},
  title =	 {Stationary Kernels and {G}aussian Processes on {L}ie
                  Groups and their Homogeneous Spaces {I}: the Compact
                  Case},
  url =		 {https://arxiv.org/abs/2208.14960},
  year =	 2022,
  abstract =	 {Gaussian processes are arguably the most important
                  model class in spatial statistics. They encode prior
                  information about the modeled function and can be
                  used for exact or approximate Bayesian inference. In
                  many applications, particularly in physical sciences
                  and engineering, but also in areas such as
                  geostatistics and neuroscience, invariance to
                  symmetries is one of the most fundamental forms of
                  prior information one can consider. The invariance
                  of a Gaussian process' covariance to such symmetries
                  gives rise to the most natural generalization of the
                  concept of stationarity to such spaces. In this
                  work, we develop constructive and practical
                  techniques for building stationary Gaussian
                  processes on a very large class of non-Euclidean
                  spaces arising in the context of symmetries. Our
                  techniques make it possible to (i) calculate
                  covariance kernels and (ii) sample from prior and
                  posterior Gaussian processes defined on such spaces,
                  both in a practical manner. This work is split into
                  two parts, each involving different technical
                  considerations: part I studies compact spaces, while
                  part II studies non-compact spaces possessing
                  certain structure. Our contributions make the
                  non-Euclidean Gaussian process models we study
                  compatible with well-understood computational
                  techniques available in standard Gaussian process
                  software packages, thereby making them accessible to
                  practitioners.}
}

@inproceedings{Azr07,
  cat =		 {ssl},
  author =	 {Arik Azran},
  title =	 {The {R}endezvous algorithm: multiclass
                  semi-supervised learning with {M}arkov random walks},
  booktitle =	 icml24,
  address =	 {Corvallis, OR, USA},
  editor =	 {Zoubin Ghahramani},
  publisher =	 {Omnipress},
  year =	 2007,
  month =	 {June},
  pages =	 {49--56},
  url =		 {.},
  abstract =	 {We consider the problem of multiclass classification
                  where both labeled and unlabeled data points are
                  given. We introduce and demonstrate a new approach
                  for estimating a distribution over the missing
                  labels where data points are viewed as nodes of a
                  graph, and pairwise similarities are used to derive
                  a transition probability matrix P for a Markov
                  random walk between them. The algorithm associates
                  each point with a particle which moves between
                  points according to P. Labeled points are set to be
                  absorbing states of the Markov random walk, and the
                  probability of each particle to be absorbed by the
                  different labeled points, as the number of steps
                  increases, is then used to derive a distribution
                  over the associated missing label. A computationally
                  efficient algorithm to implement this is derived and
                  demonstrated on both real and artificial data sets,
                  including a numerical comparison with other
                  methods.}
}

@inproceedings{AzrGha06,
  author =	 {Arik Azran and Zoubin Ghahramani},
  title =	 {A new approach to data driven clustering},
  booktitle =	 icml23,
  publisher =	 {Omnipress},
  editor =	 {William Cohen and Andrew Moore},
  address =	 {Pittsburgh, PA, USA},
  pages =	 {57--64},
  year =	 2006,
  month =	 {June},
  url =		 {.},
  abstract =	 {We consider the problem of clustering in its most
                  basic form where only a local metric on the data
                  space is given. No parametric statistical model is
                  assumed, and the number of clusters is learned from
                  the data. We introduce, analyze and demonstrate a
                  novel approach to clustering where data points are
                  viewed as nodes of a graph, and pairwise
                  similarities are used to derive a transition
                  probability matrix P for a Markov random walk
                  between them. The algorithm automatically reveals
                  structure at increasing scales by varying the number
                  of steps taken by this random walk. Points are
                  represented as rows of Pt, which are the t-step
                  distributions of the walk starting at that point;
                  these distributions are then clustered using a
                  KL-minimizing iterative algorithm. Both the number
                  of clusters, and the number of steps that best
                  reveal it, are found by optimizing spectral
                  properties of P.}
}

@inproceedings{AzrGha06b,
  author =	 {Arik Azran and Zoubin Ghahramani},
  title =	 {Spectral Methods for Automatic Multiscale Data
                  Clustering},
  booktitle =	 {IEEE Conference on Computer Vision and Pattern
                  Recognition (CVPR)},
  year =	 2006,
  month =	 {June},
  address =	 {New York, NY, USA},
  publisher =	 {IEEE Computer Society},
  isbn =	 {0-7695-2597-0},
  pages =	 {190--197},
  doi =		 {10.1109/CVPR.2006.289},
  url =		 {.},
  abstract =	 {Spectral clustering is a simple yet powerful method
                  for finding structure in data using spectral
                  properties of an associated pairwise similarity
                  matrix. This paper provides new insights into how
                  the method works and uses these to derive new
                  algorithms which given the data alone automatically
                  learn different plausible data partitionings. The
                  main theoretical contribution is a generalization of
                  a key result in the field, the multicut lemma (Meila
                  2001). We use this generalization to derive two
                  algorithms. The first uses the eigenvalues of a
                  given affinity matrix to infer the number of
                  clusters in data, and the second combines learning
                  the affinity matrix with inferring the number of
                  clusters. A hierarchical implementation of the
                  algorithms is also derived. The algorithms are
                  theoretically motivated and demonstrated on
                  nontrivial data sets.}
}

@inproceedings{BabBhaWel22,
  cat =		 {interpretability},
  title =	 {On the Utility of Prediction Sets in Human-AI Teams},
  author =	 {Varun Babbar and Umang Bhatt and Adrian Weller},
  booktitle =	 ijcai,
  year =	 2022,
  url =		 {https://arxiv.org/abs/2205.01411},
  abstract =	 {Research on human-AI teams usually provides experts
                  with a single label, which ignores the uncertainty
                  in a model's recommendation. Conformal prediction
                  (CP) is a well established line of research that
                  focuses on building a theoretically grounded,
                  calibrated prediction set, which may contain
                  multiple labels. We explore how such prediction sets
                  impact expert decision-making in human-AI teams. Our
                  evaluation on human subjects finds that set valued
                  predictions positively impact experts. However, we
                  notice that the predictive sets provided by CP can
                  be very large, which leads to unhelpful AI
                  assistants. To mitigate this, we introduce D-CP, a
                  method to perform CP on some examples and defer to
                  experts. We prove that D-CP can reduce the
                  prediction set size of non-deferred examples. We
                  show how D-CP performs in quantitative and in human
                  subject experiments (n=120). Our results suggest
                  that CP prediction sets improve human-AI team
                  performance over showing the top-1 prediction alone,
                  and that experts find D-CP prediction sets are more
                  useful than CP prediction sets.}
}

@inproceedings{Babaetal18,
  cat =		 {fairness},
  title =	 {Purple Feed: Identifying High Consensus News Posts
                  on Social Media},
  booktitle =	 aies1,
  year =	 2018,
  month =	 {February},
  address =	 {New Orleans},
  abstract =	 {Although diverse news stories are actively posted on
                  social media, readers often focus on news which
                  reinforces their pre-existing views, leading to
                  ‘filter bubble’ effects. To combat this, some recent
                  systems expose and nudge readers toward stories with
                  different points of view. One example is the Wall
                  Street Journal’s ‘Blue Feed, Red Feed’ system, which
                  presents posts from biased publishers on each side
                  of a topic.  However, these systems have had limited
                  success.  In this work, we present a complementary
                  approach which identifies high consensus ‘purple’
                  posts that generate similar reactions from both
                  ‘blue’ and ‘red’ readers. We define and
                  operationalize consensus for news posts on Twitter
                  in the context of US politics. We identify several
                  high consensus posts and discuss their empirical
                  properties. We present a highly scalable method for
                  automatically identifying high and low consensus
                  news posts on Twitter, by utilizing a novel category
                  of audience leaning based features, which we show
                  are well suited to this task. Finally, we build and
                  publicly deploy our ‘Purple Feed’ system
                  (twitter-app.mpi-sws.org/purple-feed), which
                  highlights high consensus posts from publishers on
                  both sides of the political spectrum.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/AIES18-crowd_signals.pdf},
  author =	 {Mahmoudreza Babaei and Juhi Kulshrestha and Abhijnan
                  Chakraborty and Fabricio Benevenuto and Krishna
                  P. Gummadi and Adrian Weller}
}

@inproceedings{BahvanSchGha12,
  author =	 {A. Bahramisharif and M. A. J. {van Gerven} and
                  J-M. Schoffelen and Z. Ghahramani and T. Heskes},
  year =	 2012,
  title =	 {The dynamic beamformer},
  editor =	 {G. Langs et al},
  booktitle =	 {Machine Learning in Interpretation of Neuroimaging
                  (MLINI) 2011 LNAI 7263},
  pages =	 {148--155},
  abstract =	 {Beamforming is one of the most commonly used methods
                  for estimating the active neural sources from the
                  MEG or EEG sensor readings. The basic assumption in
                  beamforming is that the sources are uncorrelated,
                  which allows for estimating each source independent
                  of the others. In this paper, we incorporate the
                  independence assumption of the standard beamformer
                  in a linear dynamical system, thereby introducing
                  the dynamic beamformer. Using empirical data, we
                  show that the dynamic beamformer outperforms the
                  standard beamformer in predicting the condition of
                  interest which strongly suggests that it also
                  outperforms the standard method in localizing the
                  active neural generators.},
  url =		 {.}
}

@phdthesis{Bal20,
  author =	 {Matej Balog},
  title =	 {Converting to Optimization in Machine Learning:
                  {P}erturb-and-{MAP}, Differential Privacy, and
                  Program Synthesis},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2020,
  address =	 {Cambridge, UK},
  url =		 {http://matejbalog.eu/research/thesis.pdf},
  abstract =	 {On a mathematical level, most computational problems
                  encountered in machine learning are instances of one
                  of four abstract, fundamental problems: sampling,
                  integration, optimization, and search.  Thanks to
                  the rich history of the respective mathematical
                  fields, disparate methods with different properties
                  have been developed for these four problem classes.
                  As a result it can be beneficial to convert a
                  problem from one abstract class into a problem of a
                  different class, because the latter might come with
                  insights, techniques, and algorithms well suited to
                  the particular problem at hand. In particular, this
                  thesis contributes four new methods and
                  generalizations of existing methods for converting
                  specific non-optimization machine learning tasks
                  into optimization problems with more appealing
                  properties.  The first example is partition function
                  estimation (an integration problem), where an
                  existing algorithm -- the Gumbel trick -- for
                  converting to the MAP optimization problem is
                  generalized into a more general family of
                  algorithms, such that other instances of this family
                  have better statistical properties. Second, this
                  family of algorithms is further generalized to
                  another integration problem, the problem of
                  estimating Rényi entropies. The third example shows
                  how an intractable sampling problem arising when
                  wishing to publicly release a database containing
                  sensitive data in a safe ("differentially private")
                  manner can be converted into an optimization problem
                  using the theory of Reproducing Kernel Hilbert
                  Spaces. Finally, the fourth case study casts the
                  challenging discrete search problem of program
                  synthesis from input-output examples as a supervised
                  learning task that can be efficiently tackled using
                  gradient-based optimization.  In all four instances,
                  the conversions result in novel algorithms with
                  desirable properties. In the first instance, new
                  generalizations of the Gumbel trick can be used to
                  construct statistical estimators of the partition
                  function that achieve the same estimation error
                  while using up to 40% fewer samples. The second
                  instance shows that unbiased estimators of the Rényi
                  entropy can be constructed in the Perturb-and-MAP
                  framework. The main contribution of the third
                  instance is theoretical: the conversion shows that
                  it is possible to construct an algorithm for
                  releasing synthetic databases that approximate
                  databases containing sensitive data in a
                  mathematically precise sense, and to prove results
                  about their approximation errors. Finally, the
                  fourth conversion yields an algorithm for
                  synthesising program source code from input-output
                  examples that is able to solve test problems 1-3
                  orders of magnitude faster than a wide range of
                  baselines.}
}

@inproceedings{BalGauBroetal17,
  cat =		 {deep},
  author =	 {Matej Balog and Alexander L.~Gaunt and Marc
                  Brockschmidt and S<!>ebastian Nowozin and Daniel
                  Tarlow},
  title =	 {{D}eep{C}oder: {L}earning to Write Programs},
  abstract =	 {We develop a first line of attack for solving
                  programming competition-style problems from
                  input-output examples using deep learning. The
                  approach is to train a neural network to predict
                  properties of the program that generated the outputs
                  from the inputs. We use the neural network's
                  predictions to augment search techniques from the
                  programming languages community, including
                  enumerative search and an SMT-based
                  solver. Empirically, we show that our approach leads
                  to an order of magnitude speedup over the strong
                  non-augmented baselines and a Recurrent Neural
                  Network approach, and that we are able to solve
                  problems of difficulty comparable to the simplest
                  problems on programming competition websites.},
  booktitle =	 {5th International Conference on Learning
                  Representations},
  year =	 2017,
  month =	 {April},
  address =	 {Toulon, France},
  url =		 {https://openreview.net/forum?id=ByldLrqlx}
}

@inproceedings{BalLakGhaetal16,
  cat =		 {gp np},
  author =	 {Matej Balog and Balaji Lakshminarayanan and Zoubin
                  Ghahramani and Daniel M.~Roy and Yee Whye Teh},
  title =	 {The {M}ondrian Kernel},
  abstract =	 {We introduce the Mondrian kernel, a fast random
                  feature approximation to the Laplace kernel. It is
                  suitable for both batch and online learning, and
                  admits a fast kernel-width-selection procedure as
                  the random features can be re-used efficiently for
                  all kernel widths. The features are constructed by
                  sampling trees via a Mondrian process [Roy and Teh,
                  2009], and we highlight the connection to Mondrian
                  forests [Lakshminarayanan et al., 2014], where trees
                  are also sampled via a Mondrian process, but fit
                  independently. This link provides a new insight into
                  the relationship between kernel methods and random
                  forests.},
  pages =	 {32--41},
  booktitle =	 uai32,
  year =	 2016,
  month =	 {June},
  address =	 {Jersey City, New Jersey, USA},
  url =
                  {http://www.auai.org/uai2016/proceedings/papers/236.pdf},
  annote =	 {[<a
                  href="http://www.auai.org/uai2016/proceedings/supp/236_supp.pdf">Supplementary
                  Material</a>] [<a
                  href="https://arxiv.org/abs/1606.05241">arXiv</a>]
                  [<a
                  href="http://matejbalog.eu/research/mondrian_kernel_poster.pdf">Poster</a>]
                  [<a
                  href="http://matejbalog.eu/research/mondrian_kernel_slides.pdf">Slides</a>]
                  [<a
                  href="https://github.com/matejbalog/mondrian-kernel">Code</a>]
                  }
}

@article{BalMerMoi19,
  cat =		 {deep},
  author =	 {Matej Balog and Bart van Merri{\"e}nboer and
                  Subhodeep Moitra and Yujia Li and Daniel Tarlow},
  journal =	 {arXiv},
  title =	 {Fast training of sparse graph neural networks on
                  dense hardware},
  url =		 {https://arxiv.org/abs/1906.11786},
  year =	 2019,
  abstract =	 {Graph neural networks have become increasingly
                  popular in recent years due to their ability to
                  naturally encode relational input data and their
                  ability to scale to large graphs by operating on a
                  sparse representation of graph adjacency
                  matrices. As we look to scale up these models using
                  custom hardware, a natural assumption would be that
                  we need hardware tailored to sparse operations
                  and/or dynamic control flow. In this work, we
                  question this assumption by scaling up sparse graph
                  neural networks using a platform targeted at dense
                  computation on fixed-size data. Drawing inspiration
                  from optimization of numerical algorithms on sparse
                  matrices, we develop techniques that enable training
                  the sparse graph neural network model from Allamanis
                  et al. [2018] in 13 minutes using a 512-core TPUv2
                  Pod, whereas the original training takes almost a
                  day.}
}

@article{BalSinMan20,
  cat =		 {deep},
  author =	 {Matej Balog and Rishabh Singh and Petros Maniatis
                  and Charles Sutton},
  journal =	 {arXiv},
  title =	 {Neural program synthesis with a differentiable
                  fixer},
  url =		 {https://arxiv.org/abs/2006.10924},
  year =	 2020,
  abstract =	 {We present a new program synthesis approach that
                  combines an encoder-decoder based synthesis
                  architecture with a differentiable program
                  fixer. Our approach is inspired from the fact that
                  human developers seldom get their program correct on
                  the first attempt, and perform iterative
                  testing-based program fixing to get to the desired
                  program functionality. Similarly, our approach first
                  learns a distribution over programs conditioned on
                  an encoding of a set of input-output examples, and
                  then iteratively performs fix operations using the
                  differentiable fixer. The fixer takes as input the
                  original examples and the current program's outputs
                  on example inputs, and generates a new distribution
                  over the programs with the goal of reducing the
                  discrepancies between the current program outputs
                  and the desired example outputs. We train our
                  architecture end-to-end on the RobustFill domain,
                  and show that the addition of the fixer module leads
                  to a significant improvement on synthesis accuracy
                  compared to using beam search.}
}

@inproceedings{BalTolSch18,
  cat =		 {gp},
  author =	 {Matej Balog and Ilya Tolstikhin and Bernhard
                  Sch\"olkopf},
  title =	 {Differentially Private Database Release via Kernel
                  Mean Embeddings},
  abstract =	 {We lay theoretical foundations for new database
                  release mechanisms that allow third-parties to
                  construct consistent estimators of population
                  statistics, while ensuring that the privacy of each
                  individual contributing to the database is
                  protected. The proposed framework rests on two main
                  ideas. First, releasing (an estimate of) the kernel
                  mean embedding of the data generating random
                  variable instead of the database itself still allows
                  third-parties to construct consistent estimators of
                  a wide class of population statistics. Second, the
                  algorithm can satisfy the definition of differential
                  privacy by basing the released kernel mean embedding
                  on entirely synthetic data points, while controlling
                  accuracy through the metric available in a
                  Reproducing Kernel Hilbert Space.  We describe two
                  instantiations of the proposed framework, suitable
                  under different scenarios, and prove theoretical
                  results guaranteeing differential privacy of the
                  resulting algorithms and the consistency of
                  estimators constructed from their outputs.},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm, Sweden},
  url =
                  {http://matejbalog.eu/research/database_RKHS_privacy.pdf},
  annote =	 {[<a
                  href="https://arxiv.org/abs/1710.01641">arXiv</a>] }
}

@inproceedings{BalTriGhaetal17,
  cat =		 {gm approx},
  author =	 {Matej Balog and Nilesh Tripuraneni and Zoubin
                  Ghahramani and Adrian Weller},
  title =	 {Lost Relatives of the {G}umbel Trick},
  abstract =	 {The Gumbel trick is a method to sample from a
                  discrete probability distribution, or to estimate
                  its normalizing partition function.  The method
                  relies on repeatedly applying a random perturbation
                  to the distribution in a particular way, each time
                  solving for the most likely configuration. We derive
                  an entire family of related methods, of which the
                  Gumbel trick is one member, and show that the new
                  methods have superior properties in several settings
                  with minimal additional computational cost. In
                  particular, for the Gumbel trick to yield
                  computational benefits for discrete graphical
                  models, Gumbel perturbations on all configurations
                  are typically replaced with so-called low-rank
                  perturbations. We show how a subfamily of our new
                  methods adapts to this setting, proving new upper
                  and lower bounds on the log partition function and
                  deriving a family of sequential samplers for the
                  Gibbs distribution.  Finally, we balance the
                  discussion by showing how the simpler analytical
                  form of the Gumbel trick enables additional
                  theoretical results.},
  booktitle =	 icml34,
  year =	 2017,
  month =	 {August},
  address =	 {Sydney, Australia},
  url =
                  {http://matejbalog.eu/research/lost_relatives_of_the_gumbel_trick.pdf},
  annote =	 {[<a
                  href="http://arxiv.org/abs/1706.04161">arXiv</a>]
                  [<a
                  href="http://matejbalog.eu/research/lost_relatives_of_the_gumbel_trick_poster.pdf">Poster</a>]
                  [<a
                  href="http://matejbalog.eu/research/lost_relatives_of_the_gumbel_trick_slides.pdf">Slides</a>]
                  [<a
                  href="https://github.com/matejbalog/gumbel-relatives">Code</a>]
                  }
}

@inproceedings{BauWilRas16,
  cat =		 {gp approx},
  author =	 {Matthias Stephan Bauer and Mark van der Wilk and
                  Carl Edward Rasmussen},
  title =	 {Understanding Probabilistic Sparse {G}aussian
                  Process Approximations},
  booktitle =	 nips29,
  abstract =	 {Good sparse approximations are essential for
                  practical inference in Gaussian Processes as the
                  computational cost of exact methods is prohibitive
                  for large datasets. The Fully Independent Training
                  Conditional (FITC) and the Variational Free Energy
                  (VFE) approximations are two recent popular
                  methods. Despite superficial similarities, these
                  approximations have surprisingly different
                  theoretical properties and behave differently in
                  practice. We thoroughly investigate the two methods
                  for regression both analytically and through
                  illustrative examples, and draw conclusions to guide
                  practical application.},
  year =	 2016,
  url =
                  {http://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations.pdf},
  annote =	 {<a href="http://arxiv.org/abs/1606.04820">arXiv</a>},
}

@article{BeaFalGha05a,
  author =	 {Beal, Matthew J. and Falciani, Francesco and
                  Ghahramani, Zoubin and Rangel, Claudia and Wild,
                  David L.},
  cat =		 {bioinf},
  journal =	 {Bioinformatics},
  number =	 3,
  pages =	 {349-356},
  title =	 {A {B}ayesian approach to reconstructing genetic
                  regulatory networks with hidden factors},
  url =		 {.},
  volume =	 21,
  year =	 2005,
  abstract =	 {Motivation: We have used state-space models (SSMs)
                  to reverse engineer transcriptional networks from
                  highly replicated gene expression profiling time
                  series data obtained from a well-established model
                  of T cell activation. SSMs are a class of dynamic
                  Bayesian networks in which the observed measurements
                  depend on some hidden state variables that evolve
                  according to Markovian dynamics. These hidden
                  variables can capture effects that cannot be
                  directly measured in a gene expression profiling
                  experiment, for example: genes that have not been
                  included in the microarray, levels of regulatory
                  proteins, the effects of mRNA and protein
                  degradation, etc.  Results: We have approached the
                  problem of inferring the model structure of these
                  state-space models using both classical and Bayesian
                  methods. In our previous work, a bootstrap procedure
                  was used to derive classical confidence intervals
                  for parameters representing `gene--gene'
                  interactions over time. In this article, variational
                  approximations are used to perform the analogous
                  model selection task in the Bayesian
                  context. Certain interactions are present in both
                  the classical and the Bayesian analyses of these
                  regulatory networks. The resulting models place JunB
                  and JunD at the centre of the mechanisms that
                  control apoptosis and proliferation. These
                  mechanisms are key for clonal expansion and for
                  controlling the long term behavior (e.g. programmed
                  cell death) of these cells.}
}

@inproceedings{BeaGhaRas02,
  cat =		 {np},
  author =	 {Matthew J.~Beal and Zoubin Ghahramani and Carl
                  Edward Rasmussen},
  title =	 {The Infinite Hidden {M}arkov Model},
  booktitle =	 nips14,
  pages =	 {577--584},
  editors =	 {T. Dietterich, S. Becker, Z. Ghahramani},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  year =	 2002,
  month =	 {December},
  url =		 {.},
  abstract =	 {We show that it is possible to extend hidden Markov
                  models to have a countably infinite number of hidden
                  states. By using the theory of Dirichlet processes
                  we can implicitly integrate out the infinitely many
                  transition parameters, leaving only three
                  hyperparameters which can be learned from
                  data. These three hyperparameters define a
                  hierarchical Dirichlet process capable of capturing
                  a rich set of transition dynamics. The three
                  hyperparameters control the time scale of the
                  dynamics, the sparsity of the underlying
                  state-transition matrix, and the expected number of
                  distinct hidden states in a finite sequence. In this
                  framework it is also natural to allow the alphabet
                  of emitted symbols to be infinite --- consider, for
                  example, symbols being possible words appearing in
                  English text.}
}

@InProceedings{BerTurSah08a,
  cat =		 {approx mvision neuroscience},
  author =	 {Pietro Berkes and Richard E. Turner and Maneesh
                  Sahani},
  title =	 {On sparsity and overcompleteness in image models},
  booktitle =	 {nips20},
  year =	 2008,
  editor =	 {J. C. Platt and D. Koller and Y. Singer and
                  S. Roweis},
  volume =	 20,
  publisher =	 {mit},
  abstract =	 {Computational models of visual cortex, and in
                  particular those based on sparse coding, have
                  enjoyed much recent attention. Despite this
                  currency, the question of how sparse or how
                  over-complete a sparse representation should be, has
                  gone without principled answer. Here, we use
                  Bayesian model-selection methods to address these
                  questions for a sparse-coding model based on a
                  Student-t prior. Having validated our methods on toy
                  data, we find that natural images are indeed best
                  modelled by extremely sparse distributions; although
                  for the Student-t prior, the associated optimal
                  basis size is only modestly over-complete.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/berkes-et-al-2008a.html},
}

@article{BerkTurSah09a,
  cat =		 {approx time mvision neuroscience},
  author =	 {Pietro Berkes and Richard E. Turner and Maneesh
                  Sahani},
  journal =	 {PLoS Computational Biology},
  publisher =	 {Public Library of Science},
  title =	 {A Structured Model of Video Reproduces Primary
                  Visual Cortical Organisation},
  year =	 2009,
  month =	 09,
  volume =	 5,
  number =	 9,
  abstract =	 {The visual system must learn to infer the presence
                  of objects and features in the world from the images
                  it encounters, and as such it must, either
                  implicitly or explicitly, model the way these
                  elements interact to create the image. Do the
                  response properties of cells in the mammalian visual
                  system reflect this constraint? To address this
                  question, we constructed a probabilistic model in
                  which the identity and attributes of simple visual
                  elements were represented explicitly and learnt the
                  parameters of this model from unparsed, natural
                  video sequences. After learning, the behaviour and
                  grouping of variables in the probabilistic model
                  corresponded closely to functional and anatomical
                  properties of simple and complex cells in the
                  primary visual cortex (V1). In particular, feature
                  identity variables were activated in a way that
                  resembled the activity of complex cells, while
                  feature attribute variables responded much like
                  simple cells. Furthermore, the grouping of the
                  attributes within the model closely parallelled the
                  reported anatomical grouping of simple cells in cat
                  V1. Thus, this generative model makes explicit an
                  interpretation of complex and simple cells as
                  elements in the segmentation of a visual scene into
                  basic independent features, along with a
                  parametrisation of their moment-by-moment
                  appearances. We speculate that such a segmentation
                  may form the initial stage of a hierarchical system
                  that progressively separates the identity and
                  appearance of more articulated visual elements,
                  culminating in view-invariant object recognition.},
  doi =		 {10.1371/journal.pcbi.1000495},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/berkes-et-al-2009a.html},
}

@inproceedings{BhaAleAvi17,
  cat =		 {review},
  title =	 {Mapping Intelligence: Requirements and
                  Possibilities},
  booktitle =	 {Philosophy and Theory of Artificial Intelligence
                  (PT-AI)},
  year =	 2017,
  author =	 {Sankalp Bhatnagar and Anna Alexandrova and Shahar
                  Avin and Stephen Cave and Lucy Cheke and Matthew
                  Crosby and Jan Feyereisl and Marta Halina and Bao
                  Sheng Loe and Sean o Heigeartaigh and Fernando
                  Martínez-Plumed and Huw Price and Henry Shevlin and
                  Adrian Weller and Alan Winfield and Jose
                  Hernandez-Orallo},
  abstract =	 {New types of artificial intelligence (AI), from
                  cognitive assistants to social robots, are
                  challenging meaningful comparison with other kinds
                  of intelligence. How can such intelligent systems be
                  catalogued, evaluated, and contrasted, with
                  representations and projections that offer
                  meaningful insights? To catalyse the research in AI
                  and the future of cognition, we present the
                  motivation, requirements and possibilities for an
                  atlas of intelligence: an integrated framework and
                  collaborative open repository for collecting and
                  exhibiting information of all kinds of intelligence,
                  including humans, non-human animals, AI systems,
                  hybrids and collectives thereof. After presenting
                  this initiative, we review related efforts and
                  present the requirements of such a framework. We
                  survey existing visualisations and representations,
                  and discuss which criteria of inclusion should be
                  used to configure an atlas of intelligence.},
  url =
                  {https://link.springer.com/chapter/10.1007/978-3-319-96448-5_13}
}

@inproceedings{BhaAntZhaetal21,
  cat =		 {interpretability review},
  title =	 {Uncertainty as a form of transparency: Measuring,
                  communicating, and using uncertainty},
  author =	 {Bhatt, Umang and Antor{\'a}n, Javier and Zhang,
                  Yunfeng and Liao, Q Vera and Sattigeri, Prasanna and
                  Fogliato, Riccardo and Melan{\c{c}}on, Gabrielle and
                  Krishnan, Ranganath and Stanley, Jason and Tickoo,
                  Omesh and others},
  booktitle =	 aies4,
  year =	 2021,
  url =		 {https://dl.acm.org/doi/abs/10.1145/3461702.3462571},
  abstract =	 {Algorithmic transparency entails exposing system
                  properties to various stakeholders for purposes that
                  include understanding, improving, and contesting
                  predictions. Until now, most research into
                  algorithmic transparency has predominantly focused
                  on explainability. Explainability attempts to
                  provide reasons for a machine learning model's
                  behavior to stakeholders. However, understanding a
                  model's specific behavior alone might not be enough
                  for stakeholders to gauge whether the model is wrong
                  or lacks sufficient knowledge to solve the task at
                  hand. In this paper, we argue for considering a
                  complementary form of transparency by estimating and
                  communicating the uncertainty associated with model
                  predictions. First, we discuss methods for assessing
                  uncertainty. Then, we characterize how uncertainty
                  can be used to mitigate model unfairness, augment
                  decision-making, and build trustworthy
                  systems. Finally, we outline methods for displaying
                  uncertainty to stakeholders and recommend how to
                  collect information required for incorporating
                  uncertainty into existing ML pipelines. This work
                  constitutes an interdisciplinary review drawn from
                  literature spanning machine learning,
                  visualization/HCI, design, decision-making, and
                  fairness. We aim to encourage researchers and
                  practitioners to measure, communicate, and use
                  uncertainty as a form of transparency.}
}

@inproceedings{BhaWelMou20,
  cat =		 {interpretability},
  title =	 {Evaluating and Aggregating Feature-based Model
                  Explanations},
  author =	 {Umang Bhatt and Adrian Weller and Jose M. F. Moura},
  booktitle =	 ijcai,
  year =	 2020,
  url =		 {https://arxiv.org/abs/2005.00631},
  abstract =	 {A feature-based model explanation denotes how much
                  each input feature contributes to a model's output
                  for a given data point. As the number of proposed
                  explanation functions grows, we lack quantitative
                  evaluation criteria to help practitioners know when
                  to use which explanation function. This paper
                  proposes quantitative evaluation criteria for
                  feature-based explanations: low sensitivity, high
                  faithfulness, and low complexity. We devise a
                  framework for aggregating explanation functions. We
                  develop a procedure for learning an aggregate
                  explanation function with lower complexity and then
                  derive a new aggregate Shapley value explanation
                  function that minimizes sensitivity.}
}

@inproceedings{BhaXiaShaWeletal20,
  cat =		 {interpretability},
  title =	 {Explainable Machine Learning in Deployment},
  author =	 {Umang Bhatt and Alice Xiang and Shubham Sharma and
                  Adrian Weller and Ankur Taly and Yunhan Jia and
                  Joydeep Ghosh and Ruchir Puri and José M. F. Moura
                  and Peter Eckersley},
  booktitle =	 {ACM Conference on Fairness, Accountability, and
                  Transparency (FAT*)},
  year =	 2020,
  url =		 {https://arxiv.org/abs/1909.06342},
  abstract =	 {Explainable machine learning offers the potential to
                  provide stakeholders with insights into model
                  behavior by using various methods such as feature
                  importance scores, counterfactual explanations, or
                  influential training data. Yet there is little
                  understanding of how organizations use these methods
                  in practice. This study explores how organizations
                  view and use explainability for stakeholder
                  consumption. We find that, currently, the majority
                  of deployments are not for end users affected by the
                  model but rather for machine learning engineers, who
                  use explainability to debug the model itself. There
                  is thus a gap between explainability in practice and
                  the goal of transparency, since explanations
                  primarily serve internal stakeholders rather than
                  external ones. Our study synthesizes the limitations
                  of current explainability techniques that hamper
                  their use for end users. To facilitate end user
                  interaction, we develop a framework for establishing
                  clear goals for explainability. We end by discussing
                  concerns raised regarding explainability.}
}

@inproceedings{BisNguHooetal14,
  cat =		 {gp rl},
  author =	 {B.~Bischoff and D.~Nguyen-Tuong and D.~van Hoof and
                  A.~McHutchon and Carl Edward Rasmussen and A.~Knoll
                  and M.~P.~Deisenroth},
  title =	 {Policy search for learning robot control using
                  sparse data},
  year =	 2014,
  booktitle =	 icra,
  address =	 {Hong Kong, China},
  publisher =	 {IEEE},
  pages =	 {3882-3887},
  url =		 {.},
  doi =		 {10.1109/ICRA.2014.6907422},
  abstract =	 {In many complex robot applications, such as grasping
                  and manipulation, it is difficult to program desired
                  task solutions beforehand, as robots are within an
                  uncertain and dynamic environment. In such cases,
                  learning tasks from experience can be a useful
                  alternative. To obtain a sound learning and
                  generalization performance, machine learning,
                  especially, reinforcement learning, usually requires
                  sufficient data. However, in cases where only little
                  data is available for learning, due to system
                  constraints and practical issues, reinforcement
                  learning can act suboptimally. In this paper, we
                  investigate how model-based reinforcement learning,
                  in particular the probabilistic inference for
                  learning control method (PILCO), can be tailored to
                  cope with the case of sparse data to speed up
                  learning. The basic idea is to include further prior
                  knowledge into the learning process. As PILCO is
                  built on the probabilistic Gaussian processes
                  framework, additional system knowledge can be
                  incorporated by defining appropriate prior
                  distributions, e.g. a linear mean Gaussian
                  prior. The resulting PILCO formulation remains in
                  closed form and analytically tractable. The proposed
                  approach is evaluated in simulation as well as on a
                  physical robot, the Festo Robotino XT. For the robot
                  evaluation, we employ the approach for learning an
                  object pick-up task. The results show that by
                  including prior knowledge, policy learning can be
                  sped up in presence of sparse data.}
}

@inproceedings{Bloetal17,
  cat =		 {mcm np},
  title =	 {Sampling and inference for discrete random
                  probability measures in probabilistic programs},
  author =	 {Ben Bloem-Reddy and Emile Mathieu and Adam Foster
                  and Tom Rainforth and Hong Ge and Maria Lomeli and
                  Zoubin Ghahramani},
  address =	 {California, United States},
  booktitle =	 {{NIPS} workshop on {A}dvances in {A}pproximate
                  {I}nference},
  url =		 {http://approximateinference.org/accepted/},
  year =	 2017,
  month =	 {December},
  abstract =	 {We consider the problem of sampling a sequence from
                  a discrete random prob- ability measure (RPM) with
                  countable support, under (probabilistic) constraints
                  of finite memory and computation. A canonical
                  example is sampling from the Dirichlet Process,
                  which can be accomplished using its well-known
                  stick-breaking representation and lazy
                  initialization of its atoms. We show that
                  efficiently lazy initialization is possible if and
                  only if a size-biased representation of the discrete
                  RPM is known. For models constructed from such
                  discrete RPMs, we consider the implications for
                  generic particle-based inference methods in
                  probabilistic program- ming systems. To demonstrate,
                  we implement posterior inference for Normalized
                  Inverse Gaussian Process mixture models in Turing.}
}

@article{BooPenFreetal14,
  author =	 {Boomsma, Wouter and Tian, Pengfei and Frellsen, Jes
                  and Ferkinghoff-Borg, Jesper and Hamelryck, Thomas
                  and Lindorff-Larsen, Kresten and Vendruscolo,
                  Michele},
  cat =		 {bioinf gm mcm},
  title =	 {Equilibrium simulations of proteins using molecular
                  fragment replacement and {NMR} chemical shifts},
  journal =	 {Proceedings of the National Academy of Sciences},
  volume =	 111,
  number =	 38,
  pages =	 {13852-13857},
  year =	 2014,
  abstract =	 {Methods of protein structure determination based on
                  NMR chemical shifts are becoming increasingly
                  common. The most widely used approaches adopt the
                  molecular fragment replacement strategy, in which
                  structural fragments are repeatedly reassembled into
                  different complete conformations in molecular
                  simulations. Although these approaches are effective
                  in generating individual structures consistent with
                  the chemical shift data, they do not enable the
                  sampling of the conformational space of proteins
                  with correct statistical weights. Here, we present a
                  method of molecular fragment replacement that makes
                  it possible to perform equilibrium simulations of
                  proteins, and hence to determine their free energy
                  landscapes. This strategy is based on the encoding
                  of the chemical shift information in a probabilistic
                  model in Markov chain Monte Carlo
                  simulations. First, we demonstrate that with this
                  approach it is possible to fold proteins to their
                  native states starting from extended
                  structures. Second, we show that the method
                  satisfies the detailed balance condition and hence
                  it can be used to carry out an equilibrium sampling
                  from the Boltzmann distribution corresponding to the
                  force field used in the simulations. Third, by
                  comparing the results of simulations carried out
                  with and without chemical shift restraints we
                  describe quantitatively the effects that these
                  restraints have on the free energy landscapes of
                  proteins. Taken together, these results demonstrate
                  that the molecular fragment replacement strategy can
                  be used in combination with chemical shift
                  information to characterize not only the native
                  structures of proteins but also their conformational
                  fluctuations.},
  doi =		 {10.1073/pnas.1404948111},
}

@article{BorGha09a,
  author =	 {Karsten M. Borgwardt and Zoubin Ghahramani},
  cat =		 {np},
  journal =	 {arXiv},
  title =	 {Bayesian two-sample tests},
  url =		 {.},
  volume =	 {abs/0906.4032},
  year =	 2009,
  abstract =	 {In this paper, we present two classes of Bayesian
                  approaches to the two-sample problem. Our first
                  class of methods extends the Bayesian t-test to
                  include all parametric models in the exponential
                  family and their conjugate priors. Our second class
                  of methods uses Dirichlet process mixtures (DPM) of
                  such conjugate-exponential distributions as flexible
                  nonparametric priors over the unknown distributions.
                  }
}

@inproceedings{BorGorOuyetal16,
  author =	 {Borgstr\"{o}m, Johannes and Gordon, Andrew D. and
                  Ouyang, Long and Russo, Claudio and \'{S}cibior,
                  Adam and Szymczak, Marcin},
  title =	 {Fabular: Regression Formulas As Probabilistic
                  Programming},
  booktitle =	 {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
                  Symposium on Principles of Programming Languages},
  series =	 {POPL 2016},
  year =	 2016,
  isbn =	 {978-1-4503-3549-2},
  location =	 {St. Petersburg, FL, USA},
  pages =	 {271--283},
  numpages =	 13,
  url =		 {.},
  doi =		 {10.1145/2837614.2837653},
  acmid =	 2837653,
  publisher =	 {acm},
  address =	 {New York, NY, USA},
  keywords =	 {Bayesian inference, hierarchical models, linear
                  regression, probabilistic programming, relational
                  data},
  abstract =	 {Regression formulas are a domain-specific language
                  adopted by several R packages for describing an
                  important and useful class of statistical models:
                  hierarchical linear regressions. Formulas are
                  succinct, expressive, and clearly popular, so are
                  they a useful addition to probabilistic programming
                  languages? And what do they mean? We propose a core
                  calculus of hierarchical linear regression, in which
                  regression coefficients are themselves defined by
                  nested regressions (unlike in R). We explain how our
                  calculus captures the essence of the formula DSL
                  found in R. We describe the design and
                  implementation of Fabular, a version of the Tabular
                  schema-driven probabilistic programming language,
                  enriched with formulas based on our regression
                  calculus. To the best of our knowledge, this is the
                  first formal description of the core ideas of R's
                  formula notation, the first development of a
                  calculus of regression formulas, and the first
                  demonstration of the benefits of composing
                  regression formulas and latent variables in a
                  probabilistic programming language.}
}

@inproceedings{BouAllKre04a,
  author =	 {Bourne, Philip E. and Allerston, C. K. J. and Krebs,
                  Werner G. and Li, Wilfred W. and Shindyalov, Ilya
                  N. and Godzik, Adam and Friedberg, Iddo and Liu,
                  Tong and Wild, David L. and Hwang, Seungwoo and
                  Ghahramani, Zoubin and Chen, Li and Westbrook, John
                  D.},
  booktitle =	 {Pacific Symposium on Biocomputing},
  cat =		 {bioinf},
  editor =	 {Altman, Russ B. and Dunker, A. Keith and Hunter,
                  Lawrence and Jung, Tiffany A. and Klein, Teri E.},
  isbn =	 {981-238-598-3},
  pages =	 {375-386},
  publisher =	 {World Scientific},
  title =	 {The Status of Structural Genomics Defined Through
                  the Analysis of Current Targets and Structures},
  url =		 {.},
  year =	 2004,
  abstract =	 {Structural genomics--large-scale macromolecular
                  3-dimenional structure determination--is unique in
                  that major participants report scientific progress
                  on a weekly basis. The target database (TargetDB)
                  maintained by the Protein Data Bank
                  (http://targetdb.pdb.org) reports this progress
                  through the status of each protein sequence (target)
                  under consideration by the major structural genomics
                  centers worldwide. Hence, TargetDB provides a unique
                  opportunity to analyze the potential impact that
                  this major initiative provides to scientists
                  interested in the
                  sequence-structure-function-disease paradigm. Here
                  we report such an analysis with a focus on: (i)
                  temporal characteristics--how is the project doing
                  and what can we expect in the future? (ii) target
                  characteristics--what are the predicted functions of
                  the proteins targeted by structural genomics and how
                  biased is the target set when compared to the PDB
                  and to predictions across complete genomes? (iii)
                  structures solved--what are the characteristics of
                  structures solved thus far and what do they
                  contribute? The analysis required a more extensive
                  database of structure predictions using different
                  methods integrated with data from other
                  sources. This database, associated tools and related
                  data sources are available from
                  http://spam.sdsc.edu.  }
}

@inproceedings{BouZafMor13a,
  author =	 {Bousmalis, Konstantinos and Zafeiriou, Stefanos and
                  Morency, Louis-Philippe and Pantic, Maja and
                  Ghahramani, Zoubin},
  booktitle =	 {ECML/PKDD},
  editor =	 {Blockeel, Hendrik and Kersting, Kristian and
                  Nijssen, Siegfried and Zelezn\'{y}, Filip},
  isbn =	 {978-3-642-40990-5},
  pages =	 {531-547},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {Variational Hidden Conditional Random Fields with
                  Coupled {D}irichlet Process Mixtures},
  url =		 {.},
  volume =	 8189,
  year =	 2013,
  abstract =	 {Hidden Conditional Random Fields (HCRFs) are
                  discriminative latent variable models which have
                  been shown to successfully learn the hidden
                  structure of a given classification problem. An
                  infinite HCRF is an HCRF with a countably infinite
                  number of hidden states, which rids us not only of
                  the necessity to specify a priori a fixed number of
                  hidden states available but also of the problem of
                  overfitting. Markov chain Monte Carlo (MCMC)
                  sampling algorithms are often employed for inference
                  in such models. However, convergence of such
                  algorithms is rather difficult to verify, and as the
                  complexity of the task at hand increases, the
                  computational cost of such algorithms often becomes
                  prohibitive. These limitations can be overcome by
                  variational techniques. In this paper, we present a
                  generalized framework for infinite HCRF models, and
                  a novel variational inference approach on a model
                  based on coupled Dirichlet Process Mixtures, the
                  HCRF--DPM. We show that the variational HCRF--DPM is
                  able to converge to a correct number of represented
                  hidden states, and performs as well as the best
                  parametric HCRFs ---chosen via cross--validation---
                  for the difficult tasks of recognizing instances of
                  agreement, disagreement, and pain in audiovisual
                  sequences.  }
}

@inproceedings{BraOrt10,
  cat =		 {rl},
  author =	 {Daniel A.~Braun and Pedro A.~Ortega},
  title =	 {A minimum relative entropy principle for adaptive
                  control in linear quadratic regulators},
  booktitle =	 {Proceedings of the 7th international conference on
                  informatics in control, automation and robotics},
  pages =	 {(in press)},
  year =	 2010,
  url =		 {.},
  abstract =	 {The design of optimal adaptive controllers is
                  usually based on heuristics, because solving
                  Bellman's equations over information states is
                  notoriously intractable. Approximate adaptive
                  controllers often rely on the principle of
                  certainty-equivalence where the control process
                  deals with parameter point estimates as if they
                  represented ``true'' parameter values. Here we
                  present a stochastic control rule instead where
                  controls are sampled from a posterior distribution
                  over a set of probabilistic input-output models and
                  the true model is identified by Bayesian
                  inference. This allows reformulating the adaptive
                  control problem as an inference and sampling problem
                  derived from a minimum relative entropy principle.
                  Importantly, inference and action sampling both work
                  forward in time and hence such a Bayesian adaptive
                  controller is applicable on-line. We demonstrate the
                  improved performance that can be achieved by such an
                  approach for linear quadratic regulator examples.}
}

@inproceedings{BraOrtTheSch11,
  cat =		 {rl},
  author =	 {Daniel A.~Braun and Pedro A.~Ortega and Evangelos
                  Theodorou and Stefan Schaal},
  title =	 {Path Integral Control and Bounded Rationality},
  booktitle =	 {2011 {IEEE} Symposium on Adaptive Dynamic
                  Programming and Reinforcement Learning},
  year =	 2011,
  url =		 {.},
  abstract =	 {Path integral methods have recently been shown to be
                  applicable to a very general class of optimal
                  control problems. Here we examine the path integral
                  formalism from a decision-theoretic point of view,
                  since an optimal controller can always be regarded
                  as an instance of a perfectly rational
                  decision-maker that chooses its actions so as to
                  maximize its expected utility. The problem with
                  perfect rationality is, however, that finding
                  optimal actions is often very difficult due to
                  prohibitive computational resource costs that are
                  not taken into account. In contrast, a bounded
                  rational decision-maker has only limited resources
                  and therefore needs to strike some compromise
                  between the desired utility and the required
                  resource costs. In particular, we suggest an
                  information-theoretic measure of resource costs that
                  can be derived axiomatically. As a consequence we
                  obtain a variational principle for choice
                  probabilities that trades off maximizing a given
                  utility criterion and avoiding resource costs that
                  arise due to deviating from initially given default
                  choice probabilities. The resulting bounded rational
                  policies are in general probabilistic. We show that
                  the solutions found by the path integral formalism
                  are such bounded rational policies. Furthermore, we
                  show that the same formalism generalizes to discrete
                  control problems, leading to linearly solvable
                  bounded rational control policies in the case of
                  Markov systems. Importantly, Bellman's optimality
                  principle is not presupposed by this variational
                  principle, but it can be derived as a limit
                  case. This suggests that the information theoretic
                  formalization of bounded rationality might serve as
                  a general principle in control design that unifies a
                  number of recently reported approximate optimal
                  control methods both in the continuous and discrete
                  domain.},
}

@article{BraOrtWol11,
  author =	 {Daniel A. Braun and Pedro A. Ortega and Daniel
                  M. Wolpert},
  title =	 {Motor coordination: When two have to act as one},
  journal =	 {Special issue of Experimental Brain Research on
                  Joint Action},
  year =	 2011,
  abstract =	 {Trying to pass someone walking toward you in a
                  narrow corridor is a familiar example of a
                  two-person motor game that requires coordination. In
                  this study, we investigate coordination in
                  sensorimotor tasks that correspond to classic
                  coordination games with multiple Nash equilibria,
                  such as "choosing sides", "stag hunt", "chicken",
                  and "battle of sexes". In these tasks, subjects made
                  reaching movements reflecting their continuously
                  evolving "decisions" while they received a
                  continuous payoff in the form of a resistive force
                  counteracting their movements. Successful
                  coordination required two subjects to "choose" the
                  same Nash equilibrium in this force-payoff landscape
                  within a single reach. We found that on the majority
                  of trials coordination was achieved. Compared to the
                  proportion of trials in which miscoordination
                  occurred, successful coordination was characterized
                  by several distinct features: an increased mutual
                  information between the players' movement endpoints,
                  an increased joint entropy during the movements, and
                  by differences in the timing of the players'
                  responses.  Moreover, we found that the probability
                  of successful coordination depends on the players'
                  initial distance from the Nash equilibria. Our
                  results suggest that two-person coordination arises
                  naturally in motor interactions and is facilitated
                  by favorable initial positions, stereotypical motor
                  pattern, and differences in response times.},
  url =		 {http://www.ncbi.nlm.nih.gov/pubmed/21455618}
}

@article{BraOrtWol11b,
  author =	 {Daniel A.~Braun and Pedro A.~Ortega and Daniel
                  M.~Wolpert},
  title =	 {Nash equilibria in multi-agent motor interactions},
  journal =	 {PLoS Computational Biology},
  year =	 2009,
  volume =	 5,
  number =	 8,
  abstract =	 {Social interactions in classic cognitive games
                  likeBlack-box alpha (BB-α) is a new approximate
                  inference method based on the minimization of
                  α-divergences. BB-αscales to large datasets because
                  it can be implemented using stochastic gradient
                  descent. BB-αcan be applied to complex probabilistic
                  models with little effort since it only requires as
                  input the likelihood function and its
                  gradients. These gradients can be easily obtained
                  using automatic differentiation. By changing the
                  divergence parameter α, the method is able to
                  interpolate between variational Bayes (VB) (α→0) and
                  an algorithm similar to expectation propagation (EP)
                  (α= 1). Experiments on probit regression and neural
                  network regression and classification problems show
                  that BB-αwith non-standard settings of α, such as α=
                  0.5, usually produces better predictions than with
                  α→0 (VB) or α= 1 (EP).  the ultimatum game or the
                  prisoner's dilemma typically lead to Nash equilibria
                  when multiple competitive decision makers with
                  perfect knowledge select optimal
                  strategies. However, in evolutionary game theory it
                  has been shown that Nash equilibria can also arise
                  as attractors in dynamical systems that can
                  describe, for example, the population dynamics of
                  microorganisms. Similar to such evolutionary
                  dynamics, we find that Nash equilibria arise
                  naturally in motor interactions in which players vie
                  for control and try to minimize effort. When
                  confronted with sensorimotor interaction tasks that
                  correspond to the classical prisoner's dilemma and
                  the rope-pulling game, two-player motor interactions
                  led predominantly to Nash solutions. In contrast,
                  when a single player took both roles, playing the
                  sensorimotor game bimanually, cooperative solutions
                  were found. Our methodology opens up a new avenue
                  for the study of human motor interactions within a
                  game theoretic framework, suggesting that the
                  coupling of motor systems can lead to game theoretic
                  solutions.},
  url =		 {http://www.ncbi.nlm.nih.gov/pubmed/19680426}
}

@article{BraQuaGha14a,
  author =	 {Brati\`{e}res, S\'{e}bastien and Quadrianto, Novi
                  and Ghahramani, Zoubin},
  cat =		 {gp, gm},
  journal =	 {arXiv},
  title =	 {Bayesian Structured Prediction Using {G}aussian
                  Processes},
  url =		 {.},
  volume =	 {abs/1307.3846},
  year =	 2013,
  abstract =	 {We introduce a conceptually novel structured
                  prediction model, GPstruct, which is kernelized,
                  non-parametric and Bayesian, by design. We motivate
                  the model with respect to existing approaches, among
                  others, conditional random fields (CRFs), maximum
                  margin Markov networks (M3N), and structured support
                  vector machines (SVMstruct), which embody only a
                  subset of its properties. We present an inference
                  procedure based on Markov Chain Monte Carlo. The
                  framework can be instantiated for a wide range of
                  structured objects such as linear chains, trees,
                  grids, and other general graphs. As a proof of
                  concept, the model is benchmarked on several natural
                  language processing tasks and a video gesture
                  segmentation task involving a linear chain
                  structure. We show prediction accuracies for
                  GPstruct which are comparable to or exceeding those
                  of CRFs and SVMstruct.  }
}

@inproceedings{BraQuaNowGha14,
  cat =		 {gp mvision np},
  author =	 {S\'{e}bastien Brati\`{e}res and Novi Quadrianto and
                  S<!>ebastian Nowozin and Zoubin Ghahramani},
  year =	 2014,
  title =	 {Scalable {Gaussian Process} Structured Prediction
                  for Grid Factor Graph Applications},
  booktitle =	 icml31,
  abstract =	 {Structured prediction is an important and well
                  studied problem with many applications across
                  machine learning. GPstruct is a recently proposed
                  structured prediction model that offers appealing
                  properties such as being kernelised, non-parametric,
                  and supporting Bayesian inference (Bratières et
                  al. 2013).  The model places a Gaussian process
                  prior over energy functions which describe
                  relationships between input variables and structured
                  output variables.  However, the memory demand of
                  GPstruct is quadratic in the number of latent
                  variables and training runtime scales cubically.
                  This prevents GPstruct from being applied to
                  problems involving grid factor graphs, which are
                  prevalent in computer vision and spatial statistics
                  applications.  Here we explore a scalable approach
                  to learning GPstruct models based on ensemble
                  learning, with weak learners (predictors) trained on
                  subsets of the latent variables and bootstrap data,
                  which can easily be distributed.  We show
                  experiments with 4M latent variables on image
                  segmentation.  Our method outperforms widely-used
                  conditional random field models trained with
                  pseudo-likelihood.  Moreover, in image segmentation
                  problems it improves over recent state-of-the-art
                  marginal optimisation methods in terms of predictive
                  performance and uncertainty calibration. Finally, it
                  generalises well on all training set sizes. },
  url =		 {.}
}

@inproceedings{BraVanVlaGha10,
  cat =		 {np mcmc},
  author =	 {S\'{e}bastien Brati\`{e}res and Jurgen {Van Gael}
                  and Andreas Vlachos and Zoubin Ghahramani},
  abstract =	 {This paper compares parallel and distributed
                  implementations of an iterative, Gibbs sampling,
                  machine learning algorithm. Distributed
                  implementations run under Hadoop on facility
                  computing clouds. The probabilistic model under
                  study is the infinite HMM <a
                  href="/pub/#BeaGhaRas02">Beal, Ghahramani and
                  Rasmussen, 2002</a>, in which parameters are learnt
                  using an instance blocked Gibbs sampling, with a
                  step consisting of a dynamic program. We apply this
                  model to learn part-of-speech tags from newswire
                  text in an unsupervised fashion. However our focus
                  here is on runtime performance, as opposed to
                  NLP-relevant scores, embodied by iteration duration,
                  ease of development, deployment and debugging.},
  address =	 {Bradford, UK},
  booktitle =	 {Proceedings of the 2010 10th IEEE International
                  Conference on Computer and Information Technology},
  pages =	 {1235--1240},
  title =	 {Scaling the {iHMM}: {P}arallelization versus
                  {H}adoop},
  url =		 {.},
  year =	 2010,
  isbn =	 {978-0-7695-4108-2},
  doi =		 {10.1109/CIT.2010.223},
  publisher =	 {IEEE Computer Society}
}

@phdthesis{Bro20,
  cat =		 {deep},
  author =	 {John Bronskill},
  title =	 {Data and computation efficient meta-learning},
  school =	 {University of Cambridge},
  year =	 2020,
  address =	 {Cambridge, UK},
  month =	 {November},
  url =
                  {https://www.repository.cam.ac.uk/bitstream/handle/1810/313327/2020-11-22_John_Bronskil_PhD_Thesis_Final.pdf},
  abstract =	 {In order to make predictions with high accuracy,
                  conventional deep learning systems require large
                  training datasets consisting of thousands or
                  millions of examples and long training times
                  measured in hours or days, consuming high levels of
                  electricity with a negative impact on our
                  environment. It is desirable to have have machine
                  learning systems that can emulate human behavior
                  such that they can quickly learn new concepts from
                  only a few examples. This is especially true if we
                  need to quickly customize or personalize machine
                  learning models to specific scenarios where it would
                  be impractical to acquire a large amount of training
                  data and where a mobile device is the means for
                  computation. We define a data efficient machine
                  learning system to be one that can learn a new
                  concept from only a few examples (or shots) and a
                  computation efficient machine learning system to be
                  one that can learn a new concept rapidly without
                  retraining on an everyday computing device such as a
                  smart phone. In this work, we design, develop,
                  analyze, and extend the theory of machine learning
                  systems that are both data efficient and computation
                  efficient. We present systems that are trained using
                  multiple tasks such that it "learns how to learn" to
                  solve new tasks from only a few examples. These
                  systems can efficiently solve new, unseen tasks
                  drawn from a broad range of data distributions, in
                  both the low and high data regimes, without the need
                  for costly retraining. Adapting to a new task
                  requires only a forward pass of the example task
                  data through the trained network making the learning
                  of new tasks possible on mobile devices. In
                  particular, we focus on few-shot image
                  classification systems, i.e. machine learning
                  systems that can distinguish between numerous
                  classes of objects depicted in digital images given
                  only a few examples of each class of object to learn
                  from.}
}

@inproceedings{BroGorReqetal20,
  cat =		 {deep},
  author =	 {John Bronskill and Jonathan Gordon and James
                  Requeima and S<!>ebastian Nowozin and Richard
                  E. Turner},
  title =	 {Task{N}orm: rethinking batch normalization for
                  meta-learning},
  booktitle =	 icml37,
  year =	 2020,
  publisher =	 pmlr,
  url =
                  {https://proceedings.icml.cc/static/paper_files/icml/2020/2696-Paper.pdf},
  abstract =	 {Modern meta-learning approaches for image
                  classification rely on increasingly deep networks to
                  achieve state-of-the-art performance, making batch
                  normalization an essential component of
                  meta-learning pipelines. However, the hierarchical
                  nature of the meta-learning setting presents several
                  challenges that can render conventional batch
                  normalization ineffective, giving rise to the need
                  to rethink normalization in this setting. We
                  evaluate a range of approaches to batch
                  normalization for meta-learning scenarios, and
                  develop a novel approach that we call
                  TASKNORM. Experiments on fourteen datasets
                  demonstrate that the choice of batch normalization
                  has a dramatic effect on both classification
                  accuracy and training time for both gradient based
                  and gradient free meta-learning
                  approaches. Importantly, TASKNORM is found to
                  consistently improve performance. Finally, we
                  provide a set of best practices for normalization
                  that will allow fair comparison of meta-learning
                  algorithms.}
}

@inproceedings{BroMasPatetal21,
  cat =		 {deep},
  author =	 {John Bronskill and Daniela Massiceti and
                  Massimiliano Patacchiola and Katja Hofmann and
                  S<!>ebastian Nowozin and Richard E. Turner},
  title =	 {Memory efficient meta-learning with large images},
  booktitle =	 nips35,
  year =	 2021,
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},
  abstract =	 {Meta learning approaches to few-shot classification
                  are computationally efficient at test time,
                  requiring just a few optimization steps or single
                  forward pass to learn a new task, but they remain
                  highly memory-intensive to train.  This limitation
                  arises because a task's entire support set, which
                  can contain up to 1000 images, must be processed
                  before an optimization step can be taken. Harnessing
                  the performance gains offered by large images thus
                  requires either parallelizing the meta-learner
                  across multiple GPUs, which may not be available, or
                  trade-offs between task and image size when memory
                  constraints apply. We improve on both options by
                  proposing LITE, a general and memory efficient
                  episodic training scheme that enables meta-training
                  on large tasks composed of large images on a single
                  GPU. We achieve this by observing that the gradients
                  for a task can be decomposed into a sum of gradients
                  over the task's training images. This enables us to
                  perform a forward pass on a task's entire training
                  set but realize significant memory savings by
                  back-propagating only a random subset of these
                  images which we show is an unbiased approximation of
                  the full gradient. We use LITE to train
                  meta-learners and demonstrate new state-of-the-art
                  accuracy on the real-world ORBIT benchmark and 3 of
                  the 4 parts of the challenging VTAB+ MD benchmark
                  relative to leading meta-learners.  LITE also
                  enables meta-learners to be competitive with
                  transfer learning approaches but at a fraction of
                  the test-time computational cost, thus serving as a
                  counterpoint to the recent narrative that transfer
                  learning is all you need for few-shot
                  classification.}
}

@inproceedings{BruPerTebetal20,
  cat =		 {gp},
  author =	 {Wessel Bruinsma and Eric Perim and Will Tebbutt and
                  J. Scott Hosking and Arno Solin and Richard
                  E. Turner},
  title =	 {Scalable Exact Inference in Multi-Output {G}aussian
                  Processes},
  booktitle =	 icml37,
  year =	 2020,
  publisher =	 pmlr,
  url =
                  {https://proceedings.icml.cc/static/paper_files/icml/2020/4027-Paper.pdf},
  abstract =	 {Multi-output Gaussian processes (MOGPs) leverage the
                  flexibility and interpretability of GPs while
                  capturing structure across outputs, which is
                  desirable, for example, in spatio-temporal
                  modelling. The key problem with MOGPs is their
                  computational scaling $O(n^3 p^3)$, which is cubic
                  in the number of both inputs $n$ (e.g., time points
                  or locations) and outputs $p$. For this reason, a
                  popular class of MOGPs assumes that the data live
                  around a low-dimensional linear subspace, reducing
                  the complexity to $O(n^3 m^3)$. However, this cost
                  is still cubic in the dimensionality of the subspace
                  $m$, which is still prohibitively expensive for many
                  applications. We propose the use of a sufficient
                  statistic of the data to accelerate inference and
                  learning in MOGPs with orthogonal bases. The method
                  achieves linear scaling in $m$ in practice, allowing
                  these models to scale to large $m$ without
                  sacrificing significant expressivity or requiring
                  approximation. This advance opens up a wide range of
                  real-world tasks and can be combined with existing
                  GP approximations in a plug-and-play way. We
                  demonstrate the efficacy of the method on various
                  synthetic and real-world data sets.}
}

@inproceedings{BruReqFoetal21,
  cat =		 {deep},
  author =	 {Wessel P. Bruinsma and James Requeima and Andrew
                  Y. K. Foong and Jonathan Gordon and Richard
                  E. Turner},
  title =	 {The {G}aussian Neural Process},
  booktitle =	 aabi3,
  year =	 2021,
  url =		 {https://openreview.net/pdf?id=rzsDn7Vzxf},
  abstract =	 {Neural Processes (NPs; Garnelo et al., 2018a,b) are
                  a rich class of models for meta-learning that map
                  data sets directly to predictive stochastic
                  processes. We provide a rigorous analysis of the
                  standard maximum-likelihood objective used to train
                  conditional NPs. Moreover, we propose a new member
                  to the Neural Process family called the Gaussian
                  Neural Process (GNP), which models predictive
                  correlations, incorporates translation equivariance,
                  provides universal approximation guarantees, and
                  demonstrates encouraging performance.}
}

@inproceedings{BruTegTur22,
  cat =		 {gp approx time},
  title =	 {Modelling Non-Smooth Signals with Complex Spectral
                  Structure},
  author =	 {Wessel P. Bruinsma and Martin Tegn{\'e}r and Richard
                  E. Turner},
  booktitle =	 {aistats25},
  year =	 2022,
  url =		 {https://proceedings.mlr.press/v151/bruinsma22a.html},
  abstract =	 {The Gaussian Process Convolution Model (GPCM; Tobar
                  et al., 2015a) is a model for signals with complex
                  spectral structure. A significant limitation of the
                  GPCM is that it assumes a rapidly decaying spectrum:
                  it can only model smooth signals. Moreover,
                  inference in the GPCM currently requires (1) a
                  mean-field assumption, resulting in poorly
                  calibrated uncertainties, and (2) a tedious
                  variational optimisation of large covariance
                  matrices. We redesign the GPCM model to induce a
                  richer distribution over the spectrum with relaxed
                  assumptions about smoothness: the Causal Gaussian
                  Process Convolution Model (CGPCM) introduces a
                  causality assumption into the GPCM, and the Rough
                  Gaussian Process Convolution Model (RGPCM) can be
                  interpreted as a Bayesian nonparametric
                  generalisation of the fractional Ornstein–Uhlenbeck
                  process.  We also propose a more effective
                  variational inference scheme, going beyond the
                  mean-field assumption: we design a Gibbs sampler
                  which directly samples from the optimal variational
                  solution, circumventing any variational optimisation
                  entirely. The proposed variations of the GPCM are
                  validated in experiments on synthetic and real-world
                  data, showing promising results.}
}

@inproceedings{BuiHerHerLiTur16,
  cat =		 {gp approx deep},
  author =	 {Thang D.~Bui and Daniel Hern\'andez-Lobato and
                  Jos\'e Miguel Hern\'andez-Lobato and Yingzhen Li and
                  Richard E.~Turner},
  title =	 {Deep {G}aussian Processes for Regression using
                  Approximate Expectation Propagation},
  booktitle =	 icml33,
  year =	 2016,
  address =	 {New York, USA},
  month =	 {June},
  url =		 {http://proceedings.mlr.press/v48/bui16.html},
  abstract =	 {Deep Gaussian processes (DGPs) are multi-layer
                  hierarchical generalisations of Gaussian processes
                  (GPs) and are formally equivalent to neural networks
                  with multiple, infinitely wide hidden layers. DGPs
                  are nonparametric probabilistic models and as such
                  are arguably more flexible, have a greater capacity
                  to generalise, and provide better calibrated
                  uncertainty estimates than alternative deep
                  models. This paper develops a new approximate
                  Bayesian learning scheme that enables DGPs to be
                  applied to a range of medium to large scale
                  regression problems for the first time. The new
                  method uses an approximate Expectation Propagation
                  procedure and a novel and efficient extension of the
                  probabilistic backpropagation algorithm for
                  learning. We evaluate the new method for non-linear
                  regression on eleven real-world datasets, showing
                  that it always outperforms GP regression and is
                  almost always better than state-of-the-art
                  deterministic and sampling-based approximate
                  inference methods for Bayesian neural networks. As a
                  by-product, this work provides a comprehensive
                  analysis of six approximate Bayesian methods for
                  training neural networks. }
}

@inproceedings{BuiNguTur14,
  cat =		 {gp approx},
  author =	 {Thang D.~Bui and Cuong V.~Nguyen and Richard
                  E.~Turner},
  title =	 {Streaming sparse {G}aussian process approximations},
  booktitle =	 nips31,
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California, USA},
  volume =	 31,
  url =
                  {https://papers.nips.cc/paper/6922-streaming-sparse-gaussian-process-approximations},
  abstract =	 {Sparse approximations for Gaussian process models
                  provide a suite of methods that enable these models
                  to be deployed in large data regime and enable
                  analytic intractabilities to be
                  sidestepped. However, the field lacks a principled
                  method to handle streaming data in which the
                  posterior distribution over function values and the
                  hyperparameters are updated in an online
                  fashion. The small number of existing approaches
                  either use suboptimal hand-crafted heuristics for
                  hyperparameter learning, or suffer from catastrophic
                  forgetting or slow updating when new data
                  arrive. This paper develops a new principled
                  framework for deploying Gaussian process
                  probabilistic models in the streaming setting,
                  providing principled methods for learning
                  hyperparameters and optimising pseudo-input
                  locations. The proposed framework is experimentally
                  validated using synthetic and real-world datasets.},
  annote =	 {The first two authors contributed equally.}
}

@inproceedings{BuiTur14,
  cat =		 {gp approx time},
  author =	 {Thang D.~Bui and Richard E.~Turner},
  title =	 {Tree-structured {G}aussian Process Approximations},
  booktitle =	 nips28,
  year =	 2014,
  volume =	 28,
  editor =	 {Z. Ghahramani and M. Welling and C. Cortes and
                  N.D. Lawrence and K.Q. Weinberger},
  pages =	 {2213--2221},
  publisher =	 {Curran Associates, Inc.},
  abstract =	 {Gaussian process regression can be accelerated by
                  constructing a small pseudo-dataset to summarize the
                  observed data. This idea sits at the heart of many
                  approximation schemes, but such an approach requires
                  the number of pseudo-datapoints to be scaled with
                  the range of the input space if the accuracy of the
                  approximation is to be maintained. This presents
                  problems in time-series settings or in spatial
                  datasets where large numbers of pseudo-datapoints
                  are required since computation typically scales
                  quadratically with the pseudo-dataset size. In this
                  paper we devise an approximation whose complexity
                  grows linearly with the number of
                  pseudo-datapoints. This is achieved by imposing a
                  tree or chain structure on the pseudo-datapoints and
                  calibrating the approximation using a
                  Kullback-Leibler (KL) minimization. Inference and
                  learning can then be performed efficiently using the
                  Gaussian belief propagation algorithm. We
                  demonstrate the validity of our approach on a set of
                  challenging regression tasks including missing data
                  imputation for audio and spatial datasets. We trace
                  out the speed-accuracy trade-off for the new method
                  and show that the frontier dominates those obtained
                  from a large number of existing approximation
                  techniques.}
}

@article{BuiYanTur17,
  cat =		 {gp approx},
  author =	 {Thang D.~Bui and Josiah Yan and Richard E.~Turner},
  title =	 {A Unifying Framework for {G}aussian Process
                  Pseudo-Point Approximations using Power Expectation
                  Propagation},
  journal =	 jmlr,
  year =	 2017,
  volume =	 18,
  number =	 104,
  pages =	 {1-72},
  url =		 {http://jmlr.org/papers/v18/16-603.html},
  abstract =	 {Gaussian processes (GPs) are flexible distributions
                  over functions that enable high-level assumptions
                  about unknown functions to be encoded in a
                  parsimonious, flexible and general way. Although
                  elegant, the application of GPs is limited by
                  computational and analytical intractabilities that
                  arise when data are sufficiently numerous or when
                  employing non-Gaussian models. Consequently, a
                  wealth of GP approximation schemes have been
                  developed over the last 15 years to address these
                  key limitations. Many of these schemes employ a
                  small set of pseudo data points to summarise the
                  actual data. In this paper we develop a new
                  pseudo-point approximation framework using Power
                  Expectation Propagation (Power EP) that unifies a
                  large number of these pseudo-point approximations.
                  Unlike much of the previous venerable work in this
                  area, the new framework is built on standard methods
                  for approximate inference (variational free-energy,
                  EP and Power EP methods) rather than employing
                  approximations to the probabilistic generative model
                  itself. In this way all of the approximation is
                  performed at `inference time' rather than at
                  `modelling time', resolving awkward philosophical
                  and empirical questions that trouble previous
                  approaches.  Crucially, we demonstrate that the new
                  framework includes new pseudo-point approximation
                  methods that outperform current approaches on
                  regression and classification tasks.}
}

@phdthesis{Bur2022,
  author =	 {David R. Burt},
  cat =		 {gp},
  school =	 {University of Cambridge, Department of Engineering},
  address =	 {Cambridge, UK},
  abstract =	 {Models with Gaussian process priors and Gaussian
                  likelihoods are one of only a handful of Bayesian
                  models where inference can be performed without the
                  need for approximation. However, a frequent
                  criticism of these models from practitioners of
                  Bayesian machine learning is that they are
                  challenging to scale to large datasets due to the
                  need to compute a large kernel matrix and perform
                  standard linear-algebraic operations with this
                  matrix. This limitation has driven decades of
                  research in both statistics and machine learning
                  seeking to scale Gaussian process regression models
                  to ever-larger datasets. This thesis builds on this
                  line of research. We focus on the problem of
                  approximate inference and model selection with
                  approximate maximum marginal likelihood as applied
                  to Gaussian process regression. Our discussion is
                  guided by three questions: Does an approximation
                  work on a range of models and datasets? Can you
                  verify that an approximation has worked on a given
                  dataset? Is an approximation easy for a practitioner
                  to use? While we are far from the first to ask these
                  questions, we offer new insights into each question
                  in the context of Gaussian process regression. In
                  the first part of this thesis, we focus on sparse
                  variational Gaussian process regression (Titsias,
                  2009). We provide new diagnostics for inference with
                  this method that can be used as practical guides for
                  practitioners trying to balance computation and
                  accuracy with this approximation. We then provide an
                  asymptotic analysis that highlights properties of
                  the model and dataset that are sufficient for this
                  approximation to perform reliable inference with a
                  small computational cost. This analysis builds on an
                  approach laid out in Burt (2018), as well as on
                  similar guarantees in the kernel ridge regression
                  literature. In the second part of this thesis, we
                  consider iterative methods, especially the method of
                  conjugate gradients, as applied to Gaussian process
                  regression (Gibbs and MacKay, 1997). We primarily
                  focus on improving the reliability of approximate
                  maximum marginal likelihood when using these
                  approximations. We investigate how the method of
                  conjugate gradients and related approaches can be
                  used to derive bounds on quantities related to the
                  log marginal likelihood. This idea can be used to
                  improve the speed and stability of model selection
                  with these approaches, making them easier to use in
                  practice.},
  title =	 {Scalable Approximate Inference and Model Selection
                  in {G}aussian Process Regression},
  year =	 2022,
  url =
                  {https://www.repository.cam.ac.uk/bitstream/handle/1810/339367/burt_thesis_2022.pdf}
}

@inproceedings{BurObeGaretal21,
  cat =		 {deep approx},
  author =	 {David R. Burt and Sebastian W. Ober and Adri\`{a}
                  Garriga-Alonso and Mark van der Wilk},
  title =	 {Understanding variational inference in
                  function-space},
  booktitle =	 aabi3,
  year =	 2021,
  url =		 {https://openreview.net/pdf?id=7P9y3sRa5Mk},
  abstract =	 {Recent work has attempted to directly approximate
                  the ‘function-space’ or predictive posterior
                  distribution of Bayesian models, without
                  approximating the posterior distribution over the
                  parameters. This is appealing in e.g.  Bayesian
                  neural networks, where we only need the former, and
                  the latter is hard to represent. In this work, we
                  highlight some advantages and limitations of
                  employing the Kullback-Leibler divergence in this
                  setting. For example, we show that minimizing the KL
                  divergence between a wide class of parametric
                  distributions and the posterior induced by a
                  (non-degenerate) Gaussian process prior leads to an
                  ill-defined objective function. Then, we propose
                  (featurized) Bayesian linear regression as a
                  benchmark for ‘function-space’ inference methods
                  that directly measures approximation quality. We
                  apply this methodology to assess aspects of the
                  objective function and inference scheme considered
                  in Sun et al. (2018), emphasizing the quality of
                  approximation to Bayesian inference as opposed to
                  predictive performance.}
}

@article{BurRasWil19,
  cat =		 {gp approx},
  author =	 {David R Burt and Carl Edward Rasmussen and Mark van
                  der Wilk},
  journal =	 {arXiv},
  title =	 {Rates of Convergence for Sparse Variational
                  {G}aussian Process Regression},
  url =		 {http://arxiv.org/abs/1903.03571},
  year =	 2019,
  abstract =	 {Excellent variational approximations to Gaussian
                  process posteriors have been developed which avoid
                  the O(N<sup>3</sup>) scaling with dataset size
                  N. They reduce the computational cost to
                  O(NM<sup>2</sup>), with M&Lt;N being the number of
                  inducing variables, which summarise the
                  process. While the computational cost seems to be
                  linear in N, the true complexity of the algorithm
                  depends on how M must increase to ensure a certain
                  quality of approximation. We address this by
                  characterising the behavior of an upper bound on the
                  KL divergence to the posterior. We show that with
                  high probability the KL divergence can be made
                  arbitrarily small by growing M more slowly than N. A
                  particular case of interest is that for regression
                  with normally distributed inputs in D-dimensions
                  with the popular Squared Exponential kernel,
                  M=O(log<sup>D</sup>N) is sufficient. Our results
                  show that as datasets grow, Gaussian process
                  posteriors can truly be approximated cheaply, and
                  provide a concrete rule for how to increase M in
                  continual learning scenarios.}
}

@article{BurRasWil20,
  cat =		 {gp approx},
  author =	 {David R. Burt and Carl Edward Rasmussen and Mark van
                  der Wilk},
  title =	 {Convergence of Sparse Variational Inference in
                  {G}aussian Processes Regression},
  journal =	 jmlr,
  year =	 2020,
  volume =	 21,
  abstract =	 {Gaussian processes are distributions over functions
                  that are versatile and mathematically convenient
                  priors in Bayesian modelling. However, their use is
                  often impeded for data with large numbers of
                  observations, N, due to the cubic (in N) cost of
                  matrix operations used in exact inference. Many
                  solutions have been proposed that rely on M << N
                  inducing variables to form an approximation at a
                  cost of O(NM<sup>2</sup>). While the computational
                  cost appears linear in N, the true complexity
                  depends on how M must scale with N to ensure a
                  certain quality of the approximation. In this work,
                  we investigate upper and lower bounds on how M needs
                  to grow with N to ensure high quality
                  approximations. We show that we can make the
                  KL-divergence between the approximate model and the
                  exact posterior arbitrarily small for a
                  Gaussian-noise regression model with M <<
                  N. Specifically, for the popular squared exponential
                  kernel and D-dimensional Gaussian distributed
                  covariates, M = O((log N)<sup>D</sup>) suffice and a
                  method with an overall computational cost of O(N(log
                  N)<sup>2D</sup>(log log N)<sup>2</sup>) can be used
                  to perform inference.},
  url =
                  {http://www.jmlr.org/papers/volume21/19-1015/19-1015.pdf}
}

@article{ButRobZiletal22,
  cat =		 {fairness},
  author =	 {Butcher, Bradley and Robinson, Chris and Zilka, Miri
                  and Fogliato, Riccardo and Ashurst, Carolyn and
                  Weller, Adrian},
  title =	 {Racial Disparities in the Enforcement of Marijuana
                  Violations in the US},
  journal =	 {Proceedings of the 2022 AAAI/ACM Conference on AI,
                  Ethics, and Society},
  year =	 2022,
  url =		 {https://doi.org/10.1145/3514094.3534184},
  abstract =	 {Racial disparities in US drug arrest rates have been
                  observed for decades, but their causes and policy
                  implications are still contested. Some have argued
                  that the disparities largely reflect differences in
                  drug use between racial groups, while others have
                  hypothesized that discriminatory enforcement
                  policies and police practices play a significant
                  role. In this work, we analyze racial disparities in
                  the enforcement of marijuana violations in the
                  US. Using data from the National Incident-Based
                  Reporting System (NIBRS) and the National Survey on
                  Drug Use and Health (NSDUH) programs, we investigate
                  whether marijuana usage and purchasing behaviors can
                  explain the racial composition of offenders in
                  police records. We examine potential driving
                  mechanisms behind these disparities and the extent
                  to which county-level socioeconomic factors are
                  associated with corresponding disparities. Our
                  results indicate that the significant racial
                  disparities in reported incidents and arrests cannot
                  be explained by differences in marijuana days-of-use
                  alone. Variations in the location where marijuana is
                  purchased and in the frequency of these purchases
                  partially explain the observed disparities. We
                  observe an increase in racial disparities across
                  most counties over the last decade, with the
                  greatest increases in states that legalized the use
                  of marijuana within this timeframe. Income, high
                  school graduation rate, and rate of employment
                  positively correlate with larger racial disparities,
                  while the rate of incarceration is negatively
                  correlated. We conclude with a discussion of the
                  implications of the observed racial disparities in
                  the context of algorithmic fairness.}
}

@inproceedings{ByrHasTroetal22,
  cat =		 {rl},
  author =	 {Arunkumar Byravan and Leonard Hasenclever and Piotr
                  Trochim and Mehdi Mirza and Alessandro Davide
                  Ialongo and Yuval Tassa and Jost Tobias Springenberg
                  and Abbas Abdolmaleki and Nicolas Heess and Josh
                  Merel and Martin Riedmiller},
  title =	 {Evaluating Model-Based Planning and Planner
                  Amortization for Continuous Control},
  booktitle =	 iclr10,
  year =	 2022,
  url =		 {https://openreview.net/pdf?id=SS8F6tFX3-},
  abstract =	 {There is a widespread intuition that model-based
                  control methods should be able to surpass the data
                  efficiency of model-free approaches. In this paper
                  we attempt to evaluate this intuition on various
                  challenging locomotion tasks. We take a hybrid
                  approach, combining model predictive control (MPC)
                  with a learned model and model-free policy learning;
                  the learned policy serves as a proposal for MPC. We
                  show that MPC with learned proposals and models
                  (trained on the fly or transferred from related
                  tasks) can significantly improve performance and
                  data efficiency with respect to model-free
                  methods. However, we find that well-tuned model-free
                  agents are strong baselines even for high DoF
                  control problems. Finally, we show that it is
                  possible to distil a model-based planner into a
                  policy that amortizes the planning computation
                  without any loss of performance.}
}

@inproceedings{Cal15,
  cat =		 {np},
  author =	 {Jan-Peter Calliess},
  title =	 {Bayesian {L}ipschitz Constant Estimation and
                  Quadrature},
  booktitle =	 {Workshop on Probabilistic Integration, NIPS},
  year =	 2015,
  address =	 {Montreal, Canada},
  month =	 {December},
  url =
                  {http://probabilistic-numerics.org/assets/pdf/jcalliess_NIPS15.pdf},
  abstract =	 {Lipschitz quadrature methods provide an approach to
                  one-dimensional numerical integration on bounded
                  domains. On the basis of the assumption that the
                  integrand is Lipschitz continuous with a known
                  Lipschitz constant, these quadrature rules can
                  provide a tight error bound around their integral
                  estimates and utilise the Lipschitz constant to
                  guide exploration in the context of adaptive
                  quadrature. In this paper, we outline our ongoing
                  work on extending this approach to settings where
                  the Lipschitz constant is probabilistically
                  uncertain. As the key component, we introduce a
                  Bayesian approach for updating a subjectively
                  probabilistic belief of the Lipschitz
                  constant. Combined with any Lipschitz quadrature
                  rule, we obtain an approach for translating a sample
                  into an integral estimate with probabilistic
                  uncertainty intervals. The paper concludes with an
                  illustration of the approach followed by a
                  discussion of open issues and future work.}
}

@article{Cal16,
  author =	 {Jan-Peter Calliess},
  cat =		 {np rl},
  journal =	 {arXiv},
  title =	 {Lazily Adapted Constant Kinky Inference for
                  Nonparametric Regression and Model-Reference
                  Adaptive Control},
  url =		 {https://arxiv.org/abs/1701.00178},
  volume =	 {arXiv:1701.00178},
  year =	 2016,
  abstract =	 {Techniques known as Nonlinear Set Membership
                  prediction, Lipschitz Interpolation or Kinky
                  Inference are approaches to machine learning that
                  utilise presupposed Lipschitz properties to compute
                  inferences over unobserved function values. Provided
                  a bound on the true best Lipschitz constant of the
                  target function is known a priori they offer
                  convergence guarantees as well as bounds around the
                  predictions. Considering a more general setting that
                  builds on H\"older continuity relative to
                  pseudo-metrics, we propose an online method for
                  estimating the Hoelder constant online from function
                  value observations that possibly are corrupted by
                  bounded observational errors. Utilising this to
                  compute adaptive parameters within a kinky inference
                  rule gives rise to a nonparametric machine learning
                  method, for which we establish strong universal
                  approximation guarantees. That is, we show that our
                  prediction rule can learn any continuous function in
                  the limit of increasingly dense data to within a
                  worst-case error bound that depends on the level of
                  observational uncertainty. We apply our method in
                  the context of nonparametric model-reference
                  adaptive control (MRAC). Across a range of simulated
                  aircraft roll-dynamics and performance metrics our
                  approach outperforms recently proposed alternatives
                  that were based on Gaussian processes and RBF-neural
                  networks. For discrete-time systems, we provide
                  stability guarantees for our learning-based
                  controllers both for the batch and the online
                  learning setting.}
}

@inproceedings{Cal17,
  cat =		 {np},
  author =	 {Jan-Peter Calliess},
  title =	 {Lipschitz Optimisation for {L}ipschitz
                  Interpolation},
  booktitle =	 {2017 American Control Conference (ACC 2017)},
  year =	 2017,
  address =	 {Seattle, WA, USA},
  month =	 {May},
  abstract =	 {Techniques known as Nonlinear Set Membership
                  prediction, Kinky Inference or Lipschitz
                  Interpolation are fast and numerically robust
                  approaches to nonparametric machine learning that
                  have been proposed to be utilised in the context of
                  system identification and learning-based
                  control. They utilise presupposed Lipschitz
                  properties in order to compute inferences over
                  unobserved function values. Unfortunately, most of
                  these approaches rely on exact knowledge about the
                  input space metric as well as about the Lipschitz
                  constant. Furthermore, existing techniques to
                  estimate the Lipschitz constants from the data are
                  not robust to noise or seem to be ad-hoc and
                  typically are decoupled from the ultimate learning
                  and prediction task. To overcome these limitations,
                  we propose an approach for optimising parameters of
                  the presupposed metrics by minimising validation set
                  prediction errors. To avoid poor performance due to
                  local minima, we propose to utilise Lipschitz
                  properties of the optimisation objective to ensure
                  global optimisation success. The resulting approach
                  is a new flexible method for nonparametric black-box
                  learning. We illustrate its competitiveness on a set
                  of benchmark problems.},
  url =
                  {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7963430}
}

@inproceedings{CalKorGor16,
  author =	 {Jan-Peter Calliess and Nathan Korda and Geoffrey
                  J. Gordon},
  title =	 {A Distributed Mechanism for Multi-Agent Convex
                  Optimisation and Coordination with No-Regret
                  Learners},
  booktitle =	 {Workshop on Learning, Inference and Control of
                  Multi-Agent Systems, NIPS},
  year =	 2016,
  address =	 {Barcelona, Spain},
  month =	 {December},
  url =		 {https://sites.google.com/site/malicnips2016/},
  abstract =	 {We develop an indirect mechanism for coordinated,
                  distributed multi-agent optimisation, and
                  decision-making.  Our approach extends previous work
                  in no-regret learning based mechanism design and
                  renders it applicable to partial information
                  settings.  We consider planning problems that can be
                  stated as a collection of single-agent convex
                  programmes coupled by common soft constraints.  A
                  key idea is to recast the joint optimisation problem
                  as distributed learning in a repeated game between
                  the original agents and a newly introduced group of
                  adversarial agents who influence prices for
                  decisions and facilitate coordination.  Under the
                  weak behavioural assumption that all agents employ
                  selfish, sub-linear regret algorithms in the course
                  of the repeated game, we guarantee that our
                  mechanism can achieve design goals such as social
                  optimality (efficiency) and Nash-equilibrium
                  convergence to within an error which approaches zero
                  as the agents gain experience.  Our error bounds are
                  deterministic or probabilistic, depending on the
                  nature of the regret bounds available for the
                  algorithms employed by the agents.  We illustrate
                  our method in an emissions market application.}
}

@inproceedings{CalPetRasDei16,
  cat =		 {gp},
  author =	 {Roberto Calandra and Jan Peters and Carl Edward
                  Rasmussen and Marc Peter Deisenroth},
  title =	 {Manifold {G}aussian Processes for Regression},
  booktitle =	 {International Joint Conference on Neural Networks},
  year =	 2016,
  url =		 {.},
  abstract =	 {Off-the-shelf Gaussian Process (GP) covariance
                  functions encode smoothness assumptions on the
                  structure of the function to be modeled. To model
                  complex and nondifferentiable functions, these
                  smoothness assumptions are often too
                  restrictive. One way to alleviate this limitation is
                  to find a different representation of the data by
                  introducing a feature space. This feature space is
                  often learned in an unsupervised way, which might
                  lead to data representations that are not useful for
                  the overall regression task. In this paper, we
                  propose Manifold Gaussian Processes, a novel
                  supervised method that jointly learns a
                  transformation of the data into a feature space and
                  a GP regression from the feature space to observed
                  space. The Manifold GP is a full GP and allows to
                  learn data representations, which are useful for the
                  overall regression task. As a proof-of-concept, we
                  evaluate our approach on complex non-smooth
                  functions where standard GPs perform poorly, such as
                  step functions and robotics tasks with contacts.}
}

@inproceedings{CalRobRasMac18,
  cat =		 {rl},
  author =	 {Jan-Peter Calliess and Stephen Roberts and Carl
                  Edward Rasmussen and Jan Maciejowski},
  title =	 {Nonlinear Set Membership Regression with Adaptive
                  Hyper-Parameter Estimation for Online Learning and
                  Control},
  booktitle =	 {Proceedings of the European Control Conference},
  abstract =	 {Methods known as Lipschitz Interpolation or
                  Nonlinear Set Membership regression have become
                  established tools for nonparametric
                  system-identification and data-based control. They
                  utilise presupposed Lipschitz properties to compute
                  inferences over unobserved function
                  values. Unfortunately, they rely on the a priori
                  knowledge of a Lipschitz constant of the underlying
                  target function which serves as a hyperparameter.
                  We propose a closed-form estimator of the Lipschitz
                  constant that is robust to bounded observational
                  noise in the data. The merger of Lipschitz
                  Interpolation with the new hyperparameter estimator
                  gives a new nonparametric machine learning method
                  for which we derive online learning convergence
                  guarantees. Furthermore, we apply our learning
                  method to model-reference adaptive control and
                  provide a convergence guarantee on the closed-loop
                  dynamics. In a simulated flight manoeuvre control
                  scenario, we compare the performance of our approach
                  to recently proposed alternative learning-based
                  controllers.},
  url =		 {.},
  year =	 2018
}

@article{CalRobRasMac20,
  cat =		 {gp,np,approx},
  author =	 {Jan-Peter Calliess and Stephen J.~Roberts and Carl
                  Edward Rasmussen and Jan Maciejowski},
  title =	 {Lazily Adapted Constant Kinky Inference for
                  non-parametric regression and model-reference
                  adaptive control},
  journal =	 {Automatica},
  doi =		 {10.1016/j.automatica.2020.109216},
  volume =	 122,
  year =	 2020,
  abstract =	 {Techniques known as Nonlinear Set Membership
                  prediction or Lipschitz Interpolation are approaches
                  to supervised machine learning that utilise
                  presupposed Lipschitz properties to perform
                  inference over unobserved function values. Provided
                  a bound on the true best Lipschitz constant of the
                  target function is known a priori, they offer
                  convergence guarantees, as well as bounds around the
                  predictions. Considering a more general setting that
                  builds on Lipschitz continuity, we propose an online
                  method for estimating the Lipschitz constant online
                  from function value observations that are possibly
                  corrupted by bounded noise. Utilising this as a
                  data-dependent hyper-parameter gives rise to a
                  nonparametric machine learning method, for which we
                  establish strong universal approximation
                  guarantees. That is, we show that our prediction
                  rule can learn any continuous function on compact
                  support in the limit of increasingly dense data, up
                  to a worst-case error that can be bounded by the
                  level of observational error. We also consider
                  applications of our nonparametric regression method
                  to learning-based control. For a class of
                  discrete-time settings, we establish convergence
                  guarantees on the closed-loop tracking error of our
                  online learning-based controllers. To provide
                  evidence that our method can be beneficial not only
                  in theory but also in practice, we apply it in the
                  context of nonparametric model-reference adaptive
                  control (MRAC). Across a range of simulated aircraft
                  roll-dynamics and performance metrics our approach
                  outperforms recently proposed alternatives that were
                  based on Gaussian processes and RBF-neural
                  networks.}
}

@article{CavNyrVolWel19,
  cat =		 {fairness interpretability},
  author =	 {Stephen Cave and Rune Nyrup and Karina Vold and
                  Adrian Weller},
  year =	 2019,
  title =	 {Motivations and Risks of Machine Ethics},
  journal =	 {Proceedings of the IEEE},
  volume =	 {107(3)},
  pages =	 {562--574},
  url =		 {https://ieeexplore.ieee.org/document/8456834},
  abstract =	 {This paper surveys reasons for and against pursuing
                  the field of machine ethics, understood as research
                  aiming to build ``ethical machines.'' We clarify the
                  nature of this goal, why it is worth pursuing, and
                  the risks involved in its pursuit. First, we survey
                  and clarify some of the philosophical issues
                  surrounding the concept of an ``ethical machine''
                  and the aims of machine ethics. Second, we argue
                  that while there are good prima facie reasons for
                  pursuing machine ethics, including the potential to
                  improve the ethical alignment of both humans and
                  machines, there are also potential risks that must
                  be considered. Third, we survey these potential
                  risks and point to where research should be devoted
                  to clarifying and managing potential risks. We
                  conclude by making some recommendations about the
                  questions that future work could address.}
}

@article{ChaCunGlo09,
  cat =		 {gp neuro},
  author =	 {C. Chang and J. P. Cunningham and G. Glover},
  title =	 {Influence of heart rate on the BOLD signal: the
                  cardiac response function},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/ChangNeuroimage2009.pdf},
  journal =	 {NeuroImage},
  volume =	 44,
  pages =	 {857-869},
  year =	 2009,
  abstract =	 {It has previously been shown that low-frequency
                  fluctuations in both respiratory volume and cardiac
                  rate can induce changes in the blood-oxygen level
                  dependent (BOLD) signal. Such physiological noise
                  can obscure the detection of neural activation using
                  fMRI, and it is therefore important to model and
                  remove the effects of this noise. While a
                  hemodynamic response function relating respiratory
                  variation (RV) and the BOLD signal has been
                  described, no such mapping for heart rate (HR) has
                  been proposed. In the current study, the effects of
                  RV and HR are simultaneously deconvolved from
                  resting state fMRI. It is demonstrated that a
                  convolution model including RV and HR can explain
                  significantly more variance in gray matter BOLD
                  signal than a model that includes RV alone, and an
                  average HR response function is proposed that well
                  characterizes our subject population. It is observed
                  that the voxel-wise morphology of the deconvolved RV
                  responses is preserved when HR is included in the
                  model, and that its form is adequately modeled by
                  Birn et al.'s previously described respiration
                  response function. Furthermore, it is shown that
                  modeling out RV and HR can significantly alter
                  functional connectivity maps of the default-mode
                  network.}
}

@inproceedings{Che21,
  title =	 {Understanding Local Linearisation in Variational
                  {G}aussian Process State Space Models},
  cat =		 {gp np time},
  author =	 {Cheema, Talay M},
  url =		 {https://talaych.github.io/files/2021_tsw/paper.pdf},
  booktitle =	 {Time Series Workshop at the 38th International
                  Conference on Machine Learning},
  year =	 2021,
  abstract =	 {We describe variational inference approaches in
                  Gaussian process state space models in terms of
                  local linearisations of the approximate posterior
                  function. Most previous approaches have either
                  assumed independence between the posterior dynamics
                  and latent states (the mean-field (MF)
                  approximation), or optimised free parameters for
                  both, leading to limited scalability. We use our
                  framework to prove that (i) there is a theoretical
                  imperative to use non-MF approaches, to avoid
                  excessive bias in the process noise hyperparameter
                  estimate, and (ii) we can parameterise only the
                  posterior dynamics without any less of
                  performance. Our approach suggests further
                  approximations, based on the existing rich
                  literature on filtering and smoothing for nonlinear
                  systems, and unifies approaches for discrete and
                  continuous time models.}
}

@article{CheHorRic22,
  cat =		 {approx deep},
  title =	 {Optimal Client Sampling for Federated Learning},
  author =	 {Wenlin Chen and Samuel Horv{\'a}th and Peter
                  Richt{\'a}rik},
  journal =	 {Transactions on Machine Learning Research},
  year =	 2022,
  month =	 {August},
  url =		 {https://openreview.net/forum?id=8GvRCWKHIL},
  abstract =	 {It is well understood that client-master
                  communication can be a primary bottleneck in
                  federated learning (FL). In this work, we address
                  this issue with a novel client subsampling scheme,
                  where we restrict the number of clients allowed to
                  communicate their updates back to the master
                  node. In each communication round, all participating
                  clients compute their updates, but only the ones
                  with important updates communicate back to the
                  master. We show that importance can be measured
                  using only the norm of the update and give a formula
                  for optimal client participation.  This formula
                  minimizes the distance between the full update,
                  where all clients participate, and our limited
                  update, where the number of participating clients is
                  restricted. In addition, we provide a simple
                  algorithm that approximates the optimal formula for
                  client participation, which allows for secure
                  aggregation and stateless clients, and thus does not
                  compromise client privacy. We show both
                  theoretically and empirically that for Distributed
                  SGD (DSGD) and Federated Averaging (FedAvg), the
                  performance of our approach can be close to full
                  participation and superior to the baseline where
                  participating clients are sampled
                  uniformly. Moreover, our approach is orthogonal to
                  and compatible with existing methods for reducing
                  communication overhead, such as local methods and
                  communication compression methods.},
  annote =	 {<a
                  href="https://arxiv.org/abs/2010.13723">arXiv</a>}
}

@inproceedings{CheRas22,
  title =	 {Contrasting Discrete and Continuous Methods for
                  {B}ayesian System Identification},
  cat =		 {gp np time},
  author =	 {Cheema, Talay M},
  url =
                  {https://talaych.github.io/files/2022_ctmml/paper.pdf},
  booktitle =	 {Workshop on Continuous Time Machine Learning at the
                  39th International Conference on Machine Learning},
  year =	 2022,
  abstract =	 {In recent years, there has been considerable
                  interest in embedding continuous time methods in
                  machine learning algorithms. In system
                  identification, the task is to learn a dynamical
                  model from incomplete observation data, and when
                  prior knowledge is in continuous time – for example,
                  mechanistic differential equation models – it seems
                  natural to use continuous time models for
                  learning. Yet when learning flexible, nonlinear,
                  probabilistic dynamics models, most previous work
                  has focused on discrete time models to avoid
                  computational, numerical, and mathematical
                  difficulties. In this work we show, with the aid of
                  small-scale examples, that this mismatch between
                  model and data generating process can be
                  consequential under certain circumstances, and we
                  discuss possible modifications to discrete time
                  models which may better suit them to handling data
                  generated by continuous time processes.}
}

@article{CheTriHer22,
  cat =		 {gp deep bioinf},
  title =	 {Meta-learning Adaptive Deep Kernel {G}aussian
                  Processes for Molecular Property Prediction},
  author =	 {Wenlin Chen and Austin Tripp and Jos{\'e} Miguel
                  Hern{\'a}ndez-Lobato},
  journal =	 {arXiv},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2205.02708},
  abstract =	 {We propose Adaptive Deep Kernel Fitting with
                  Implicit Function Theorem (ADKF-IFT), a novel
                  framework for learning deep kernel Gaussian
                  processes (GPs) by interpolating between
                  meta-learning and conventional deep kernel
                  learning. Our approach employs a bilevel
                  optimization objective where we meta-learn generally
                  useful feature representations across tasks, in the
                  sense that task-specific GP models estimated on top
                  of such features achieve the lowest possible
                  predictive loss on average.  We solve the resulting
                  nested optimization problem using the implicit
                  function theorem (IFT). We show that our ADKF-IFT
                  framework contains previously proposed Deep Kernel
                  Learning (DKL) and Deep Kernel Transfer (DKT) as
                  special cases.  Although ADKF-IFT is a completely
                  general method, we argue that it is especially
                  well-suited for drug discovery problems and
                  demonstrate that it significantly outperforms
                  previous state-of-the-art methods on a variety of
                  real-world few-shot molecular property prediction
                  tasks and out-of-domain molecular property
                  prediction and optimization tasks.}
}

@inproceedings{ChoCheiDavLiketal20,
  cat =		 {deep rl approx},
  title =	 {Stochastic Flows and Geometric Optimization on the
                  Orthogonal Group},
  author =	 {Krzysztof Choromanski and David Cheikhi and Jared
                  Davis and Valerii Likhosherstov and Achille Nazaret
                  and Achraf Bahamou and Xingyou Song and Mrugank
                  Akarte and Jack Parker-Holder and Jacob Bergquist
                  and Yuan Gao and Aldo Pacchiano and Tamas Sarlos and
                  Adrian Weller and Vikas Sindhwani},
  booktitle =	 icml37,
  year =	 2020,
  url =		 {https://arxiv.org/abs/2003.13563},
  abstract =	 {We present a new class of stochastic,
                  geometrically-driven optimization algorithms on the
                  orthogonal group O(d) and naturally reductive
                  homogeneous manifolds obtained from the action of
                  the rotation group SO(d). We theoretically and
                  experimentally demonstrate that our methods can be
                  applied in various fields of machine learning
                  including deep, convolutional and recurrent neural
                  networks, reinforcement learning, normalizing flows
                  and metric learning. We show an intriguing
                  connection between efficient stochastic optimization
                  on the orthogonal group and graph theory
                  (e.g. matching problem, partition functions over
                  graphs, graph-coloring). We leverage the theory of
                  Lie groups and provide theoretical results for the
                  designed class of algorithms. We demonstrate broad
                  applicability of our methods by showing strong
                  performance on the seemingly unrelated tasks of
                  learning world models to obtain stable policies for
                  the most difficult Humanoid agent from OpenAI Gym
                  and improving convolutional neural networks.}
}

@inproceedings{
ChoLikDohetal21,
  cat =		 {nlp},
  title =	 {Rethinking Attention with Performers},
  author =	 {Krzysztof Marcin Choromanski and Valerii
                  Likhosherstov and David Dohan and Xingyou Song and
                  Andreea Gane and Tamas Sarlos and Peter Hawkins and
                  Jared Quincy Davis and Afroz Mohiuddin and Lukasz
                  Kaiser and David Benjamin Belanger and Lucy J
                  Colwell and Adrian Weller},
  booktitle =	 {International Conference on Learning
                  Representations},
  year =	 2021,
  url =		 {https://openreview.net/forum?id=Ua6zuk0WRH},
  abstract =	 {We introduce Performers, Transformer architectures
                  which can estimate regular (softmax)
                  full-rank-attention Transformers with provable
                  accuracy, but using only linear (as opposed to
                  quadratic) space and time complexity, without
                  relying on any priors such as sparsity or
                  low-rankness. To approximate softmax
                  attention-kernels, Performers use a novel Fast
                  Attention Via positive Orthogonal Random features
                  approach (FAVOR+), which may be of independent
                  interest for scalable kernel methods. FAVOR+ can
                  also be used to efficiently model kernelizable
                  attention mechanisms beyond softmax. This
                  representational power is crucial to accurately
                  compare softmax with other kernels for the first
                  time on large-scale tasks, beyond the reach of
                  regular Transformers, and investigate optimal
                  attention-kernels. Performers are linear
                  architectures fully compatible with regular
                  Transformers and with strong theoretical guarantees:
                  unbiased or nearly-unbiased estimation of the
                  attention matrix, uniform convergence and low
                  estimation variance. We tested Performers on a rich
                  set of tasks stretching from pixel-prediction
                  through text models to protein sequence modeling. We
                  demonstrate competitive results with other examined
                  efficient sparse and dense attention methods,
                  showcasing effectiveness of the novel
                  attention-learning paradigm leveraged by
                  Performers.},
}

@inproceedings{ChoRowCheWel19,
  cat =		 {mcmc},
  title =	 {Unifying Orthogonal {Monte Carlo} Methods},
  author =	 {Krzysztof Choromanski and Mark Rowland and Wenyu
                  Chen and Adrian Weller},
  booktitle =	 icml36,
  year =	 2019,
  month =	 {June},
  address =	 {Long Beach},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/ICML2019-unified.pdf},
  abstract =	 {Many machine learning methods making use of Monte
                  Carlo sampling in vector spaces have been shown to
                  be improved by conditioning samples to be mutually
                  orthogonal. Exact orthogonal coupling of samples is
                  computationally intensive, hence approximate methods
                  have been of great interest. In this paper, we
                  present a unifying perspective of many approximate
                  methods by considering Givens transformations,
                  propose new approximate methods based on this
                  framework, and demonstrate the first statistical
                  guarantees for families of approximate methods in
                  kernel approximation. We provide extensive empirical
                  evaluations with guidance for practitioners.}
}

@inproceedings{ChoRowSaretal18,
  cat =		 {mcmc approx},
  title =	 {The Geometry of Random Features},
  booktitle =	 aistats21,
  year =	 2018,
  month =	 {April},
  address =	 {Playa Blanca, Lanzarote, Canary Islands},
  author =	 {Krzysztof Choromanski and Mark Rowland and Tamas
                  Sarlos and Vikas Sindhwani and Richard E. Turner and
                  Adrian Weller},
  abstract =	 {We present an in-depth examination of the
                  effectiveness of radial basis function kernel
                  (beyond Gaussian) estimators based on orthogonal
                  random feature maps. We show that orthogonal
                  estimators outperform state-of-the-art mechanisms
                  that use iid sampling under weak conditions for
                  tails of the associated Fourier distributions. We
                  prove that for the case of many dimensions, the
                  superiority of the orthogonal transform can be
                  accurately measured by a property we define called
                  the charm of the kernel, and that orthogonal random
                  features provide optimal (in terms of mean squared
                  error) kernel estimators.  We provide the first
                  theoretical results which explain why orthogonal
                  random features outperform unstructured on
                  downstream tasks such as kernel ridge regression by
                  showing that orthogonal random features provide
                  kernel algorithms with better spectral properties
                  than the previous state-of-the-art.  Our results
                  enable practitioners more generally to estimate the
                  benefits from applying orthogonal transforms.},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/geometry.pdf}
}

@inproceedings{ChoRowSinTur18,
  cat =		 {deep},
  title =	 {Structured evolution with compact architectures for
                  scalable policy optimization},
  author =	 {Krzysztof Choromanski and Mark Rowland and Vikas
                  Sindhwani and Richard Turner and Adrian Weller},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm Sweden},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/structured_icml_full.pdf},
  abstract =	 {We present a new method of blackbox optimization via
                  gradient approximation with the use of structured
                  random orthogonal matrices, providing more accurate
                  estimators than baselines and with provable
                  theoretical guarantees. We show that this algorithm
                  can be successfully applied to learn better quality
                  compact policies than those using standard gradient
                  estimation techniques. The compact policies we learn
                  have several advantages over unstructured ones,
                  including faster training algorithms and faster
                  inference. These benefits are important when the
                  policy is deployed on real hardware with limited
                  resources. Further, compact policies provide more
                  scalable architectures for derivative-free
                  optimization (DFO) in high dimensional spaces. We
                  show that most robotics tasks from the OpenAI Gym
                  can be solved using neural networks with less than
                  300 parameters, with almost linear time complexity
                  of the inference phase, with up to 13x fewer
                  parameters relative to the Evolution Strategies (ES)
                  algorithm introduced by Salimans et al. (2017). We
                  do not need heuristics such as fitness shaping to
                  learn good quality policies, resulting in a simple
                  and theoretically motivated training mechanism.  }
}

@inproceedings{ChoRowWel17,
  cat =		 {gp mcmc approx},
  title =	 {The unreasonable effectiveness of structured random
                  orthogonal embeddings},
  booktitle =	 nips31,
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California},
  author =	 {Krzysztof Choromanski and Mark Rowland and Adrian
                  Weller},
  abstract =	 {We examine a class of embeddings based on structured
                  random matrices with orthogonal rows which can be
                  applied in many machine learning applications
                  including dimensionality reduction and kernel
                  approximation. For both the Johnson-Lindenstrauss
                  transform and the angular kernel, we show that we
                  can select matrices yielding guaranteed improved
                  performance in accuracy and/or speed compared to
                  earlier methods. We introduce matrices with complex
                  entries which give significant further accuracy
                  improvement. We provide geometric and Markov
                  chain-based perspectives to help understand the
                  benefits, and empirical results which suggest that
                  the approach is helpful in a wider range of
                  applications.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/NIPS17-unreasonable-effectiveness.pdf}
}

@article{ChuCunKauetal10,
  cat =		 {neuro},
  author =	 {M. M. Churchland and J. P. Cunningham and
                  M. T. Kaufman and S. I. Ryu and K. V. Shenoy.},
  title =	 {Cortical preparatory activity: Representation of
                  movement or first cog in a dynamical machine?},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/ChurchlandNeuron2010.pdf},
  journal =	 {Neuron},
  volume =	 68,
  pages =	 {387-400},
  year =	 2010,
  abstract =	 {The motor cortices are active during both movement
                  and movement preparation. A common assumption is
                  that preparatory activity constitutes a subthreshold
                  form of movement activity: a neuron active during
                  rightward movements becomes modestly active during
                  preparation of a rightward movement. We asked
                  whether this pattern of activity is, in fact,
                  observed. We found that it was not: at the level of
                  a single neuron, preparatory tuning was weakly
                  correlated with movement-period tuning. Yet,
                  somewhat paradoxically, preparatory tuning could be
                  captured by a preferred direction in an abstract
                  "space" that described the population-level pattern
                  of movement activity. In fact, this relationship
                  accounted for preparatory responses better than did
                  traditional tuning models. These results are
                  expected if preparatory activity provides the
                  initial state of a dynamical system whose evolution
                  produces movement activity. Our results thus suggest
                  that preparatory activity may not represent specific
                  factors, and may instead play a more mechanistic
                  role.}
}

@article{ChuGha05a,
  author =	 {Chu, Wei and Ghahramani, Zoubin},
  cat =		 {gp},
  journal =	 {Journal of Machine Learning Research},
  pages =	 {1019-1041},
  title =	 {Gaussian Processes for Ordinal Regression},
  url =		 {.},
  volume =	 6,
  year =	 2005,
  abstract =	 {We present a probabilistic kernel approach to
                  ordinal regression based on Gaussian processes. A
                  threshold model that generalizes the probit function
                  is used as the likelihood function for ordinal
                  variables. Two inference techniques, based on the
                  Laplace approximation and the expectation
                  propagation algorithm respectively, are derived for
                  hyperparameter learning and model selection. We
                  compare these two Gaussian process approaches with a
                  previous ordinal regression method based on support
                  vector machines on some benchmark and real-world
                  data sets, including applications of ordinal
                  regression to collaborative filtering and gene
                  expression analysis. Experimental results on these
                  data sets verify the usefulness of our approach.  }
}

@inproceedings{ChuGha05b,
  author =	 {Chu, Wei and Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {gp},
  editor =	 {Raedt, Luc De and Wrobel, Stefan},
  isbn =	 {1-59593-180-5},
  pages =	 {137-144},
  publisher =	 {acm},
  series =	 {ACM International Conference Proceeding Series},
  title =	 {Preference learning with {G}aussian processes},
  url =		 {.},
  volume =	 119,
  year =	 2005,
  abstract =	 {In this paper, we propose a probabilistic kernel
                  approach to preference learning based on Gaussian
                  processes. A new likelihood function is proposed to
                  capture the preference relations in the Bayesian
                  framework. The generalized formulation is also
                  applicable to tackle many multiclass problems. The
                  overall approach has the advantages of Bayesian
                  methods for model selection and probabilistic
                  prediction. Experimental results compared against
                  the constraint classification approach on several
                  benchmark datasets verify the usefulness of this
                  algorithm.}
}

@inproceedings{ChuGha09,
  volume =	 5,
  author =	 {W. Chu and Z. Ghahramani},
  note =	 {ISSN 1938-7228},
  booktitle =	 aistats12,
  editor =	 {D. van Dyk and M. Welling},
  title =	 {Probabilistic models for incomplete
                  multi-dimensional arrays},
  address =	 {Clearwater Beach, FL, USA},
  publisher =	 {Microtome Publishing (paper) Journal of Machine
                  Learning Research},
  year =	 2009,
  month =	 {April},
  pages =	 {89--96},
  url =		 {.},
  abstract =	 {In multiway data, each sample is measured by
                  multiple sets of correlated attributes. We develop a
                  probabilistic framework for modeling structural
                  dependency from partially observed multi-dimensional
                  array data, known as pTucker. Latent components
                  associated with individual array dimensions are
                  jointly retrieved while the core tensor is
                  integrated out. The resulting algorithm is capable
                  of handling large-scale data sets. We verify the
                  usefulness of this approach by comparing against
                  classical models on applications to modeling amino
                  acid fluorescence, collaborative filtering and a
                  number of benchmark multiway array data.}
}

@article{ChuGhaFal05a,
  author =	 {Chu, Wei and Ghahramani, Zoubin and Falciani,
                  Francesco and Wild, David L.},
  cat =		 {bioinf},
  journal =	 {Bioinformatics},
  number =	 16,
  pages =	 {3385-3393},
  title =	 {Biomarker discovery in microarray gene expression
                  data with {G}aussian processes},
  url =		 {.},
  volume =	 21,
  year =	 2005,
  abstract =	 {MOTIVATION: In clinical practice, pathological
                  phenotypes are often labelled with ordinal scales
                  rather than binary, e.g. the Gleason grading system
                  for tumour cell differentiation. However, in the
                  literature of microarray analysis, these ordinal
                  labels have been rarely treated in a principled
                  way. This paper describes a gene selection algorithm
                  based on Gaussian processes to discover consistent
                  gene expression patterns associated with ordinal
                  clinical phenotypes. The technique of automatic
                  relevance determination is applied to represent the
                  significance level of the genes in a Bayesian
                  inference framework.  RESULTS: The usefulness of the
                  proposed algorithm for ordinal labels is
                  demonstrated by the gene expression signature
                  associated with the Gleason score for prostate
                  cancer data. Our results demonstrate how multi-gene
                  markers that may be initially developed with a
                  diagnostic or prognostic application in mind are
                  also useful as an investigative tool to reveal
                  associations between specific molecular and cellular
                  events and features of tumour physiology. Our
                  algorithm can also be applied to microarray data
                  with binary labels with results comparable to other
                  methods in the literature.}
}

@inproceedings{ChuGhaKra06a,
  author =	 {Chu, Wei and Ghahramani, Zoubin and Krause, Roland
                  and Wild, David L.},
  booktitle =	 {Pacific Symposium on Biocomputing},
  cat =		 {bioinf},
  editor =	 {Altman, Russ B. and Murray, Tiffany and Klein, Teri
                  E. and Dunker, A. Keith and Hunter, Lawrence},
  isbn =	 {981-256-463-2},
  pages =	 {231-242},
  publisher =	 {World Scientific},
  title =	 {Identifying Protein Complexes in High-Throughput
                  Protein Interaction Screens Using an Infinite Latent
                  Feature Model},
  url =		 {.},
  year =	 2006,
  abstract =	 {We propose a Bayesian approach to identify protein
                  complexes and their constituents from
                  high-throughput protein-protein interaction
                  screens. An infinite latent feature model that
                  allows for multi-complex membership by individual
                  proteins is coupled with a graph diffusion kernel
                  that evaluates the likelihood of two proteins
                  belonging to the same complex. Gibbs sampling is
                  then used to infer a catalog of protein complexes
                  from the interaction screen data. An advantage of
                  this model is that it places no prior constraints on
                  the number of complexes and automatically infers the
                  number of significant complexes from the
                  data. Validation results using affinity
                  purification/mass spectrometry experimental data
                  from yeast RNA-processing complexes indicate that
                  our method is capable of partitioning the data in a
                  biologically meaningful way. A supplementary web
                  site containing larger versions of the figures is
                  available at
                  http://public.kgi.edu/wild/PSBO6/index.html.  }
}

@article{ChuGhaPod06a,
  author =	 {Chu, Wei and Ghahramani, Zoubin and Podtelezhnikov,
                  Alexei A. and Wild, David L.},
  cat =		 {bioinf},
  journal =	 {IEEE/ACM Trans. Comput. Biology Bioinform.},
  number =	 2,
  pages =	 {98-113},
  title =	 {Bayesian Segmental Models with Multiple Sequence
                  Alignment Profiles for Protein Secondary Structure
                  and Contact Map Prediction},
  url =		 {.},
  volume =	 3,
  year =	 2006,
  abstract =	 {In this paper, we develop a segmental semi-Markov
                  model (SSMM) for protein secondary structure
                  prediction which incorporates multiple sequence
                  alignment profiles with the purpose of improving the
                  predictive performance. The segmental model is a
                  generalization of the hidden Markov model where a
                  hidden state generates segments of various length
                  and secondary structure type. A novel parameterized
                  model is proposed for the likelihood function that
                  explicitly represents multiple sequence alignment
                  profiles to capture the segmental
                  conformation. Numerical results on benchmark data
                  sets show that incorporating the profiles results in
                  substantial improvements and the generalization
                  performance is promising. By incorporating the
                  information from long range interactions in
                  beta-sheets, this model is also capable of carrying
                  out inference on contact maps. This is an important
                  advantage of probabilistic generative models over
                  the traditional discriminative approach to protein
                  secondary structure prediction. The Web server of
                  our algorithm and supplementary materials are
                  available at http://public.kgi.edu/-wild/bsm.html.}
}

@inproceedings{ChuGhaWil04a,
  author =	 {Chu, Wei and Ghahramani, Zoubin and Wild, David L.},
  booktitle =	 {ICML},
  cat =		 {bioinf},
  editor =	 {Brodley, Carla E.},
  publisher =	 {acm},
  series =	 {ACM International Conference Proceeding Series},
  title =	 {A graphical model for protein secondary structure
                  prediction},
  url =		 {.},
  volume =	 69,
  year =	 2004,
  abstract =	 {In this paper, we present a graphical model for
                  protein secondary structure prediction. This model
                  extends segmental semi-Markov models (SSMM) to
                  exploit multiple sequence alignment profiles which
                  contain information from evolutionarily related
                  sequences. A novel parameterized model is proposed
                  as the likelihood function for the SSMM to capture
                  the segmental conformation. By incorporating the
                  information from long range interactions in
                  β-sheets, this model is capable of carrying out
                  inference on contact maps. The numerical results on
                  benchmark data sets show that incorporating the
                  profiles results in substantial improvements and the
                  generalization performance is promising.}
}

@inproceedings{ChuGhaWil04b,
  author =	 {Chu, Wei and Ghahramani, Zoubin and Wild, David L.},
  booktitle =	 {ESANN},
  cat =		 {bioinf},
  pages =	 {81-86},
  title =	 {Protein secondary structure prediction using sigmoid
                  belief networks to parameterize segmental
                  semi-{M}arkov models},
  url =		 {.},
  year =	 2004,
  abstract =	 {In this paper, we merge the parametric structure of
                  neural networks into a segmental semi-Markov model
                  to set up a Bayesian framework for protein structure
                  prediction. The parametric model, which can also be
                  regarded as an extension of a sigmoid belief
                  network, captures the underlying dependency in
                  residue sequences. The results of numerical
                  experiments indicate the usefulness of this
                  approach. }
}

@inproceedings{ChuSinGhaetal07,
  cat =		 {gp},
  volume =	 19,
  month =	 {September},
  author =	 {W. Chu and V. Sindhwani and Z. Ghahramani and
                  S. Keerthi},
  series =	 {Bradford Books},
  note =	 {Online contents gives pages 314--321, and 289--296
                  on pdf of contents},
  booktitle =	 nips19,
  editor =	 {B. Sch\"olkopf and J. Platt and T. Hofmann},
  title =	 {Relational learning with {G}aussian processes},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  year =	 2007,
  pages =	 {289--296},
  url =		 {.},
  abstract =	 {Correlation between instances is often modelled via
                  a kernel function using input attributes of the
                  instances. Relational knowledge can further reveal
                  additional pairwise correlations between variables
                  of interest. In this paper, we develop a class of
                  models which incorporates both reciprocal relational
                  information and input attributes using Gaussian
                  process techniques. This approach provides a novel
                  non-parametric Bayesian framework with a
                  data-dependent prior for supervised learning
                  tasks. We also apply this framework to
                  semi-supervised learning. Experimental results on
                  several real world data sets verify the usefulness
                  of this algorithm.}
}

@article{ChuYuCunetal10,
  cat =		 {gp neuro},
  author =	 {M. M. Churchland and B. M. Yu and J. P. Cunningham
                  and L. P. Sugrue and M. R. Cohen and G. S. Corrado
                  and W. T. Newsome and A. M. Clark and P. Hosseini
                  and B. B. Scott and D. C. Bradley and M. A. Smith
                  and A. Kohn and J. A. Movshon and K. M. Armstrong
                  and T. Moore and S. W. Chang and L. H. Snyder and
                  S. G. Lisberger and N. J. Priebe and I. M. Finn and
                  D. Ferster and S. I. Ryu and G. Santhanam and
                  M. Sahani and K. V. Shenoy.},
  title =	 {Stimulus onset quashes neural variability: a
                  widespread cortical phenomenon},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/ChurchlandNN2010.pdf},
  journal =	 {Nature Neuroscience},
  volume =	 13,
  pages =	 {369-378},
  year =	 2010,
  abstract =	 {Neural responses are typically characterized by
                  computing the mean firing rate, but response
                  variability can exist across trials. Many studies
                  have examined the effect of a stimulus on the mean
                  response, but few have examined the effect on
                  response variability. We measured neural variability
                  in 13 extracellularly recorded datasets and one
                  intracellularly recorded dataset from seven areas
                  spanning the four cortical lobes in monkeys and
                  cats. In every case, stimulus onset caused a decline
                  in neural variability. This occurred even when the
                  stimulus produced little change in mean firing
                  rate. The variability decline was observed in
                  membrane potential recordings, in the spiking of
                  individual neurons and in correlated spiking
                  variability measured with implanted 96-electrode
                  arrays. The variability decline was observed for all
                  stimuli tested, regardless of whether the animal was
                  awake, behaving or anaesthetized. This widespread
                  variability decline suggests a rather general
                  property of cortex, that its state is stabilized by
                  an input.}
}

@inproceedings{CilHerIaletal18,
  author =	 {Carlo Ciliberto and Mark Herbster and Alessandro
                  Davide Ialongo and Massimiliano Pontil and Andrea
                  Rocchetto and Simone Severini and Leonard Wossnig},
  title =	 {Quantum machine learning: a classical perspective},
  booktitle =	 {Proc. R. Soc. A},
  cat =		 {review},
  volume =	 474,
  issue =	 2209,
  pages =	 20170551,
  doi =		 {10.1098/rspa.2017.0551},
  year =	 2018,
  organization = {The Royal Society},
  abstract =	 {Recently, increased computational power and data
                  availability, as well as algorithmic advances, have
                  led machine learning techniques to impressive
                  results in regression, classification,
                  data-generation and reinforcement learning
                  tasks. Despite these successes, the proximity to the
                  physical limits of chip fabrication alongside the
                  increasing size of datasets are motivating a growing
                  number of researchers to explore the possibility of
                  harnessing the power of quantum computation to
                  speed-up classical machine learning algorithms. Here
                  we review the literature in quantum machine learning
                  and discuss perspectives for a mixed readership of
                  classical machine learning and quantum computation
                  experts. Particular emphasis will be placed on
                  clarifying the limitations of quantum algorithms,
                  how they compare with their best classical
                  counterparts and why quantum resources are expected
                  to provide advantages for learning
                  problems. Learning in the presence of noise and
                  certain computationally hard problems in machine
                  learning are identified as promising directions for
                  the field. Practical questions, like how to upload
                  classical data into quantum form, will also be
                  addressed.  }
}

@inproceedings{ClaOldHer22,
  cat =		 {deep},
  title =	 {Scalable One-Pass Optimisation of High-Dimensional
                  Weight-Update Hyperparameters by Implicit
                  Differentiation},
  booktitle =	 iclr10,
  year =	 2022,
  month =	 {April},
  address =	 {Virtual},
  author =	 {Ross M. Clarke and Elre T. Oldewage and Jos\'e
                  Miguel Hern\'andez-Lobato},
  url =		 {https://openreview.net/forum?id=hfU7Ka5cfrC},
  abstract =	 {Machine learning training methods depend plentifully
                  and intricately on hyperparameters, motivating
                  automated strategies for their optimisation. Many
                  existing algorithms restart training for each new
                  hyperparameter choice, at considerable computational
                  cost. Some hypergradient- based one-pass methods
                  exist, but these either cannot be applied to
                  arbitrary optimiser hyperparameters (such as
                  learning rates and momenta) or take several times
                  longer to train than their base models. We extend
                  these existing methods to develop an approximate
                  hypergradient-based hyperparameter optimiser which
                  is applicable to any continuous hyperparameter
                  appearing in a differentiable model weight update,
                  yet requires only one training episode, with no
                  restarts. We also provide a motivating argument for
                  convergence to the true hypergradient, and perform
                  tractable gradient-based optimisation of independent
                  learning rates for each model parameter. Our method
                  performs competitively from varied random
                  hyperparameter initialisations on several UCI
                  datasets and Fashion-MNIST (using a one-layer MLP),
                  Penn Treebank (using an LSTM) and CIFAR-10 (using a
                  ResNet-18), in time only 2-3x greater than vanilla
                  training.}
}

@inproceedings{CohGhaJor94a,
  author =	 {Cohn, David A. and Ghahramani, Zoubin and Jordan,
                  Michael I.},
  booktitle =	 nips7,
  cat =		 {active},
  editor =	 {Tesauro, Gerald and Touretzky, David S. and Leen,
                  Todd K.},
  pages =	 {705-712},
  publisher =	 {MIT Press},
  title =	 {Active Learning with Statistical Models},
  url =		 {.},
  year =	 1994,
  abstract =	 {For many types of machine learning algorithms, one
                  can compute the statistically ``optimal'' way to
                  select training data. In this paper, we review how
                  optimal data selection techniques have been used
                  with feedforward neural networks. We then show how
                  the same principles may be used to select data for
                  two alternative, statistically-based learning
                  architectures: mixtures of Gaussians and locally
                  weighted regression. While the techniques for neural
                  networks are computationally expensive and
                  approximate, the techniques for mixtures of
                  Gaussians and locally weighted regression are both
                  efficient and accurate. Empirically, we observe that
                  the optimality criterion sharply decreases the
                  number of training examples the learner needs in
                  order to achieve good performance.  }
}

@article{CohGhaJor96b,
  author =	 {Cohn, David A. and Ghahramani, Zoubin and Jordan,
                  Michael I.},
  cat =		 {active},
  journal =	 {J. Artif. Intell. Res. (JAIR)},
  pages =	 {129-145},
  title =	 {Active Learning with Statistical Models},
  url =		 {.},
  volume =	 4,
  year =	 1996,
  abstract =	 {For many types of machine learning algorithms, one
                  can compute the statistically ``optimal'' way to
                  select training data. In this paper, we review how
                  optimal data selection techniques have been used
                  with feedforward neural networks. We then show how
                  the same principles may be used to select data for
                  two alternative, statistically-based learning
                  architectures: mixtures of Gaussians and locally
                  weighted regression. While the techniques for neural
                  networks are computationally expensive and
                  approximate, the techniques for mixtures of
                  Gaussians and locally weighted regression are both
                  efficient and accurate. Empirically, we observe that
                  the optimality criterion sharply decreases the
                  number of training examples the learner needs in
                  order to achieve good performance.  }
}

@InProceedings{CokBruBuretal2022,
  title =	 {Wide Mean-Field {B}ayesian Neural Networks Ignore
                  the Data},
  author =	 {Coker, Beau and Bruinsma, Wessel P. and Burt, David
                  R. and Pan, Weiwei and Doshi-Velez, Finale},
  booktitle =	 aistats25,
  year =	 2022,
  abstract =	 {Bayesian neural networks (BNNs) combine the
                  expressive power of deep learning with the
                  advantages of Bayesian formalism. In recent years,
                  the analysis of wide, deep BNNs has provided
                  theoretical insight into their priors and
                  posteriors. However, we have no analogous insight
                  into their posteriors under approximate
                  inference. In this work, we show that mean-field
                  variational inference <em>entirely fails to model
                  the data</em> when the network width is large and
                  the activation function is odd. Specifically, for
                  fully-connected BNNs with odd activation functions
                  and a homoscedastic Gaussian likelihood, we show
                  that the <em>optimal</em> mean-field variational
                  posterior predictive (i.e., function space)
                  distribution converges to the prior predictive
                  distribution as the width tends to infinity. We
                  generalize aspects of this result to other
                  likelihoods. Our theoretical results are suggestive
                  of underfitting behavior previously observered in
                  BNNs. While our convergence bounds are
                  non-asymptotic and constants in our analysis can be
                  computed, they are currently too loose to be
                  applicable in standard training regimes. Finally, we
                  show that the optimal approximate posterior need not
                  tend to the prior if the activation function is not
                  odd, showing that our statements cannot be
                  generalized arbitrarily.},
  cat =		 {approx deep},
  url =
                  {https://proceedings.mlr.press/v151/coker22a/coker22a.pdf}
}

@inproceedings{ColBhaWel22,
  title =	 {Eliciting and Learning with Soft Labels from Every
                  Annotator},
  author =	 {Katherine M. Collins and Umang Bhatt and Adrian
                  Weller},
  booktitle =	 {Proceedings of the AAAI Conference on Human
                  Computation and Crowdsourcing (HCOMP)},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2207.00810},
  abstract =	 {The labels used to train machine learning (ML)
                  models are of paramount importance. Typically for ML
                  classification tasks, datasets contain hard labels,
                  yet learning using soft labels has been shown to
                  yield benefits for model generalization, robustness,
                  and calibration. Earlier work found success in
                  forming soft labels from multiple annotators' hard
                  labels; however, this approach may not converge to
                  the best labels and necessitates many annotators,
                  which can be expensive and inefficient. We focus on
                  efficiently eliciting soft labels from individual
                  annotators. We collect and release a dataset of soft
                  labels (which we call CIFAR-10S) over the CIFAR-10
                  test set via a crowdsourcing study (N=248). We
                  demonstrate that learning with our labels achieves
                  comparable model performance to prior approaches
                  while requiring far fewer annotators -- albeit with
                  significant temporal costs per elicitation. Our
                  elicitation methodology therefore shows nuanced
                  promise in enabling practitioners to enjoy the
                  benefits of improved model performance and
                  reliability with fewer annotators, and serves as a
                  guide for future dataset curators on the benefits of
                  leveraging richer information, such as categorical
                  uncertainty, from individual annotators.},
  doi =		 {10.17863/CAM.87954},
  annote =	 {[<a
                  href="https://sites.google.com/view/eliciting-individ-soft-labels">Project
                  Page</a>] [<a
                  href="https://github.com/cambridge-mlg/cifar-10s/tree/master/cifar10s_data">Data</a>]
                  [<a
                  href="https://github.com/cambridge-mlg/cifar-10s">Code</a>]
                  }
}

@article{CooZilDeSetal22,
  cat =		 {nlp},
  author =	 {Cook, Darren and Zilka, Miri and DeSandre, Heidi and
                  Giles, Susan and Weller, Adrian and Maskell, Simon},
  title =	 {Can We Automate the Analysis of Online Child Sexual
                  Exploitation Discourse?},
  journal =	 {arXiv},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2209.12320},
  abstract =	 {Social media's growing popularity raises concerns
                  around children's online safety. Interactions
                  between minors and adults with predatory intentions
                  is a particularly grave concern. Research into
                  online sexual grooming has often relied on domain
                  experts to manually annotate conversations, limiting
                  both scale and scope. In this work, we test how
                  well-automated methods can detect conversational
                  behaviors and replace an expert human
                  annotator. Informed by psychological theories of
                  online grooming, we label 6772 chat messages sent by
                  child-sex offenders with one of eleven predatory
                  behaviors. We train bag-of-words and natural
                  language inference models to classify each behavior,
                  and show that the best performing models classify
                  behaviors in a manner that is consistent, but not
                  on-par, with human annotation.}
}

@article{CooZilMasetal21,
  cat =		 {nlp},
  author =	 {Darren Cook and Miri Zilka and Simon Maskell and
                  Laurence Alison},
  title =	 {A Psychology-Driven Computational Analysis of
                  Political Interviews},
  journal =	 {Proc. Interspeech},
  year =	 2021,
  url =
                  {https://www.isca-speech.org/archive/pdfs/interspeech_2021/cook21_interspeech.pdf},
  abstract =	 {Can an interviewer influence the cooperativeness of
                  an interviewee? The role of an interviewer in
                  actualising a successful interview is an active
                  field of social psychological research. A
                  large-scale analysis of interviews, however,
                  typically involves time-exorbitant manual tasks and
                  considerable human effort. Despite recent advances
                  in computational fields, many automated methods
                  continue to rely on manually labelled training data
                  to establish ground-truth. This reliance obscures
                  explainability and hinders the mobility of analysis
                  between applications. In this work, we introduce a
                  cross-disciplinary approach to analysing interviewer
                  efficacy. We suggest computational success measures
                  as a transparent, automated, and reproducible
                  alternative for pre-labelled data. We validate these
                  measures with a small-scale study with
                  human-responders. To study the interviewer’s
                  influence on the interviewee we utilise features
                  informed by social psychological theory to predict
                  interview quality based on the interviewer’s
                  linguistic behaviour. Our psychologically informed
                  model significantly outperforms a bag-of-words
                  model, demonstrating the strength of a
                  cross-disciplinary approach toward the analysis of
                  conversational data at scale.}
}

@techreport{Cun08,
  cat =		 {gp approx neuro},
  title =	 {Derivation of {E}xpectation {P}ropagation for "Fast
                  {G}aussian process methods for point process
                  intensity estimation"},
  author =	 {J. P. Cunningham},
  year =	 2008,
  institution =	 {Stanford University},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/CunninghamEPTR2008.pdf},
  abstract =	 {We derive the Expectation Propagation algorithm
                  updates for approximating the posterior distribution
                  on intensity in a conditionally inhomogeneous gamma
                  interval process with a Gaussian Process prior (GP
                  IGIP), a model which appeared in Cunningham, Shenoy,
                  Sahani (2008) ICML.}
}

@inproceedings{CunGhaRas12,
  cat =		 {gp time},
  author =	 {John P. Cunningham and Zoubin Ghahramani and Carl
                  Edward Rasmussen},
  year =	 2012,
  title =	 {{Gaussian Processes} for time-marked time-series
                  data},
  booktitle =	 aistats15,
  abstract =	 {In many settings, data is collected as multiple time
                  series, where each recorded time series is an
                  observation of some underlying dynamical process of
                  interest. These observations are often time-marked
                  with known event times, and one desires to do a
                  range of standard analyses. When there is only one
                  time marker, one simply aligns the observations
                  temporally on that marker. When multiple
                  time-markers are present and are at different times
                  on different time series observations, these
                  analyses are more difficult. We describe a Gaussian
                  Process model for analyzing multiple time series
                  with multiple time markings, and we test it on a
                  variety of data.},
  pages =	 {255--263},
  url =		 {.}
}

@article{CunNuyGiletal11,
  cat =		 {time neuro},
  author =	 {J. P. Cunningham and P. Nuyujukian and V. Gilja and
                  C. A. Chestek and S. I. Ryu and K. V. Shenoy.},
  title =	 {A closed-loop human simulator for investigating the
                  role of feedback-control in brain-machine
                  interfaces},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/CunninghamJNP2011.pdf},
  journal =	 {Journal of Neurophysiology},
  volume =	 105,
  pages =	 {1932-1949},
  year =	 2011,
  abstract =	 {Neural prosthetic systems seek to improve the lives
                  of severely disabled people by decoding neural
                  activity into useful behavioral commands. These
                  systems and their decoding algorithms are typically
                  developed "offline", using neural activity
                  previously gathered from a healthy animal, and the
                  decoded movement is then compared with the true
                  movement that accompanied the recorded neural
                  activity. However, this offline design and testing
                  may neglect important features of a real prosthesis,
                  most notably the critical role of feedback control,
                  which enables the user to adjust neural activity
                  while using the prosthesis. We hypothesize that
                  under- standing and optimally designing
                  high-performance decoders require an experimental
                  platform where humans are in closed-loop with the
                  various candidate decode systems and algorithms. It
                  remains unexplored the extent to which the subject
                  can, for a particular decode system, algorithm, or
                  parameter, engage feedback and other strategies to
                  improve decode performance. Closed-loop testing may
                  suggest different choices than offline
                  analyses. Here we ask if a healthy human subject,
                  using a closed-loop neural prosthesis driven by
                  synthetic neural activity, can inform system
                  design. We use this online pros- thesis simulator
                  (OPS) to optimize "online" decode performance based
                  on a key parameter of a current state-of-the-art
                  decode algorithm, the bin width of a Kalman
                  filter. First, we show that offline and online
                  analyses indeed suggest different parameter
                  choices. Previous literature and our offline
                  analyses agree that neural activity should be
                  analyzed in bins of 100- to 300-ms width. OPS
                  analysis, which incorporates feedback control,
                  suggests that much shorter bin widths (25-50 ms)
                  yield higher decode performance. Second, we confirm
                  this surprising finding using a closed-loop rhesus
                  monkey prosthetic system. These findings illustrate
                  the type of discovery made possible by the OPS, and
                  so we hypothesize that this novel testing approach
                  will help in the design of prosthetic systems that
                  will translate well to human patients.}
}

@inproceedings{CunSheSah08,
  cat =		 {gp neuro},
  author =	 {J. P. Cunningham and K. V. Shenoy and M. Sahani},
  booktitle =	 icml25,
  title =	 {Fast {G}aussian process methods for point process
                  intensity estimation},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/CunninghamICML2008.pdf},
  year =	 2008,
  address =	 {Helsinki, Finland},
  month =	 {June},
  pages =	 {1--8},
  abstract =	 {Point processes are difficult to analyze because
                  they provide only a sparse and noisy observation of
                  the intensity function driving the process. Gaussian
                  Processes offer an attractive framework within which
                  to infer underlying intensity functions. The result
                  of this inference is a continuous function defined
                  across time that is typically more amenable to
                  analytical efforts. However, a naive implementation
                  will become computationally infeasible in any
                  problem of reasonable size, both in memory and run
                  time requirements. We demonstrate problem specific
                  methods for a class of renewal processes that
                  eliminate the memory burden and reduce the solve
                  time by orders of magnitude.}
}

@inproceedings{CunYuSheetal08,
  cat =		 {gp neuro},
  author =	 {J. P. Cunningham and B. M. Yu and K. V. Shenoy and
                  M. Sahani},
  booktitle =	 nips20,
  title =	 {Inferring neural firing rates from spike trains
                  using {G}aussian processes},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/CunninghamNIPS2008.pdf},
  year =	 2008,
  address =	 {Vancouver, BC},
  month =	 {December},
  pages =	 {1--8},
  abstract =	 {Neural spike trains present challenges to analytical
                  efforts due to their noisy, spiking nature. Many
                  studies of neuroscientific and neural prosthetic
                  importance rely on a smoothed, denoised estimate of
                  the spike train's underlying firing rate. Current
                  techniques to find time-varying firing rates require
                  ad hoc choices of parameters, offer no confidence
                  intervals on their estimates, and can obscure
                  potentially important single trial variability. We
                  present a new method, based on a Gaussian Process
                  prior, for inferring probabilistically optimal
                  estimates of firing rate functions underlying single
                  or multiple neural spike trains. We test the
                  performance of the method on simulated data and
                  experimentally gathered neural spike trains, and we
                  demonstrate improvements over conventional
                  estimators.},
  annote =	 {Spotlight Presentation}
}

@inproceedings{DabRowBelMun18,
  cat =		 {rl},
  title =	 {Distributional Reinforcement Learning with Quantile
                  Regression},
  booktitle =	 aaai32,
  year =	 2018,
  month =	 {February},
  address =	 {New Orleans},
  author =	 {Will Dabney and Mark Rowland and Marc G. Bellemare
                  and R{\'e}mi Munos},
  abstract =	 {In reinforcement learning an agent interacts with
                  the environment by taking actions and observing the
                  next state and reward. When sampled
                  probabilistically, these state transitions, rewards,
                  and actions can all induce randomness in the
                  observed long-term return. Traditionally,
                  reinforcement learning algorithms average over this
                  randomness to estimate the value function. In this
                  paper, we build on recent work advocating a
                  distributional approach to reinforcement learning in
                  which the distribution over returns is modeled
                  explicitly instead of only estimating the mean. That
                  is, we examine methods of learning the value
                  distribution instead of the value function. We give
                  results that close a number of gaps between the
                  theoretical and algorithmic results given by
                  Bellemare, Dabney, and Munos (2017). First, we
                  extend existing results to the approximate
                  distribution setting. Second, we present a novel
                  distributional reinforcement learning algorithm
                  consistent with our theoretical
                  formulation. Finally, we evaluate this new algorithm
                  on the Atari 2600 games, observing that it
                  significantly outperforms many of the recent
                  improvements on DQN, including the related
                  distributional algorithm C51.},
  url =		 {https://arxiv.org/abs/1710.10044},
}

@phdthesis{Dav15,
  author =	 {Alex Davies},
  cat =		 {gp},
  title =	 {Effective implementation of {G}aussian process
                  regression for machine learning},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2015,
  abstract =	 {This thesis presents frameworks for the effective
                  implementation of Gaussian process regression for
                  machine learning. It addresses this in three parts:
                  effective iterative methods for calculating the
                  predictive distribution and derivatives of a
                  Gaussian process with fixed hyper-parameters,
                  defining three broad classes of kernels of
                  controllable complexity that allow for an order of
                  magnitude scaling in the previous framework and an
                  investigation into alternative objective functions
                  and improved derivatives for the optimization of
                  model hyper-parameters.},
  address =	 {Cambridge, UK},
  url =		 {.}
}

@inproceedings{DavGha11,
  month =	 {August},
  cat =		 {nlp},
  author =	 {Davies, A. and Ghahramani, Z.},
  title =	 {Language-independent {B}ayesian sentiment mining of
                  Twitter},
  year =	 2011,
  booktitle =	 {In {\em The Fifth Workshop on Social Network Mining
                  and Analysis (SNA-KDD 2011)}},
  url =
                  {http://www.alexdavies.net/wordpress/wp-content/uploads/2011/09/Language-Indepedent-Bayesian-Sentiment-Mining-of-Twitter.pdf},
  abstract =	 {This paper outlines a new language-independent model
                  for sentiment analysis of short, social-network
                  statuses. We demonstrate this on data from Twitter,
                  modelling happy vs sad sentiment, and show that in
                  some circumstances this outperforms similar Naive
                  Bayes models by more than 10\%.  We also propose an
                  extension to allow the modelling of differ- ent
                  sentiment distributions in different geographic
                  regions, while incorporating information from
                  neighbouring regions.  We outline the considerations
                  when creating a system analysing Twitter data and
                  present a scalable system of data acquisi- tion and
                  prediction that can monitor the sentiment of tweets
                  in real time.}
}

@article{DavGha14a,
  author =	 {Davies, Alex and Ghahramani, Zoubin},
  cat =		 {gp},
  journal =	 {arXiv},
  title =	 {The Random Forest Kernel and other kernels for big
                  data from random partitions},
  url =		 {.},
  volume =	 {abs/1402.4293},
  year =	 2014,
  abstract =	 {We present Random Partition Kernels, a new class of
                  kernels derived by demonstrating a natural
                  connection between random partitions of objects and
                  kernels between those objects. We show how the
                  construction can be used to create kernels from
                  methods that would not normally be viewed as random
                  partitions, such as Random Forest. To demonstrate
                  the potential of this method, we propose two new
                  kernels, the Random Forest Kernel and the Fast
                  Cluster Kernel, and show that these kernels
                  consistently outperform standard kernels on problems
                  involving real-world datasets. Finally, we show how
                  the form of these kernels lend themselves to a
                  natural approximation that is appropriate for
                  certain big data problems, allowing O(N) inference
                  in methods such as Gaussian Processes, Support
                  Vector Machines and Kernel PCA.}
}

@inproceedings{DaxNalAllEtal21,
  cat =		 {approx deep},
  author =	 {Erik A. Daxberger and Eric T. Nalisnick and James
                  Urquhart Allingham and Javier Antor\'an and Jos\'e
                  Miguel Hern\'andez-Lobato},
  editor =	 {Marina Meila and Tong Zhang},
  title =	 {Bayesian Deep Learning via Subnetwork Inference},
  booktitle =	 icml32,
  series =	 {Proceedings of Machine Learning Research},
  volume =	 139,
  pages =	 {2510--2521},
  publisher =	 {{PMLR}},
  year =	 2021,
  url =		 {http://proceedings.mlr.press/v139/daxberger21a.html},
  abstract =	 {The Bayesian paradigm has the potential to solve
                  core issues of deep neural networks such as poor
                  calibration and data inefficiency. Alas, scaling
                  Bayesian inference to large weight spaces often
                  requires restrictive approximations. In this work,
                  we show that it suffices to perform inference over a
                  small subset of model weights in order to obtain
                  accurate predictive posteriors. The other weights
                  are kept as point estimates. This subnetwork
                  inference framework enables us to use expressive,
                  otherwise intractable, posterior approximations over
                  such subsets.  In particular, we implement
                  subnetwork linearized Laplace: We first obtain a MAP
                  estimate of all weights and then infer a
                  full-covariance Gaussian posterior over a
                  subnetwork. We propose a subnetwork selection
                  strategy that aims to maximally preserve the model's
                  predictive uncertainty. Empirically, our approach is
                  effective compared to ensembles and less expressive
                  posterior approximations over full networks.},
}

@phdthesis{Dei10,
  cat =		 {gp rl},
  author =	 {Marc Peter Deisenroth},
  title =	 {Efficient Reinforcement Learning using {G}aussian
                  Processes},
  school =	 {Karlsruhe Institute of Technology},
  address =	 {Karlsruhe, Germany},
  year =	 2010,
  url =		 {.},
  abstract =	 {In many research areas, including control and
                  medical applications, we face decision-making
                  problems where data are limited and/or the
                  underlying generative process is complicated and
                  partially unknown. In these scenarios, we can profit
                  from algorithms that learn from data and aid
                  decision making.<br> Reinforcement learning (RL) is
                  a general computational approach to experience-based
                  goal-directed learning for sequential decision
                  making under uncertainty. However, RL often lacks
                  efficiency in terms of the number of required trials
                  when no task-specific knowledge is available. This
                  lack of efficiency makes RL often inapplicable to
                  (optimal) control problems. Thus, a central issue in
                  RL is to speed up learning by extracting more
                  information from available experience.<br> The
                  contributions of this dissertation are
                  threefold:<br> 1. We propose PILCO, a fully Bayesian
                  approach for efficient RL in continuous-valued state
                  and action spaces when no expert knowledge is
                  available. PILCO is based on well-established ideas
                  from statistics and machine learning. PILCO's key
                  ingredient is a probabilistic dynamics model learned
                  from data, which is implemented by a Gaussian
                  process (GP). The GP carefully quantifies knowledge
                  by a probability distribution over plausible
                  dynamics models. By averaging over all these models
                  during long-term planning and decision making, PILCO
                  takes uncertainties into account in a principled way
                  and, therefore, reduces model bias, a central
                  problem in model-based RL.<br> 2. Due to its
                  generality and efficiency, PILCO can be considered a
                  conceptual and practical approach to jointly
                  learning models and controllers when expert
                  knowledge is difficult to obtain or simply not
                  available. For this scenario, we investigate PILCO's
                  properties its applicability to challenging real and
                  simulated nonlinear control problems. For example,
                  we consider the tasks of learning to swing up a
                  double pendulum attached to a cart or to balance a
                  unicycle with five degrees of freedom. Across all
                  tasks we report unprecedented automation and an
                  unprecedented learning efficiency for solving these
                  tasks.<br> 3. As a step toward pilco's extension to
                  partially observable Markov decision processes, we
                  propose a principled algorithm for robust filtering
                  and smoothing in GP dynamic systems. Unlike commonly
                  used Gaussian filters for nonlinear systems, it does
                  neither rely on function linearization nor on
                  finite-sample representations of densities.  Our
                  algorithm profits from exact moment matching for
                  predictions while keeping all computations
                  analytically tractable. We present experimental
                  evidence that demonstrates the robustness and the
                  advantages of our method over unscented Kalman
                  filters, the cubature Kalman filter, and the
                  extended Kalman filter.}
}

@article{DeiFoxRas15,
  cat =		 {gp rl},
  author =	 {Marc Peter Deisenroth and Dieter Fox and Carl Edward
                  Rasmussen},
  title =	 {Gaussian Processes for Data-Efficient Learning in
                  Robotics and Control},
  journal =	 PAMI,
  pages =	 {408--423},
  volume =	 37,
  issue =	 2,
  year =	 2015,
  abstract =	 {Autonomous learning has been a promising direction
                  in control and robotics for more than a decade since
                  data-driven learning allows to reduce the amount of
                  engineering knowledge, which is otherwise
                  required. However, autonomous reinforcement learning
                  (RL) approaches typically require many interactions
                  with the system to learn controllers, which is a
                  practical limitation in real systems, such as
                  robots, where many interactions can be impractical
                  and time consuming. To address this problem, current
                  learning approaches typically require task-speciﬁc
                  knowledge in form of expert demonstrations,
                  realistic simulators, pre-shaped policies, or
                  speciﬁc knowledge about the underlying dynamics. In
                  this article, we follow a different approach and
                  speed up learning by extracting more information
                  from data. In particular, we learn a probabilistic,
                  non-parametric Gaussian process transition model of
                  the system.  By explicitly incorporating model
                  uncertainty into long-term planning and controller
                  learning our approach reduces the effects of model
                  errors, a key problem in model-based
                  learning. Compared to state-of-the art RL our
                  model-based policy search method achieves an
                  unprecedented speed of learning. We demonstrate its
                  applicability to autonomous learning in real robot
                  and control tasks.},
  doi =		 {10.1109/TPAMI.2013.218}
}

@inproceedings{DeiHubHan09,
  cat =		 {gp time},
  author =	 {Marc Peter Deisenroth and Marco F.~Huber and Uwe
                  D.~Hanebeck},
  title =	 {Analytic Moment-based {G}aussian Process Filtering},
  booktitle =	 icml26,
  year =	 2009,
  editor =	 {L\'{e}on Bottou and Michael Littman},
  pages =	 {225--232},
  address =	 {Montr\'{e}al, QC, Canada},
  month =	 {June},
  publisher =	 {Omnipress},
  url =		 {.},
  abstract =	 {We propose an analytic moment-based filter for
                  nonlinear stochastic dynamic systems modeled by
                  Gaussian processes. Exact expressions for the
                  expected value and the covariance matrix are
                  provided for both the prediction step and the filter
                  step, where an additional Gaussian assumption is
                  exploited in the latter case. Our filter does not
                  require further approximations. In particular, it
                  avoids finite-sample approximations. We compare the
                  filter to a variety of Gaussian filters, that is,
                  the EKF, the UKF, and the recent GP-UKF proposed by
                  <a
                  href="http://www.cs.washington.edu/homes/fox/postscripts/gp-ukf-iros-07.pdf">Ko
                  et al. (2007)</a>.},
  annote =	 {With corrections. <a
                  href="http://mlg.eng.cam.ac.uk/marc/downloads/gpadf.zip">code</a>.}
}

@inproceedings{DeiPetRas08,
  cat =		 {gp approx},
  author =	 {Marc Peter Deisenroth and Jan Peters and Carl Edward
                  Rasmussen},
  title =	 {Approximate Dynamic Programming with {G}aussian
                  Processes},
  booktitle =	 {2008 American Control Conference (ACC 2008)},
  year =	 2008,
  pages =	 {4480--4485},
  address =	 {Seattle, WA, USA},
  month =	 {June},
  url =		 {.},
  abstract =	 {In general, it is difficult to determine an optimal
                  closed-loop policy in nonlinear control problems
                  with continuous-valued state and control
                  domains. Hence, approximations are often
                  inevitable. The standard method of discretizing
                  states and controls suffers from the curse of
                  dimensionality and strongly depends on the chosen
                  temporal sampling rate. The paper introduces
                  Gaussian Process Dynamic Programming (GPDP).  In
                  GPDP, value functions in the Bellman recursion of
                  the dynamic programming algorithm are modeled using
                  Gaussian processes. GPDP returns an optimal
                  state-feedback for a finite set of states. Based on
                  these outcomes, we learn a possibly discontinuous
                  closed-loop policy on the entire state space by
                  switching between two independently trained Gaussian
                  processes.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/marc/code/acc2008.zip">code</a>.}
}

@inproceedings{DeiRas09,
  cat =		 {rl},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen},
  title =	 {Bayesian Inference for Efficient Learning in
                  Control},
  booktitle =	 {Multidisciplinary Symposium on Reinforcement
                  Learning},
  year =	 2009,
  address =	 {Montr\'{e}al, QC, Canada},
  month =	 {June},
  url =		 {.},
  abstract =	 {In contrast to humans or animals, artificial
                  learners often require more trials when learning
                  motor control tasks solely based on experience.
                  Efficient autonomous learners will reduce the amount
                  of engineering required to solve control
                  problems. By using probabilistic forward models, we
                  can employ two key ingredients of biological
                  learning systems to speed up artificial learning. We
                  present a consistent and coherent Bayesian framework
                  that allows for efficient autonomous
                  experience-based learning. We demonstrate the
                  success of our learning algorithm by applying it to
                  challenging nonlinear control problems in simulation
                  and in hardware.}
}

@inproceedings{DeiRas09b,
  cat =		 {rl},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen},
  title =	 {Efficient Reinforcement Learning for Motor Control},
  booktitle =	 {10th International PhD Workshop on Systems and
                  Control},
  year =	 2009,
  address =	 {Hlubok\'{a} nad Vltavou, Czech Republic},
  month =	 {September},
  url =		 {.},
  abstract =	 {Artificial learners often require many more trials
                  than humans or animals when learning motor control
                  tasks in the absence of expert knowledge. We
                  implement two key ingredients of biological learning
                  systems, generalization and incorporation of
                  uncertainty into the decision-making process, to
                  speed up artificial learning. We present a coherent
                  and fully Bayesian framework that allows for
                  efficient artificial learning in the absence of
                  expert knowledge. The success of our learning
                  framework is demonstrated on challenging nonlinear
                  control problems in simulation and in hardware.}
}

@inproceedings{DeiRas11,
  cat =		 {gp rl},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen},
  title =	 {{PILCO}: {A} Model-Based and Data-Efficient Approach
                  to Policy Search},
  booktitle =	 icml28,
  year =	 2011,
  abstract =	 {In this paper, we introduce PILCO, a practical,
                  data-efficient model-based policy search
                  method. PILCO reduces model bias, one of the key
                  problems of model-based reinforcement learning, in a
                  principled way. By learning a probabilistic dynamics
                  model and explicitly incorporating model uncertainty
                  into long-term planning, PILCO can cope with very
                  little data and facilitates learning from scratch in
                  only a few trials. Policy evaluation is performed in
                  closed form using state-of-the-art approximate
                  inference. Furthermore, policy gradients are
                  computed analytically for policy improvement. We
                  report unprecedented learning efficiency on
                  challenging and high-dimensional control tasks.},
  url =		 {.},
  annote =	 {<a href="http://mlg.eng.cam.ac.uk/pilco">web
                  site</a>}
}

@inproceedings{DeiRasFox11,
  cat =		 {rl},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen and
                  Dieter Fox},
  title =	 {Learning to Control a Low-Cost Manipulator using
                  Data-Efficient Reinforcement Learning},
  booktitle =	 rss9,
  year =	 2011,
  month =	 {June},
  address =	 {Los Angeles, CA, USA},
  abstract =	 {Over the last years, there has been substantial
                  progress in robust manipulation in unstructured
                  environments. The long-term goal of our work is to
                  get away from precise, but very expensive robotic
                  systems and to develop affordable, potentially
                  imprecise, self-adaptive manipulator systems that
                  can interactively perform tasks such as playing with
                  children. In this paper, we demonstrate how a
                  low-cost off-the-shelf robotic system can learn
                  closed-loop policies for a stacking task in only a
                  handful of trials - from scratch. Our manipulator is
                  inaccurate and provides no pose feedback. For
                  learning a controller in the work space of a
                  Kinect-style depth camera, we use a model-based
                  reinforcement learning technique. Our learning
                  method is data efficient, reduces model bias, and
                  deals with several noise sources in a principled way
                  during long-term planning. We present a way of
                  incorporating state-space constraints into the
                  learning process and analyze the learning gain by
                  exploiting the sequential structure of the stacking
                  task.},
  url =		 {.},
  annote =	 {<a
                  href="http://www.cs.washington.edu/ai/Mobile_Robotics/projects/robot-rl">project
                  site</a>}
}

@inproceedings{DeiRasPet08,
  cat =		 {rl gp},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen and
                  Jan Peters},
  title =	 {Model-Based Reinforcement Learning with Continuous
                  States and Actions},
  booktitle =	 {Proceedings of the 16th European Symposium on
                  Artificial Neural Networks (ESANN 2008)},
  year =	 2008,
  pages =	 {19--24},
  address =	 {Bruges, Belgium},
  month =	 {April},
  url =		 {.},
  abstract =	 {Finding an optimal policy in a reinforcement
                  learning (RL) framework with continuous state and
                  action spaces is challenging. Approximate solutions
                  are often inevitable. GPDP is an approximate dynamic
                  programming algorithm based on Gaussian process (GP)
                  models for the value functions.  In this paper, we
                  extend GPDP to the case of unknown transition
                  dynamics.  After building a GP model for the
                  transition dynamics, we apply GPDP to this model and
                  determine a continuous-valued policy in the entire
                  state space. We apply the resulting controller to
                  the underpowered pendulum swing up. Moreover, we
                  compare our results on this RL task to a nearly
                  optimal discrete DP solution in a fully known
                  environment.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/marc/code/esann2008.zip">code</a>. <a
                  href="http://mlg.eng.cam.ac.uk/marc/talks/2008-04-23-ESANN-bruges.pdf">slides</a>}
}

@article{DeiRasPet09,
  cat =		 {rl gp},
  volume =	 72,
  number =	 {7--9},
  month =	 {March},
  author =	 {Marc Peter Deisenroth and Carl Edward Rasmussen and
                  Jan Peters},
  title =	 {Gaussian process dynamic programming},
  publisher =	 {Elsevier B. V.},
  year =	 2009,
  journal =	 {Neurocomputing},
  pages =	 {1508--1524},
  url =		 {.},
  abstract =	 {Reinforcement learning (RL) and optimal control of
                  systems with continuous states and actions require
                  approximation techniques in most interesting
                  cases. In this article, we introduce Gaussian
                  process dynamic programming (GPDP), an approximate
                  value function-based RL algorithm. We consider both
                  a classic optimal control problem, where
                  problem-specific prior knowledge is available, and a
                  classic RL problem, where only very general priors
                  can be used. For the classic optimal control
                  problem, GPDP models the unknown value functions
                  with Gaussian processes and generalizes dynamic
                  programming to continuous-valued states and
                  actions. For the RL problem, GPDP starts from a
                  given initial state and explores the state space
                  using Bayesian active learning. To design a fast
                  learner, available data have to be used
                  efficiently. Hence, we propose to learn
                  probabilistic models of the a priori unknown
                  transition dynamics and the value functions on the
                  fly. In both cases, we successfully apply the
                  resulting continuous-valued controllers to the
                  under-actuated pendulum swing up and analyze the
                  performances of the suggested algorithms. It turns
                  out that GPDP uses data very efficiently and can be
                  applied to problems, where classic dynamic
                  programming would be cumbersome.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/marc/code/algpdp.zip">code</a>.},
  doi =		 {10.1016/j.neucom.2008.12.019}
}

@article{DeiTurHubetal12,
  cat =		 {gp time},
  author =	 {Marc Peter Deisenroth and Ryan D.~T<!>urner and
                  Marco F.~Huber and Uwe D.~Hanebeck and Carl Edward
                  Rasmussen},
  title =	 {Robust Filtering and Smoothing with {G}aussian
                  Processes},
  journal =	 IETAC,
  volume =	 57,
  year =	 2012,
  number =	 7,
  pages =	 {1865--1871},
  abstract =	 {We propose a principled algorithm for robust
                  Bayesian filtering and smoothing in nonlinear
                  stochastic dynamic systems when both the transition
                  function and the measurement function are described
                  by nonparametric Gaussian process (GP) models. GPs
                  are gaining increasing importance in signal
                  processing, machine learning, robotics, and control
                  for representing unknown system functions by
                  posterior probability distributions. This modern way
                  of "system identification" is more robust than
                  finding point estimates of a parametric function
                  representation. Our principled filtering/smoothing
                  approach for GP dynamic systems is based on analytic
                  moment matching in the context of the
                  forward-backward algorithm. Our numerical
                  evaluations demonstrate the robustness of the
                  proposed approach in situations where other
                  state-of-the-art Gaussian filters and smoothers can
                  fail.},
  url =		 {.},
  doi =		 {10.1109/TAC.2011.2179426}
}

@inproceedings{DimBhaJamWel20,
  cat =		 {fairness interpretability deep},
  title =	 {You shouldn't trust me: Learning models which
                  conceal unfairness from multiple explanation
                  methods},
  author =	 {Botty Dimanov and Umang Bhatt and Mateja Jamnik and
                  Adrian Weller},
  booktitle =	 {European Conference on Artificial Intelligence
                  (ECAI)},
  year =	 2020,
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/ECAI20-You_Shouldn%E2%80%99t_Trust_Me.pdf},
  abstract =	 {Transparency of algorithmic systems has been
                  discussed as a way for end-users and regulators to
                  develop appropriate trust in machine learning
                  models. One popular approach, LIME [26], even
                  suggests that model explanations can answer the
                  question ``Why should I trust you?'' Here we show a
                  straightforward method for modifying a pre-trained
                  model to manipulate the output of many popular
                  feature importance explanation methods with little
                  change in accuracy, thus demonstrating the danger of
                  trusting such explanation methods. We show how this
                  explanation attack can mask a model’s discriminatory
                  use of a sensitive feature, raising strong concerns
                  about using such explanation methods to check model
                  fairness.}
}

@mastersthesis{Dos09,
  cat =		 {np},
  author =	 {Finale Doshi-Velez},
  title =	 {The {I}ndian Buffet Process: {S}calable Inference
                  and Extensions},
  school =	 {University of Cambridge},
  year =	 2009,
  address =	 {Cambridge, UK},
  month =	 {August},
  url =		 {.},
  abstract =	 {Many unsupervised learning problems seek to identify
                  hidden features from observations. In many
                  real-world situations, the number of hidden features
                  is unknown. To avoid specifying the number of hidden
                  features a priori, one can use the Indian Buffet
                  Process (IBP): a nonparametric latent feature model
                  that does not bound the number of active features in
                  a dataset. While elegant, the lack of efficient
                  inference procedures for the IBP has prevented its
                  application in large-scale problems.  The core
                  contribution of this thesis are three new inference
                  procedures that allow inference in the IBP to be
                  scaled from a few hundred to 100,000 observations.
                  This thesis contains three parts: (1) An
                  introduction to the IBP and a review of inference
                  techniques and extensions. The first chapters
                  summarise three constructions for the IBP and review
                  all currently published inference techniques.
                  Appendix C reviews extensions of the IBP to date.
                  (2) Novel techniques for scalable Bayesian
                  inference. This thesis presents three new inference
                  procedures: (a) an accelerated Gibbs sampler for
                  efficient Bayesian inference in a broad class of
                  conjugate models, (b) a parallel, asynchronous Gibbs
                  sampler that allows the accelerated Gibbs sampler to
                  be distributed across multiple processors, and (c) a
                  variational inference procedure for the IBP.  (3) A
                  framework for structured nonparametric latent
                  feature models.  We also present extensions to the
                  IBP to model more sophisticated relationships
                  between the co-occurring hidden features, providing
                  a general framework for correlated non-parametric
                  feature models.}
}

@inproceedings{Dos09b,
  author =	 {Finale Doshi-Velez},
  title =	 {The Infinite Partially Observable {M}arkov Decision
                  Process},
  booktitle =	 nips23,
  year =	 2009,
  address =	 {Cambridge, MA, USA},
  month =	 {December},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {The Partially Observable Markov Decision Process
                  (POMDP) framework has proven useful in planning
                  domains where agents must balance actions that
                  provide knowledge and actions that provide
                  reward. Unfortunately, most POMDPs are complex
                  structures with a large number of parameters.  In
                  many real-world problems, both the structure and the
                  parameters are difficult to specify from domain
                  knowledge alone. Recent work in Bayesian
                  reinforcement learning has made headway in learning
                  POMDP models; however, this work has largely focused
                  on learning the parameters of the POMDP model. We
                  define an infinite POMDP (iPOMDP) model that does
                  not require knowledge of the size of the state
                  space; instead, it assumes that the number of
                  visited states will grow as the agent explores its
                  world and only models visited states explicitly. We
                  demonstrate the iPOMDP on several standard
                  problems.}
}

@inproceedings{DosGha09,
  cat =		 {np mcmc},
  booktitle =	 icml26,
  title =	 {Accelerated {Gibbs} sampling for the {Indian} buffet
                  process},
  author =	 {Finale Doshi-Velez and Zoubin Ghahramani},
  year =	 2009,
  address =	 {Montr\'{e}al, QC, Canada},
  month =	 {June},
  publisher =	 {Omnipress},
  pages =	 {273--280},
  editor =	 {L\'{e}on Bottou and Michael Littman},
  url =		 {.},
  abstract =	 {We often seek to identify co-occurring hidden
                  features in a set of observations. The Indian Buffet
                  Process (IBP) provides a non-parametric prior on the
                  features present in each observation, but current
                  inference techniques for the IBP often scale
                  poorly. The collapsed Gibbs sampler for the IBP has
                  a running time cubic in the number of observations,
                  and the uncollapsed Gibbs sampler, while linear, is
                  often slow to mix. We present a new linear-time
                  collapsed Gibbs sampler for conjugate likelihood
                  models and demonstrate its efficacy on large
                  real-world datasets.}
}

@inproceedings{DosGha09a,
  author =	 {Doshi-Velez, Finale and Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {np, mcmc},
  editor =	 {Danyluk, Andrea Pohoreckyj and Bottou, L{\'e}on and
                  Littman, Michael L.},
  isbn =	 {978-1-60558-516-1},
  pages =	 35,
  publisher =	 {acm},
  series =	 {ACM International Conference Proceeding Series},
  title =	 {Accelerated sampling for the {I}ndian Buffet
                  Process},
  url =		 {.},
  volume =	 382,
  year =	 2009,
  abstract =	 {We often seek to identify co-occurring hidden
                  features in a set of observations. The Indian Buffet
                  Process (IBP) provides a nonparametric prior on the
                  features present in each observation, but current
                  inference techniques for the IBP often scale
                  poorly. The collapsed Gibbs sampler for the IBP has
                  a running time cubic in the number of observations,
                  and the uncollapsed Gibbs sampler, while linear, is
                  often slow to mix. We present a new linear-time
                  collapsed Gibbs sampler for conjugate likelihood
                  models and demonstrate its efficacy on large
                  real-world datasets.}
}

@inproceedings{DosGha09b,
  cat =		 {np},
  booktitle =	 {Conference on Uncertainty in Artificial Intelligence
                  (UAI 2009)},
  title =	 {Correlated non-parametric latent feature models},
  author =	 {F. Doshi-Velez and Z. Ghahramani},
  year =	 2009,
  address =	 {Montr\'{e}al, QC, Canada},
  month =	 {June},
  pages =	 {143--150},
  publisher =	 {AUAI Press},
  url =		 {.},
  abstract =	 {We are often interested in explaining data through a
                  set of hidden factors or features.  To allow for an
                  unknown number of such hidden features, one can use
                  the IBP: a non-parametric latent feature model that
                  does not bound the number of active features in a
                  dataset. However, the IBP assumes that all latent
                  features are uncorrelated, making it inadequate for
                  many real-world problems.  We introduce a framework
                  for correlated non-parametric feature models,
                  generalising the IBP. We use this framework to
                  generate several specific models and demonstrate
                  applications on real-world datasets.}
}

@inproceedings{DosGha11,
  cat =		 {rl},
  author =	 {Finale Doshi-Velez and Zoubin Ghahramani},
  year =	 2011,
  title =	 {A Comparison of Human and Agent Reinforcement
                  Learning in Partially Observable Domains},
  booktitle =	 {33rd Annual Meeting of the Cognitive Science
                  Society},
  address =	 {Boston, MA},
  url =		 {.},
  abstract =	 {It is commonly stated that reinforcement learning
                  (RL) algorithms learn slower than humans. In this
                  work, we investigate this claim using two standard
                  problems from the RL literature. We compare the
                  performance of human subjects to RL techniques. We
                  find that context---the meaningfulness of the
                  observations—--plays a significant role in the rate
                  of human RL. Moreover, without contextual
                  information, humans often fare much worse than
                  classic algorithms. Comparing the detailed responses
                  of humans and RL algorithms, we also find that
                  humans appear to employ rather different strategies
                  from standard algorithms, even in cases where they
                  had indistinguishable performance to them. Our
                  research both sheds light on human RL and provides
                  insights for improving RL algorithms.}
}

@inproceedings{DosKnoMohGha09,
  cat =		 {np approx},
  url =		 {.},
  author =	 {Finale Doshi-Velez and David Knowles and Shakir
                  Mohamed and Zoubin Ghahramani},
  title =	 {Large Scale Non-parametric Inference: {D}ata
                  Parallelisation in the {I}ndian Buffet Process},
  booktitle =	 nips23,
  pages =	 {1294--1302},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  year =	 2009,
  month =	 {December},
  abstract =	 {Nonparametric Bayesian models provide a framework
                  for flexible probabilistic modelling of complex
                  datasets. Unfortunately, the high-dimensional
                  averages required for Bayesian methods can be slow,
                  especially with the unbounded representations used
                  by nonparametric models. We address the challenge of
                  scaling Bayesian inference to the increasingly large
                  datasets found in real-world applications.  We focus
                  on parallelisation of inference in the Indian Buffet
                  Process (IBP), which allows data points to have an
                  unbounded number of sparse latent features. Our
                  novel MCMC sampler divides a large data set between
                  multiple processors and uses message passing to
                  compute the global likelihoods and posteriors.  This
                  algorithm, the first parallel inference scheme for
                  IBP-based models, scales to datasets orders of
                  magnitude larger than have previously been
                  possible.}
}

@inproceedings{DosMilVanTeh09,
  cat =		 {np},
  author =	 {Doshi-Velez, F. and Miller, K.T. and {Van Gael},
                  J. and Teh, Y.W.},
  booktitle =	 aistats12,
  keywords =	 {Factor Analysis,IBP,Variational},
  mendeley-tags ={Factor Analysis,IBP,Variational},
  pages =	 {137--144},
  publisher =	 jmlr,
  address =	 {Clearwater Beach, FL, USA},
  title =	 {Variational inference for the {I}ndian buffet
                  process},
  url =		 {.},
  volume =	 12,
  year =	 2009,
  month =	 {April},
  abstract =	 {The Indian Buffet Process (IBP) is a nonparametric
                  prior for latent feature models in which
                  observations are influenced by a combination of
                  hidden features. For example, images may be composed
                  of several objects and sounds may consist of several
                  notes. Latent feature models seek to infer these
                  unobserved features from a set of observations; the
                  IBP provides a principled prior in situations where
                  the number of hidden features is unknown. Current
                  inference methods for the IBP have all relied on
                  sampling. While these methods are guaranteed to be
                  accurate in the limit, samplers for the IBP tend to
                  mix slowly in practice. We develop a deterministic
                  variational method for inference in the IBP based on
                  a truncated stick-breaking approximation, provide
                  theoretical bounds on the truncation error, and
                  evaluate our method in several data regimes.}
}

@techreport{DosMilVanTeh09b,
  cat =		 {np},
  author =	 {Finale Doshi-Velez and Kurt T. Miller and Jurgen
                  {Van Gael} and Yee Whye Teh},
  title =	 {Variational Inference for the {I}ndian Buffet
                  Process},
  institution =	 {University of Cambridge},
  year =	 2009,
  number =	 {CBL-2009-001},
  address =	 {Computational and Biological Learning Laboratory,
                  Department of Engineering},
  month =	 {April},
  url =		 {.},
  abstract =	 {The Indian Buffet Process (IBP) is a nonparametric
                  prior for latent feature models in which
                  observations are influenced by a combination of
                  hidden features. For example, images may be composed
                  of several objects and sounds may consist of several
                  notes. Latent feature models seek to infer these
                  unobserved features from a set of observations; the
                  IBP provides a principled prior in situations where
                  the number of hidden features is unknown. Current
                  inference methods for the IBP have all relied on
                  sampling. While these methods are guaranteed to be
                  accurate in the limit, samplers for the IBP tend to
                  mix slowly in practice. We develop a deterministic
                  variational method for inference in the IBP based on
                  truncating to infinite models, provide theoretical
                  bounds on the truncation error, and evaluate our
                  method in several data regimes. This technical
                  report is a longer version of <a
                  href="/pub/#DosMilVanTeh09">Doshi-Velez et
                  al. (2009)</a>.}
}

@article{DosRoy08,
  author =	 {Finale Doshi and Nicholas R<!>oy},
  title =	 {Spoken Language Interaction with Model Uncertainty:
                  {A}n Adaptive Human-Robot Interaction System},
  journal =	 {Connection Science},
  year =	 2008,
  volume =	 20,
  pages =	 {290--318},
  number =	 4,
  month =	 {December},
  url =		 {.},
  abstract =	 {Spoken language is one of the most intuitive forms
                  of interaction between humans and
                  agents. Unfortunately, agents that interact with
                  people using natural language often experience
                  communication errors and do not correctly understand
                  the user's intentions. Recent systems have
                  successfully used probabilistic models of speech,
                  language, and user behavior to generate robust
                  dialog performance in the presence of noisy speech
                  recognition and ambiguous language choices, but
                  decisions made using these probabilistic models are
                  still prone to errors due to the complexity of
                  acquiring and maintaining a complete model of human
                  language and behavior.  In this paper, we describe a
                  decision-theoretic model for human-robot interaction
                  using natural language. Our algorithm is based on
                  the Partially Observable Markov Decision Process
                  (POMDP), which allows agents to choose actions that
                  are robust not only to uncertainty from noisy or
                  ambiguous speech recognition but also unknown user
                  models. Like most dialog systems, a POMDP is defined
                  by a large number of parameters that may be
                  difficult to specify a priori from domain knowledge,
                  and learning these parameters from the user may
                  require an unacceptably long training period. We
                  describe an extension to the POMDP model that allows
                  the agent to acquire a linguistic model of the user
                  online, including new vocabulary and word choice
                  preferences.  Our approach not only avoids a
                  training period of constant questioning as the agent
                  learns, but also allows the agent to actively query
                  for additional information when its uncertainty
                  suggests a high risk of mistakes. We demonstrate our
                  approach both in simulation and on a natural
                  language interaction system for a robotic wheelchair
                  application.}
}

@inproceedings{DubHwaRan04a,
  author =	 {Dubey, Ananya and Hwang, Seungwoo and Rangel,
                  Claudia and Rasmussen, Carl Edward and Ghahramani,
                  Zoubin and Wild, David L.},
  booktitle =	 {Pacific Symposium on Biocomputing},
  cat =		 {bioinf},
  editor =	 {Altman, Russ B. and Dunker, A. Keith and Hunter,
                  Lawrence and Jung, Tiffany A. and Klein, Teri E.},
  isbn =	 {981-238-598-3},
  pages =	 {399-410},
  publisher =	 {World Scientific},
  title =	 {Clustering Protein Sequence and Structure Space with
                  Infinite {G}aussian Mixture Models},
  url =		 {.},
  year =	 2004,
  abstract =	 {We describe a novel approach to the problem of
                  automatically clustering protein sequences and
                  discovering protein families, subfamilies etc.,
                  based on the theory of infinite Gaussian mixtures
                  models. This method allows the data itself to
                  dictate how many mixture components are required to
                  model it, and provides a measure of the probability
                  that two proteins belong to the same cluster. We
                  illustrate our methods with application to three
                  data sets: globin sequences, globin sequences with
                  known three-dimensional structures and G-protein
                  coupled receptor sequences. The consistency of the
                  clusters indicate that our method is producing
                  biologically meaningful results, which provide a
                  very good indication of the underlying families and
                  subfamilies. With the inclusion of secondary
                  structure and residue solvent accessibility
                  information, we obtain a classification of sequences
                  of known structure which both reflects and extends
                  their SCOP classifications. A supplementray web site
                  containing larger versions of the figures is
                  available at http://public.kgi.edu/approximately
                  wid/PSB04/index.html }
}

@inproceedings{DubHwaRanetal04,
  cat =		 {np bioinf clust},
  author =	 {A.~Dubey and S.~Hwang and C.~Rangel and Carl Edward
                  Rasmussen and Zoubin Ghahramani and David L.~Wild},
  title =	 {Clustering Protein Sequence and Structure Space with
                  Infinite {G}aussian Mixture Models},
  year =	 2004,
  publisher =	 {World Scientific Publishing},
  pages =	 {399--410},
  journal =	 {Pacific Symposium on Biocomputing 2004; Vol. 9},
  address =	 {Singapore},
  abstract =	 {We describe a novel approach to the problem of
                  automatically clustering protein sequences and
                  discovering protein families, subfamilies etc.,
                  based on the thoery of infinite Gaussian mixture
                  models. This method allows the data itself to
                  dictate how many mixture components are required to
                  model it, and provides a measure of the probability
                  that two proteins belong to the same cluster. We
                  illustrate our methods with application to three
                  data sets: globin sequences, globin sequences with
                  known tree-dimensional structures and G-pretein
                  coupled receptor sequences. The consistency of the
                  clusters indicate that that our methods is producing
                  biologically meaningful results, which provide a
                  very good indication of the underlying families and
                  subfamilies. With the inclusion of secondary
                  structure and residue solvent accessibility
                  information, we obtain a classification of sequences
                  of known structure which reflects and extends their
                  SCOP classifications.},
  url =		 {.},
  booktitle =	 {Pacific Symposium on Biocomputing 2004},
  location =	 {The Big Island of Hawaii}
}

@inproceedings{DutDurHen20,
  cat =		 {gp},
  title =	 {Sparse {G}aussian Processes with Spherical Harmonic
                  Features},
  author =	 {Vincent Dutordoir and Nicolas Durrande and James
                  Hensman},
  booktitle =	 icml37,
  year =	 2020,
  month =	 {June},
  address =	 {Online},
  url =
                  {http://proceedings.mlr.press/v119/dutordoir20a/dutordoir20a.pdf},
  abstract =	 {We introduce a new class of inter-domain variational
                  Gaussian processes (GP) where data is mapped onto
                  the unit hypersphere in order to use spherical
                  harmonic representations. Our inference scheme is
                  comparable to variational Fourier features, but it
                  does not suffer from the curse of dimensionality,
                  and leads to diagonal covariance matrices between
                  inducing variables. This enables a speed-up in
                  inference, because it bypasses the need to invert
                  large covariance matrices. Our experiments show that
                  our model is able to fit a regression model for a
                  dataset with 6 million entries two orders of
                  magnitude faster compared to standard sparse GPs,
                  while retaining state of the art accuracy. We also
                  demonstrate competitive performance on
                  classification with non-conjugate likelihoods.}
}

@inproceedings{DutHenVdwEtal21,
  cat =		 {gp deep},
  title =	 {Deep Neural Networks as Point Estimates for Deep
                  {G}aussian Processes},
  author =	 {Vincent Dutordoir and James Hensman and Mark van der
                  Wilk and Carl Henrik Ek and Zoubin Ghahramani and
                  Nicolas Durrande},
  booktitle =	 nips34,
  year =	 2021,
  month =	 {Dec},
  address =	 {Online},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf},
  abstract =	 {Neural networks and Gaussian processes are
                  complementary in their strengths and
                  weaknesses. Having a better understanding of their
                  relationship comes with the promise to make each
                  method benefit from the strengths of the other. In
                  this work, we establish an equivalence between the
                  forward passes of neural networks and (deep) sparse
                  Gaussian process models. The theory we develop is
                  based on interpreting activation functions as
                  interdomain inducing features through a rigorous
                  analysis of the interplay between activation
                  functions and kernels. This results in models that
                  can either be seen as neural networks with improved
                  uncertainty prediction or deep Gaussian processes
                  with increased prediction accuracy. These claims are
                  supported by experimental results on regression and
                  classification datasets.}
}

@inproceedings{DutSalDeiHen20,
  cat =		 {gp},
  title =	 {{G}aussian Process Conditional Density Estimation},
  author =	 {Vincent Dutordoir and Hugh Salimbeni and Marc
                  Deisenroth and James Hensman},
  booktitle =	 nips32,
  year =	 2018,
  month =	 {Dec},
  address =	 {Montréal, Canada},
  url =
                  {https://proceedings.neurips.cc/paper/2018/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
  abstract =	 {Conditional Density Estimation (CDE) models deal
                  with estimating conditional distributions. The
                  conditions imposed on the distribution are the
                  inputs of the model. CDE is a challenging task as
                  there is a fundamental trade-off between model
                  complexity, representational capacity and
                  overfitting. In this work, we propose to extend the
                  model's input with latent variables and use Gaussian
                  processes (GP) to map this augmented input onto
                  samples from the conditional distribution. Our
                  Bayesian approach allows for the modeling of small
                  datasets, but we also provide the machinery for it
                  to be applied to big data using stochastic
                  variational inference. Our approach can be used to
                  model densities even in sparse data regions, and
                  allows for sharing learned structure between
                  conditions. We illustrate the effectiveness and
                  wide-reaching applicability of our model on a
                  variety of real- world problems, such as
                  spatio-temporal density estimation of taxi
                  drop-offs, non-Gaussian noise modeling, and few-shot
                  learning on omniglot images.}
}

@inproceedings{DutSauGhaSim22,
  cat =		 {gp deep},
  title =	 {Neural Diffusion Processes},
  author =	 {Vincent Dutordoir and Alan Saul and Zoubin
                  Ghahramani and Fergus Simpson},
  booktitle =	 {arXiv},
  year =	 2022,
  month =	 {Apr},
  address =	 {Online},
  url =		 {https://arxiv.org/pdf/2206.03992.pdf},
  abstract =	 {Gaussian processes provide an elegant framework for
                  specifying prior and posterior distributions over
                  functions. They are, however, also computationally
                  expensive, and limited by the expressivity of their
                  covariance function. We propose Neural Diffusion
                  Processes (NDPs), a novel approach based upon
                  diffusion models, that learn to sample from
                  distributions over functions. Using a novel
                  attention block, we can incorporate properties of
                  stochastic processes, such as exchangeability,
                  directly into the NDP's architecture. We empirically
                  show that NDPs are able to capture functional
                  distributions that are close to the true Bayesian
                  posterior of a Gaussian process. This enables a
                  variety of downstream tasks, including
                  hyperparameter marginalisation and Bayesian
                  optimisation.}
}

@inproceedings{DuvLloGroetal13,
  cat =		 {gp np time sigproc},
  title =	 {Structure Discovery in Nonparametric Regression
                  through Compositional Kernel Search},
  author =	 {David Duvenaud and James Robert Lloyd and Roger
                  Grosse and Joshua B. Tenenbaum and Zoubin
                  Ghahramani},
  booktitle =	 icml30,
  year =	 2013,
  month =	 {June},
  address =	 {Atlanta, Georgia, USA},
  url =
                  {http://jmlr.org/proceedings/papers/v28/duvenaud13.pdf},
  abstract =	 {Despite its importance, choosing the structural form
                  of the kernel in nonparametric regression remains a
                  black art. We define a space of kernel structures
                  which are built compositionally by adding and
                  multiplying a small number of base kernels. We
                  present a method for searching over this space of
                  structures which mirrors the scientific discovery
                  process. The learned structures can often decompose
                  functions into interpretable components and enable
                  long-range extrapolation on time-series
                  datasets. Our structure search method outperforms
                  many widely used kernels and kernel combination
                  methods on a variety of prediction tasks.}
}

@inproceedings{DuvNicRas11,
  cat =		 {gp},
  booktitle =	 nips24,
  title =	 {Additive {G}aussian Processes},
  author =	 {David Duvenaud and Hannes Nickisch and Carl Edward
                  Rasmussen},
  year =	 2011,
  address =	 {Granada, Spain},
  pages =	 {226--234},
  url =
                  {http://papers.nips.cc/paper/4221-additive-gaussian-processes},
  abstract =	 {We introduce a Gaussian process model of functions
                  which are additive.  An additive function is one
                  which decomposes into a sum of low-dimensional
                  functions, each depending on only a subset of the
                  input variables. Additive GPs generalize both
                  Generalized Additive Models, and the standard GP
                  models which use squared-exponential kernels.
                  Hyperparameter learning in this model can be seen as
                  Bayesian Hierarchical Kernel Learning (HKL).  We
                  introduce an expressive but tractable
                  parameterization of the kernel function, which
                  allows efficient evaluation of all input interaction
                  terms, whose number is exponential in the input
                  dimension.  The additional structure discoverable by
                  this model results in increased interpretability, as
                  well as state-of-the-art predictive power in
                  regression tasks.}
}

@inproceedings{DuvRipAdaGha14,
  cat =		 {gp np deep},
  title =	 {Avoiding pathologies in very deep networks},
  author =	 {David Duvenaud and Oren Rippel and R<!>yan P. Adams
                  and Zoubin Ghahramani},
  booktitle =	 aistats17,
  year =	 2014,
  month =	 {April},
  address =	 {Reykjavik, Iceland},
  url =		 {http://arxiv.org/pdf/1402.5836.pdf},
  abstract =	 {Choosing appropriate architectures and
                  regularization strategies for deep networks is
                  crucial to good predictive performance. To shed
                  light on this problem, we analyze the analogous
                  problem of constructing useful priors on
                  compositions of functions. Specifically, we study
                  the deep Gaussian process, a type of
                  infinitely-wide, deep neural network. We show that
                  in standard architectures, the representational
                  capacity of the network tends to capture fewer
                  degrees of freedom as the number of layers
                  increases, retaining only a single degree of freedom
                  in the limit. We propose an alternate network
                  architecture which does not suffer from this
                  pathology. We also examine deep covariance
                  functions, obtained by composing infinitely many
                  feature transforms. Lastly, we characterize the
                  class of models obtained by performing dropout on
                  Gaussian processes.}
}

@inproceedings{DziRoyGha15,
  cat =		 {deep},
  title =	 {Training generative neural networks via {M}aximum
                  {M}ean {D}iscrepancy optimization},
  booktitle =	 uai31,
  year =	 2015,
  month =	 {July},
  address =	 {Amsterdam, The Netherlands},
  author =	 {Gintare Karolina Dziugaite and Daniel M. Roy and
                  Zoubin Ghahramani},
  pages =	 {258--267},
  abstract =	 {We consider training a deep neural network to
                  generate samples from an unknown distribution given
                  i.i.d. data. We frame learning as an optimization
                  minimizing a two-sample test statistic---informally
                  speaking, a good generator network produces samples
                  that cause a two-sample test to fail to reject the
                  null hypothesis. As our two-sample test statistic,
                  we use an unbiased estimate of the maximum mean
                  discrepancy, which is the centerpiece of the
                  nonparametric kernel two-sample test proposed by
                  Gretton et al. (2012).  We compare to the
                  adversarial nets framework introduced by Goodfellow
                  et al. (2014), in which learning is a two-player
                  game between a generator network and an adversarial
                  discriminator network, both trained to outwit the
                  other. From this perspective, the MMD statistic
                  plays the role of the discriminator. In addition to
                  empirical comparisons, we prove bounds on the
                  generalization error incurred by optimizing the
                  empirical MMD.},
  url =		 {http://auai.org/uai2015/proceedings/papers/230.pdf},
}

@inproceedings{Eastwoodetal22,
  cat =		 {causal},
  title =	 {Probable Domain Generalization via Quantile Risk
                  Minimization},
  author =	 {Eastwood, C. and Robey, A. and Singh, S. and von
                  K{\"u}gelgen, J. and Hassani, H. and Pappas,
                  G. J. and Sch{\"o}lkopf, B.},
  booktitle =	 nips35,
  publisher =	 {Curran Associates, Inc.},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2207.09944},
  abstract =	 {Domain generalization (DG) seeks predictors which
                  perform well on unseen test distributions by
                  leveraging data drawn from multiple related training
                  distributions or domains. To achieve this, DG is
                  commonly formulated as an average- or worst-case
                  problem over the set of possible domains. However,
                  predictors that perform well on average lack
                  robustness while predictors that perform well in the
                  worst case tend to be overly-conservative. To
                  address this, we propose a new probabilistic
                  framework for DG where the goal is to learn
                  predictors that perform well with high
                  probability. Our key idea is that distribution
                  shifts seen during training should inform us of
                  probable shifts at test time, which we realize by
                  explicitly relating training and test domains as
                  draws from the same underlying meta-distribution. To
                  achieve probable DG, we propose a new optimization
                  problem called Quantile Risk Minimization (QRM). By
                  minimizing the $\alpha$-quantile of predictor's risk
                  distribution over domains, QRM seeks predictors that
                  perform well with probability $\alpha$. To solve QRM
                  in practice, we propose the Empirical QRM (EQRM)
                  algorithm, and prove: (i) a generalization bound for
                  EQRM; and (ii) that EQRM recovers the causal
                  predictor as $\alpha$->1. In our experiments, we
                  introduce a more holistic quantile-focused
                  evaluation protocol for DG, and demonstrate that
                  EQRM outperforms state-of-the-art baselines on
                  CMNIST and several datasets from WILDS and
                  DomainBed.},
}

@inproceedings{EatGha09,
  cat =		 {gm},
  author =	 {Frederik Eaton and Zoubin Ghahramani},
  title =	 {Choosing a Variable to Clamp: {A}pproximate
                  Inference Using Conditioned Belief Propagation},
  booktitle =	 aistats12,
  pages =	 {145--152},
  year =	 2009,
  editor =	 {D. van Dyk and M. Welling},
  volume =	 5,
  address =	 {Clearwater Beach, FL, USA},
  month =	 {April},
  publisher =	 jmlr,
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/frederik/libdai-eaton.tar.gz">Code</a>
                  (in C++ based on <a
                  href="http://www.kyb.mpg.de/bs/people/jorism/libDAI">libDAI</a>).},
  abstract =	 {In this paper we propose an algorithm for
                  approximate inference on graphical models based on
                  belief propagation (BP). Our algorithm is an
                  approximate version of Cutset Conditioning, in which
                  a subset of variables is instantiated to make the
                  rest of the graph singly connected. We relax the
                  constraint of single-connectedness, and select
                  variables one at a time for conditioning, running
                  belief propagation after each selection. We consider
                  the problem of determining the best variable to
                  clamp at each level of recursion, and propose a fast
                  heuristic which applies back-propagation to the BP
                  updates.  We demonstrate that the heuristic performs
                  better than selecting variables at random, and give
                  experimental results which show that it performs
                  competitively with existing approximate inference
                  algorithms.}
}

@article{EatGha13a,
  author =	 {Eaton, Frederik and Ghahramani, Zoubin},
  cat =		 {gm},
  journal =	 {Neural Computation},
  number =	 5,
  pages =	 {1213-1260},
  title =	 {Model Reductions for Inference: Generality of
                  Pairwise, Binary, and Planar Factor Graphs},
  url =		 {.},
  volume =	 25,
  year =	 2013,
  abstract =	 {We offer a solution to the problem of efficiently
                  translating algorithms between different types of
                  discrete statistical model. We investigate the
                  expressive power of three classes of model-those
                  with binary variables, with pairwise factors, and
                  with planar topology-as well as their four
                  intersections. We formalize a notion of "simple
                  reduction" for the problem of inferring marginal
                  probabilities and consider whether it is possible to
                  "simply reduce" marginal inference from general
                  discrete factor graphs to factor graphs in each of
                  these seven subclasses. We characterize the
                  reducibility of each class, showing in particular
                  that the class of binary pairwise factor graphs is
                  able to simply reduce only positive models. We also
                  exhibit a continuous "spectral reduction" based on
                  polynomial interpolation, which overcomes this
                  limitation. Experiments assess the performance of
                  standard approximate inference algorithms on the
                  outputs of our reductions.}
}

@inproceedings{EicTolZieetal04,
  author =	 {Jan Eichhorn and Andreas S.~Tolias and Alexander
                  Zien and Malte Ku{\ss} and Carl Edward Rasmussen and
                  Jason Weston and Nikos K.~Logothetis and Bernhard
                  Sch{\"o}lkopf},
  title =	 {Prediction on Spike Data Using Kernel Algorithms},
  year =	 2004,
  volume =	 16,
  publisher =	 mit,
  pages =	 {1367--1374},
  editor =	 {S<!>ebastian Thrun and Lawrence K.~Saul and Bernhard
                  Sch{\"o}lkopf},
  address =	 {Cambridge, MA, USA},
  abstract =	 {We report and compare the performance of different
                  learning algorithms based on data from cortical
                  recordings. The task is to predict the orientation
                  of visual stimuli from the activity of a population
                  of simultaneously recorded neurons. We compare
                  several ways of improving the coding of the input
                  (i.e., the spike data) as well as of the output
                  (i.e., the orientation), and report the results
                  obtained using different kernel algorithms.},
  booktitle =	 nips16,
  location =	 {Vancouver, BC, Canada},
  url =		 {.}
}

@inproceedings{EysGuIbaLev18,
  cat =		 {deep rl},
  title =	 {Leave no Trace: Learning to Reset for Safe and
                  Autonomous Reinforcement Learning},
  author =	 {Benjamin Eysenbach and Shixiang Gu and Julian Ibarz
                  and Sergey Levine},
  booktitle =	 iclr06,
  year =	 2018,
  month =	 {Apr},
  address =	 {Vancouver CANADA},
  url =		 {https://arxiv.org/abs/1711.06782},
  abstract =	 {Deep reinforcement learning algorithms can learn
                  complex behavioral skills, but real-world
                  application of these methods requires a large amount
                  of experience to be collected by the agent. In
                  practical settings, such as robotics, this involves
                  repeatedly attempting a task, resetting the
                  environment between each attempt. However, not all
                  tasks are easily or automatically reversible. In
                  practice, this learning process requires extensive
                  human intervention. In this work, we propose an
                  autonomous method for safe and efficient
                  reinforcement learning that simultaneously learns a
                  forward and reset policy, with the reset policy
                  resetting the environment for a subsequent
                  attempt. By learning a value function for the reset
                  policy, we can automatically determine when the
                  forward policy is about to enter a non-reversible
                  state, providing for uncertainty-aware safety
                  aborts. Our experiments illustrate that proper use
                  of the reset policy can greatly reduce the number of
                  manual resets required to learn a task, can reduce
                  the number of unsafe actions that lead to
                  non-reversible states, and can automatically induce
                  a curriculum.},
  annote =	 {[<a
                  href="https://sites.google.com/site/mlleavenotrace/">Video</a>]
                  }
}

@article{FavLomNipTeh14,
  cat =		 {np},
  title =	 {Stick-breaking representations of {sigma-S}table
                  {Poisson-K}ingman models},
  author =	 {Stefano Favaro and Maria Lomeli and Bernardo Nipoti
                  and Yee Whye Teh},
  year =	 2014,
  journal =	 ejs,
  volume =	 8,
  pages =	 {1063-1085},
  year =	 2014,
  url =		 {https://projecteuclid.org/euclid.ejs/1407243242},
  abstract =	 {In this paper we investigate the stick-breaking
                  representation for the class of sigma-Stable
                  Poisson-Kingman models, also known as Gibbs-type
                  random probability measures. This class includes as
                  special cases most of the discrete priors commonly
                  used in Bayesian nonparametrics, such as the two
                  parameter Poisson-Dirichlet process and the
                  normalized generalized Gamma process. Under the
                  assumption sigma=u/v, for any coprime integers 1
                  &lt= u &lt v such that u/v &lt 1/2, we show that a
                  sigma-stable Poisson-Kingman model admits an
                  explicit stick-breaking representation in terms of
                  random variables which are obtained by suitably
                  transforming Gamma random variables and products of
                  independent Beta and Gamma random variables.}
}

@article{FavLomTeh15,
  cat =		 {np clust mcm},
  title =	 {On a class of {sigma-Stable Poisson-Kingman} models
                  and an effective marginalised sampler},
  author =	 {Stefano Favaro and Maria Lomeli and Yee Whye Teh},
  year =	 2015,
  journal =	 staco,
  volume =	 25,
  pages =	 {67-78},
  url =
                  {https://link.springer.com/article/10.1007/s11222-014-9499-4},
  abstract =	 {We investigate the use of a large class of discrete
                  random probability measures, which is referred to as
                  the class Q, , in the context of Bayesian
                  nonparametric mixture modeling. The class Q
                  encompasses both the the two-parameter
                  Poisson?Dirichlet process and the normalized
                  generalized Gamma process, thus allowing us to
                  comparatively study the inferential advantages of
                  these two well-known nonparametric priors. Apart
                  from ahighly flexible parameterization, the
                  distinguishing feature of the class Q is the
                  availability of a tractable posterior
                  distribution. This feature, in turn, leads to derive
                  an efficient marginal MCMC algorithm for posterior
                  sampling within the framework of mixture models. We
                  demonstrate the efficacy of our modeling framework
                  on both one-dimensional and multi-dimensional
                  datasets.}
}

@inproceedings{FlaMarHer22,
  cat =		 {mcmc},
  author =	 {Gergely Flamich and Stratis Markou and Jos\'e Miguel
                  Hern\'andez-Lobato},
  title =	 {Fast relative Entropy coding with {A}* coding},
  booktitle =	 icml39,
  year =	 2022,
  url =		 {https://arxiv.org/abs/2201.12857},
  abstract =	 {Relative entropy coding (REC) algorithms encode a
                  sample from a target distribution $Q$ using a
                  proposal distribution $P$, such that the expected
                  codelength is $\mathcal{O}(D_{KL}[Q||P])$. REC can
                  be seamlessly integrated with existing learned
                  compression models since, unlike entropy coding, it
                  does not assume discrete $Q$ or $P$, and does not
                  require quantisation. However, general REC
                  algorithms require an intractable
                  $\Omega(e^{D_{KL}[Q||P]})$ runtime. We introduce AS*
                  and AD* coding, two REC algorithms based on A*
                  sampling. We prove that, for continuous
                  distributions over $\mathbb{R}$, if the density
                  ratio is unimodal, AS* has
                  $\mathcal{O}(D_{\infty}[Q||P]{Q}{P})$ expected
                  runtime, where $D_{\infty}[Q||P]{Q}{P}$ is the
                  R\'enyi $\infty$-divergence. We provide experimental
                  evidence that AD* also has
                  $\mathcal{O}(D_{\infty}[Q||P]{Q}{P})$ expected
                  runtime. We prove that AS* and AD* achieve an
                  expected codelength of
                  $\mathcal{O}(D_{KL}[Q||P])$. Further, we introduce
                  DAD*, an approximate algorithm based on AD* which
                  retains its favourable runtime and has bias similar
                  to that of alternative methods.  Focusing on VAEs,
                  we propose the IsoKL VAE (IKVAE), which can be used
                  with DAD* to further improve compression
                  efficiency. We evaluate A* coding with (IK)VAEs on
                  MNIST, showing that it can losslessly compress
                  images near the theoretically optimal limit.}
}

@inproceedings{FooBruBuretal21,
  cat =		 {deep},
  author =	 {Andrew Y. K. Foong and Wessel P. Bruinsma and David
                  R. Burt and Richard E. Turner},
  title =	 {How Tight Can {PAC}-{B}ayes Be in the Small Data
                  Regime?},
  booktitle =	 nips34,
  year =	 2021,
  publisher =	 curran,
  url =
                  {https://papers.nips.cc/paper/2021/file/214cfbe603b7f9f9bc005d5f53f7a1d3-Paper.pdf},
  abstract =	 {In this paper, we investigate the question: Given a
                  small number of datapoints, for example N = 30, how
                  tight can PAC-Bayes and test set bounds be made? For
                  such small datasets, test set bounds adversely
                  affect generalisation performance by withholding
                  data from the training procedure.  In this setting,
                  PAC-Bayes bounds are especially attractive, due to
                  their ability to use all the data to simultaneously
                  learn a posterior and bound its generalisation
                  risk. We focus on the case of i.i.d. data with a
                  bounded loss and consider the generic PAC-Bayes
                  theorem of Germain et al. While their theorem is
                  known to recover many existing PAC-Bayes bounds, it
                  is unclear what the tightest bound derivable from
                  their framework is. For a fixed learning algorithm
                  and dataset, we show that the tightest possible
                  bound coincides with a bound considered by Catoni;
                  and, in the more natural case of distributions over
                  datasets, we establish a lower bound on the best
                  bound achievable in expectation. Interestingly, this
                  lower bound recovers the Chernoff test set bound if
                  the posterior is equal to the prior. Moreover, to
                  illustrate how tight these bounds can be, we study
                  synthetic one-dimensional classification tasks in
                  which it is feasible to meta-learn both the prior
                  and the form of the bound to numerically optimise
                  for the tightest bounds possible. We ind that in
                  this simple, controlled scenario, PAC-Bayes bounds
                  are competitive with comparable, commonly used
                  Chernoff test set bounds. However, the sharpest test
                  set bounds still lead to better guarantees on the
                  generalisation error than the PAC-Bayes bounds we
                  consider.}
}

@inproceedings{FooBruGoretal20,
  cat =		 {deep},
  author =	 {Andrew Y. K. Foong and Wessel P. Bruinsma and
                  Jonathan Gordon and Yann Dubois and James Requeima
                  and Richard E. Turner},
  title =	 {Meta-Learning Stationary Stochastic Process
                  Prediction With Convolutional Neural Processes},
  booktitle =	 nips33,
  year =	 2020,
  publisher =	 curran,
  url =
                  {https://papers.nips.cc/paper/2020/file/5df0385cba256a135be596dbe28fa7aa-Paper.pdf},
  abstract =	 {Stationary stochastic processes (SPs) are a key
                  component of many probabilistic models, such as
                  those for off-the-grid spatio-temporal data. They
                  enable the statistical symmetry of underlying
                  physical phenomena to be leveraged, thereby aiding
                  generalization. Prediction in such models can be
                  viewed as a translation equivariant map from
                  observed data sets to predictive SPs, emphasizing
                  the intimate relationship between stationarity and
                  equivariance. Building on this, we propose the
                  Convolutional Neural Process (ConvNP), which endows
                  Neural Processes (NPs) with translation equivariance
                  and extends convolutional conditional NPs to allow
                  for dependencies in the predictive distribution. The
                  latter enables ConvNPs to be deployed in settings
                  which require coherent samples, such as Thompson
                  sampling or conditional image completion. Moreover,
                  we propose a new maximum-likelihood objective to
                  replace the standard ELBO objective in NPs, which
                  conceptually simplifies the framework and
                  empirically improves performance. We demonstrate the
                  strong performance and generalization capabilities
                  of ConvNPs on 1D regression, image completion, and
                  various tasks with real-world spatio-temporal data.}
}

@inproceedings{FooBurLietal2020,
  author =	 {Foong, Andrew and Burt, David and Li, Yingzhen and
                  Turner, Richard},
  booktitle =	 nips34,
  title =	 {On the Expressiveness of Approximate Inference in
                  Bayesian Neural Networks},
  cat =		 {deep approx},
  year =	 2020
}

@article{ForColWenEtal22,
  cat =		 {approx deep},
  title =	 {Deep Classifiers with Label Noise Modeling and
                  Distance Awareness},
  author =	 {Vincent Fortuin and Mark Collier and Florian Wenzel
                  and James Urquhart Allingham and Jeremiah Zhe Liu
                  and Dustin Tran and Balaji Lakshminarayanan and
                  Jesse Berent and Rodolphe Jenatton and Effrosyni
                  Kokiopoulou},
  journal =	 {Transactions on Machine Learning Research},
  year =	 2022,
  url =		 {https://openreview.net/forum?id=Id7hTt78FV},
  abstract =	 {Uncertainty estimation in deep learning has recently
                  emerged as a crucial area of interest to advance
                  reliability and robustness in safety-critical
                  applications. While there have been many proposed
                  methods that either focus on distance-aware model
                  uncertainties for out-of-distribution detection or
                  on input-dependent label uncertainties for
                  in-distribution calibration, both of these types of
                  uncertainty are often necessary. In this work, we
                  propose the HetSNGP method for jointly modeling the
                  model and data uncertainty. We show that our
                  proposed model affords a favorable combination
                  between these two types of uncertainty and thus
                  outperforms the baseline methods on some challenging
                  out-of-distribution datasets, including CIFAR-100C,
                  ImageNet-C, and ImageNet-A. Moreover, we propose
                  HetSNGP Ensemble, an ensembled version of our method
                  which additionally models uncertainty over the
                  network parameters and outperforms other ensemble
                  baselines.},
  annote =	 {<a
                  href="https://github.com/google/edward2/blob/main/edward2/tensorflow/layers/hetsngp.py">Code</a>}
}

@inproceedings{ForGarObeetal22,
  cat =		 {deep approx},
  title =	 {Bayesian neural network priors revisited},
  author =	 {Vincent Fortuin and Adri\`{a} Garriga-Alonso and
                  Sebastian W. Ober and Florian Wenzel and Gunnar
                  R\"{a}tsch and Richard E. Turner and Mark van der
                  Wilk and Laurence Aitchison},
  booktitle =	 iclr10,
  year =	 2022,
  url =		 {https://openreview.net/pdf?id=xkjqJYqRJy},
  abstract =	 {Isotropic Gaussian priors are the de facto standard
                  for modern Bayesian neural network inference.
                  However, it is unclear whether these priors
                  accurately reflect our true beliefs about the weight
                  distributions or give optimal performance. To find
                  better priors, we study summary statistics of neural
                  network weights in networks trained using stochastic
                  gradient descent (SGD). We find that convolutional
                  neural network (CNN) and ResNet weights display
                  strong spatial correlations, while fully connected
                  networks (FCNNs) display heavy-tailed weight
                  distributions. We show that building these
                  observations into priors can lead to improved
                  performance on a variety of image classification
                  datasets. Surprisingly, these priors mitigate the
                  cold posterior effect in FCNNs, but slightly
                  increase the cold posterior effect in ResNets.}
}

@inproceedings{FraKwoRasSch04,
  cat =		 {ssl},
  author =	 {Matthias O.~Franz and Younghee Kwon and Carl Edward
                  Rasmussen and Bernhard Sch{\"o}lkopf},
  title =	 {Semi-supervised kernel regression using whitened
                  function classes},
  year =	 2004,
  volume =	 3175,
  booktitle =	 lncs,
  publisher =	 {Springer},
  pages =	 {18--26},
  journal =	 {Pattern Recognition, Proceedings of the 26th {DAGM}
                  Symposium},
  editor =	 {C.~E.~Rasmussen and H.~H.~B{\"u}lthoff and
                  M.~A.~Giese and B.~Sch{\"o}lkopf},
  address =	 {Berlin, Germany},
  abstract =	 {The use of non-orthonormal basis functions in ridge
                  regression leads to an often undesired non-isotropic
                  prior in function space. In this study, we
                  investigate an alternative regularization technique
                  that results in an implicit whitening of the basis
                  functions by penalizing directions in function space
                  with a large prior variance. The regularization term
                  is computed from unlabelled input data that
                  characterizes the input distribution. Tests on two
                  datasets using polynomial basis functions showed an
                  improved average performance compared to standard
                  ridge regression.},
  url =		 {.}
}

@inproceedings{FreFerHam14,
  cat =		 {bioinf mcmc gm},
  author =	 {Jes Frellsen and Thomas Hamelryck and Jesper
                  Ferkinghoff-Borg},
  title =	 {Combining the multicanonical ensemble with
                  generative probabilistic models of local
                  biomolecular structure},
  year =	 2014,
  booktitle =	 {Proceedings of the 59th {W}orld {S}tatistics
                  {C}ongress of the {I}nternational {S}tatistical
                  {I}nstitute},
  address =	 {Hong Kong},
  pages =	 {139--144},
  abstract =	 {Markov chain Monte Carlo is a powerful tool for
                  sampling complex systems such as large biomolecular
                  structures. However, the standard
                  Metropolis-Hastings algorithm suffers from a number
                  of deficiencies when applied to systems with rugged
                  free-energy landscapes. Some of these deficiencies
                  can be addressed with the multicanonical
                  ensemble. In this paper we will present two
                  strategies for applying the multicanonical ensemble
                  to distributions constructed from generative
                  probabilistic models of local biomolecular
                  structure. In particular, we will describe how to
                  use the multicanonical ensemble efficiently in
                  conjunction with the reference ratio method.},
  url =
                  {http://2013.isiproceedings.org/Files/IPS015-P4-S.pdf}
}

@inproceedings{FreWinGhaFer16,
  cat =		 {mcmc approx},
  title =	 {Bayesian generalised ensemble {M}arkov chain {M}onte
                  {C}arlo},
  booktitle =	 aistats19,
  year =	 2016,
  month =	 {May},
  address =	 {Cadiz, Spain},
  author =	 {Jes Frellsen and Ole Winther and Zoubin Ghahramani
                  and Jesper Ferkinghoff-Borg},
  abstract =	 {Bayesian generalised ensemble (BayesGE) is a new
                  method that addresses two major drawbacks of
                  standard Markov chain Monte Carlo algorithms for
                  inference in high-dimensional probability models:
                  inapplicability to estimate the partition function,
                  and poor mixing properties. BayesGE uses a Bayesian
                  approach to iteratively update the belief about the
                  density of states (distribution of the log
                  likelihood under the prior) for the model, with the
                  dual purpose of enhancing the sampling efficiency
                  and make the estimation of the partition function
                  tractable. We benchmark BayesGE on Ising and Potts
                  systems and show that it compares favourably to
                  existing state-of-the-art methods.}
}

@phdthesis{Fri15,
  author =	 {Roger Frigola},
  cat =		 {gp time approx},
  title =	 {Bayesian Time Series Learning with {G}aussian
                  Processes},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2015,
  abstract =	 {The analysis of time series data is important in
                  fields as disparate as the social sciences, biology,
                  engineering or econometrics. In this dissertation,
                  we present a number of algorithms designed to learn
                  Bayesian nonparametric models of time series. The
                  goal of these kinds of models is twofold. First,
                  they aim at making predictions which quantify the
                  uncertainty due to limitations in the quantity and
                  the quality of the data. Second, they are flexible
                  enough to model highly complex data whilst
                  preventing overfitting when the data does not
                  warrant complex models.</br> We begin with a
                  unifying literature review on time series models
                  based on Gaussian processes. Then, we centre our
                  attention on the Gaussian Process State-Space Model
                  (GP-SSM): a Bayesian nonparametric generalisation of
                  discrete-time nonlinear state-space models. We
                  present a novel formulation of the GP-SSM that
                  offers new insights into its properties. We then
                  proceed to exploit those insights by developing new
                  learning algorithms for the GP-SSM based on particle
                  Markov chain Monte Carlo and variational
                  inference.</br> Finally, we present a filtered
                  nonlinear auto-regressive model with a simple,
                  robust and fast learning algorithm that makes it
                  well suited to its application by non-experts on
                  large datasets. Its main advantage is that it avoids
                  the computationally expensive (and potentially
                  difficult to tune) smoothing step that is a key part
                  of learning nonlinear state-space models.},
  address =	 {Cambridge, UK},
  url =		 {.}
}

@inproceedings{FriCheRas14,
  cat =		 {gp np time},
  title =	 {Variational {G}aussian Process State-Space Models},
  author =	 {Roger Frigola and Yutian Chen and Carl Edward
                  Rasmussen},
  booktitle =	 nips27,
  editor =	 {Z. Ghahramani and M. Welling and C. Cortes and
                  N.D. Lawrence and K.Q. Weinberger},
  year =	 2014,
  abstract =	 {State-space models have been successfully used for
                  more than fifty years in different areas of science
                  and engineering. We present a procedure for
                  efficient variational Bayesian learning of nonlinear
                  state-space models based on sparse Gaussian
                  processes. The result of learning is a tractable
                  posterior over nonlinear dynamical systems. In
                  comparison to conventional parametric models, we
                  offer the possibility to straightforwardly trade off
                  model capacity and computational cost whilst
                  avoiding overfitting. Our main algorithm uses a
                  hybrid inference approach combining variational
                  Bayes and sequential Monte Carlo. We also present
                  stochastic variational inference and online learning
                  approaches for fast learning with long time series.},
  url =
                  {http://papers.nips.cc/paper/5375-variational-gaussian-process-state-space-models.pdf}
}

@incollection{FriLinSchRas13,
  cat =		 {gp np time},
  title =	 {Bayesian Inference and Learning in {G}aussian
                  Process State-Space Models with Particle {MCMC}},
  author =	 {Roger Frigola and Fredrik Lindsten and Thomas
                  B. Sch\"{o}n and Carl Edward Rasmussen},
  booktitle =	 {Advances in Neural Information Processing Systems
                  26},
  editor =	 {L. Bottou and C.J.C. Burges and Z. Ghahramani and
                  M. Welling and K.Q. Weinberger},
  year =	 2013,
  publisher =	 curran,
  abstract =	 {State-space models are successfully used in many
                  areas of science, engineering and economics to model
                  time series and dynamical systems. We present a
                  fully Bayesian approach to inference and learning in
                  nonlinear nonparametric state-space models. We place
                  a Gaussian process prior over the transition
                  dynamics, resulting in a flexible model able to
                  capture complex dynamical phenomena. However, to
                  enable efficient inference, we marginalize over the
                  dynamics of the model and instead infer directly the
                  joint smoothing distribution through the use of
                  specially tailored Particle Markov Chain Monte Carlo
                  samplers. Once a sample from the smoothing
                  distribution is computed, the state transition
                  predictive distribution can be formulated
                  analytically. We make use of sparse Gaussian process
                  models to greatly reduce the computational
                  complexity of the approach.},
  url =		 {http://arxiv.org/pdf/1306.2861}
}

@inproceedings{FriLinSchRas14,
  cat =		 {gp np time},
  title =	 {Identification of {G}aussian Process State-Space
                  Models with Particle Stochastic Approximation {EM}},
  author =	 {Roger Frigola and Fredrik Lindsten and Thomas
                  B. Sch\"{o}n and Carl Edward Rasmussen},
  booktitle =	 {Proceedings of the 19th World Congress of the
                  International Federation of Automatic Control
                  (IFAC)},
  year =	 2014,
  abstract =	 {Gaussian process state-space models (GP-SSMs) are a
                  very flexible family of models of nonlinear
                  dynamical systems. They comprise a Bayesian
                  nonparametric representation of the dynamics of the
                  system and additional (hyper-)parameters governing
                  the properties of this nonparametric
                  representation. The Bayesian formalism enables
                  systematic reasoning about the uncertainty in the
                  system dynamics. We present an approach to maximum
                  likelihood identification of the parameters in
                  GP-SSMs, while retaining the full nonparametric
                  description of the dynamics. The method is based on
                  a stochastic approximation version of the EM
                  algorithm that employs recent developments in
                  particle Markov chain Monte Carlo for efficient
                  identification. },
  url =		 {http://arxiv.org/pdf/1312.4852}
}

@inproceedings{FriRas13,
  cat =		 {gp np time},
  author =	 {Roger Frigola and Carl Edward Rasmussen},
  booktitle =	 {Decision and Control (CDC), 2013 IEEE 52nd Annual
                  Conference on},
  title =	 {Integrated Pre-Processing for {B}ayesian Nonlinear
                  System Identification with {G}aussian Processes},
  year =	 2013,
  abstract =	 {We introduce GP-FNARX: a new model for nonlinear
                  system identification based on a nonlinear
                  autoregressive exogenous model (NARX) with filtered
                  regressors (F) where the nonlinear regression
                  problem is tackled using sparse Gaussian processes
                  (GP). We integrate data pre-processing with system
                  identification into a fully automated procedure that
                  goes from raw data to an identified model. Both
                  pre-processing parameters and GP hyper-parameters
                  are tuned by maximizing the marginal likelihood of
                  the probabilistic model. We obtain a Bayesian model
                  of the system's dynamics which is able to report its
                  uncertainty in regions where the data is scarce. The
                  automated approach, the modeling of uncertainty and
                  its relatively low computational cost make of
                  GP-FNARX a good candidate for applications in
                  robotics and adaptive control.},
  url =		 {http://arxiv.org/pdf/1303.2912}
}

@inproceedings{Gal2014Pitfalls,
  title =	 {Pitfalls in the use of Parallel Inference for the
                  {D}irichlet Process},
  author =	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle =	 {Proceedings of the 31th International Conference on
                  Machine Learning (ICML-14)},
  url =		 {http://jmlr.org/proceedings/papers/v32/gal14.pdf},
  cat =		 {np},
  abstract =	 {Recent work done by Lovell, Adams, and Mansingka
                  (2012) and Williamson, Dubey, and Xing (2013) has
                  suggested an alternative parametrisation for the
                  Dirichlet process in order to derive non-approximate
                  parallel MCMC inference for it – work which has been
                  picked-up and implemented in several different
                  fields. In this paper we show that the approach
                  suggested is impractical due to an extremely
                  unbalanced distribution of the data. We characterise
                  the requirements of efficient parallel inference for
                  the Dirichlet process and show that the proposed
                  inference fails most of these requirements (while
                  approximate approaches often satisfy most of
                  them). We present both theoretical and experimental
                  evidence, analysing the load balance for the
                  inference and showing that it is independent of the
                  size of the dataset and the number of nodes
                  available in the parallel implementation. We end
                  with suggestions of alternative paths of research
                  for efficient non-approximate parallel inference for
                  the Dirichlet process.},
  year =	 2014
}

@inproceedings{GalBlu13,
  cat =		 {gm np},
  title =	 "A Systematic {B}ayesian Treatment of the {IBM}
                  Alignment Models",
  author =	 "Yarin Gal and Phil Blunsom",
  year =	 2013,
  booktitle =	 "Proceedings of the 2013 Conference of the North
                  American Chapter of the Association for
                  Computational Linguistics: Human Language
                  Technologies",
  publisher =	 "Association for Computational Linguistics",
  url =		 {.},
  abstract =	 {The dominant yet ageing IBM and HMM word alignment
                  models underpin most popular Statistical Machine
                  Translation implementations in use today. Though
                  beset by the limitations of implausible independence
                  assumptions, intractable optimisation problems, and
                  an excess of tunable parameters, these models
                  provide a scalable and reliable starting point for
                  inducing translation systems. In this paper we build
                  upon this venerable base by recasting these models
                  in the non-parametric Bayesian framework. By
                  replacing the categorical distributions at their
                  core with hierarchical Pitman-Yor processes, and
                  through the use of collapsed Gibbs sampling, we
                  provide a more flexible formulation and sidestep the
                  original heuristic optimisation techniques.  The
                  resulting models are highly extendible, naturally
                  permitting the introduction of phrasal
                  dependencies. We present extensive experimental
                  results showing improvements in both AER and BLEU
                  when benchmarked against Giza++, including
                  significant improvements over IBM model 4.}
}

@inproceedings{GalCheGha15,
  Author =	 {Yarin Gal and Yutian Chen and Zoubin Ghahramani},
  Title =	 {Latent {G}aussian Processes for Distribution
                  Estimation of Multivariate Categorical Data},
  booktitle =	 {Proceedings of the 32nd International Conference on
                  Machine Learning (ICML-15)},
  year =	 2015,
  pages =	 {645--654},
  abstract =	 {Multivariate categorical data occur in many
                  applications of machine learning. One of the main
                  difficulties with these vectors of categorical
                  variables is sparsity. The number of possible
                  observations grows exponentially with vector length,
                  but dataset diversity might be poor in
                  comparison. Recent models have gained significant
                  improvement in supervised tasks with this
                  data. These models embed observations in a
                  continuous space to capture similarities between
                  them. Building on these ideas we propose a Bayesian
                  model for the unsupervised task of distribution
                  estimation of multivariate categorical data. We
                  model vectors of categorical variables as generated
                  from a non-linear transformation of a continuous
                  latent space. Non-linearity captures multi-modality
                  in the distribution. The continuous representation
                  addresses sparsity. Our model ties together many
                  existing models, linking the linear categorical
                  latent Gaussian model, the Gaussian process latent
                  variable model, and Gaussian process
                  classification. We derive inference for our model
                  based on recent developments in sampling based
                  variational inference. We show empirically that the
                  model outperforms its linear and discrete
                  counterparts in imputation tasks of sparse data.},
  url =		 {http://jmlr.org/proceedings/papers/v37/gala15.html},
  cat =		 {gp np approx bioinf}
}

@article{GalHroKen17,
  cat =		 {network},
  title =	 {Concrete dropout},
  author =	 {Gal, Yarin and Hron, Jiri and Kendall, Alex},
  journal =	 {NeurIPS},
  year =	 2017,
  url =		 {https://arxiv.org/abs/2206.13102},
  abstract =	 { Dropout is used as a practical tool to obtain
                  uncertainty estimates in large vision models and
                  reinforcement learning (RL) tasks. But to obtain
                  well-calibrated uncertainty estimates, a grid-search
                  over the dropout probabilities is necessary—a
                  prohibitive operation with large models, and an
                  impossible one with RL. We propose a new dropout
                  variant which gives improved performance and better
                  calibrated uncertainties. Relying on recent
                  developments in Bayesian deep learning, we use a
                  continuous relaxation of dropout's discrete
                  masks. Together with a principled optimisation
                  objective, this allows for automatic tuning of the
                  dropout probability in large models, and as a result
                  faster experimentation cycles. In RL this allows the
                  agent to adapt its uncertainty dynamically as more
                  data is observed. We analyse the proposed variant
                  extensively on a range of tasks, and give insights
                  into common practice in the field where larger
                  dropout probabilities are often used in deeper model
                  layers.  }
}

@inproceedings{GalTur15,
  Author =	 {Yarin Gal and Richard Turner},
  Title =	 {Improving the {G}aussian Process Sparse Spectrum
                  Approximation by Representing Uncertainty in
                  Frequency Inputs},
  booktitle =	 {Proceedings of the 32nd International Conference on
                  Machine Learning (ICML-15)},
  year =	 2015,
  pages =	 {655--664},
  abstract =	 {Standard sparse pseudo-input approximations to the
                  Gaussian process (GP) cannot handle complex
                  functions well. Sparse spectrum alternatives attempt
                  to answer this but are known to over-fit. We suggest
                  the use of variational inference for the sparse
                  spectrum approximation to avoid both issues. We
                  model the covariance function with a finite Fourier
                  series approximation and treat it as a random
                  variable. The random covariance function has a
                  posterior, on which a variational distribution is
                  placed. The variational distribution transforms the
                  random covariance function to fit the data. We study
                  the properties of our approximate inference, compare
                  it to alternative ones, and extend it to the
                  distributed and stochastic domains. Our
                  approximation captures complex functions better than
                  standard approaches and avoids over-fitting.},
  url =		 {http://jmlr.org/proceedings/papers/v37/galb15.html},
  cat =		 {gp np approx}
}

@incollection{GalWilRas14,
  title =	 {Distributed Variational Inference in Sparse
                  {G}aussian Process Regression and Latent Variable
                  Models},
  author =	 {Gal, Yarin and van der Wilk, Mark and Rasmussen,
                  Carl},
  booktitle =	 {Advances in Neural Information Processing Systems
                  27},
  editor =	 {Z. Ghahramani and M. Welling and C. Cortes and
                  N.D. Lawrence and K.Q. Weinberger},
  pages =	 {3257--3265},
  abstract =	 {Gaussian processes (GPs) are a powerful tool for
                  probabilistic inference over functions. They have
                  been applied to both regression and non-linear
                  dimensionality reduction, and offer desirable
                  properties such as uncertainty estimates, robustness
                  to over-fitting, and principled ways for tuning
                  hyper-parameters. However the scalability of these
                  models to big datasets remains an active topic of
                  research. We introduce a novel re-parametrisation of
                  variational inference for sparse GP regression and
                  latent variable models that allows for an efficient
                  distributed algorithm. This is done by exploiting
                  the decoupling of the data given the inducing points
                  to re-formulate the evidence lower bound in a
                  Map-Reduce setting. We show that the inference
                  scales well with data and computational resources,
                  while preserving a balanced distribution of the load
                  among the nodes. We further demonstrate the utility
                  in scaling Gaussian processes to big data. We show
                  that GP performance improves with increasing amounts
                  of data in regression (on flight data with 2 million
                  records) and latent variable modelling (on
                  MNIST). The results show that GPs perform better
                  than many common models often used for big data.},
  year =	 2014,
  publisher =	 {Curran Associates, Inc.},
  url =
                  {http://papers.nips.cc/paper/5593-distributed-variational-inference-in-sparse-gaussian-process-regression-and-latent-variable-models},
  cat =		 {gp np}
}

@inProceedings{GarRasAit19,
  cat =		 {gp deep},
  author =	 {Adri\`{a} Garriga-Alonso and Carl Edward Rasmussen
                  and Laurence Aitchison},
  title =	 {Deep Convolutional Networks as shallow {G}aussian
                  Processes},
  booktitle =	 {International Conference on Learning Representations
                  (ICLR)},
  year =	 2019,
  abstract =	 {We show that the output of a (residual)
                  convolutional neural network (CNN) with an
                  appropriate prior over the weights and biases is a
                  Gaussian process (GP) in the limit of infinitely
                  many convolutional filters, extending similar
                  results for dense networks. For a CNN, the
                  equivalent kernel can be computed exactly and,
                  unlike "deep kernels", has very few parameters: only
                  the hyperparameters of the original CNN. Further, we
                  show that this kernel has two properties that allow
                  it to be computed efficiently; the cost of
                  evaluating the kernel for a pair of images is
                  similar to a single forward pass through the
                  original CNN with only one filter per layer. The
                  kernel equivalent to a 32-layer ResNet obtains
                  0.84\% classification error on MNIST, a new record
                  for GPs with a comparable number of parameters.},
  url =		 {http://arxiv.org/abs/1808.05587}
}

@article{GebKilHaretal19,
  cat =		 {time deep sigproc},
  title =	 {Convolutional neural networks: A magic bullet for
                  gravitational-wave detection?},
  author =	 {Gebhard, Timothy and Kilbertus, Niki and Harry, Ian
                  and Sch{\"o}lkopf, Bernhard},
  journal =	 {Physical Review D},
  volume =	 100,
  number =	 6,
  pages =	 063015,
  publisher =	 {American Physical Society},
  month =	 sep,
  year =	 2019,
  url =
                  {https://link.aps.org/doi/10.1103/PhysRevD.100.063015},
  month_numeric =9,
  doi =		 {https://doi.org/10.1103/PhysRevD.100.063015},
  abstract =	 {In the last few years, machine learning techniques,
                  in particular convolutional neural networks, have
                  been investigated as a method to replace or
                  complement traditional matched filtering techniques
                  that are used to detect the gravitational-wave
                  signature of merging black holes. However, to date,
                  these methods have not yet been successfully applied
                  to the analysis of long stretches of data recorded
                  by the Advanced LIGO and Virgo gravitational-wave
                  observatories.  In this work, we critically examine
                  the use of convolutional neural networks as a tool
                  to search for merging black holes. We identify the
                  strengths and limitations of this approach,
                  highlight some common pitfalls in translating
                  between machine learning and gravitational-wave
                  astronomy, and discuss the interdisciplinary
                  challenges.  In particular, we explain in detail why
                  convolutional neural networks alone cannot be used
                  to claim a statistically significant
                  gravitational-wave detection.  However, we
                  demonstrate how they can still be used to rapidly
                  flag the times of potential signals in the data for
                  a more detailed follow-up. Our convolutional neural
                  network architecture as well as the proposed
                  performance metrics are better suited for this task
                  than a standard binary classifications scheme. A
                  detailed evaluation of our approach on Advanced LIGO
                  data demonstrates the potential of such systems as
                  trigger generators. Finally, we sound a note of
                  caution by constructing adversarial examples, which
                  showcase interesting "failure modes" of our model,
                  where inputs with no visible resemblance to real
                  gravitational-wave signals are identified as such by
                  the network with high confidence.}
}

@article{Gha01a,
  author =	 {Ghahramani, Zoubin},
  cat =		 {time, gm},
  journal =	 {IJPRAI},
  number =	 1,
  pages =	 {9-42},
  title =	 {An Introduction to Hidden {M}arkov Models and
                  {B}ayesian Networks},
  url =		 {.},
  volume =	 15,
  year =	 2001,
  abstract =	 {We provide a tutorial on learning and inference in
                  hidden Markov models in the context of the recent
                  literature on Bayesian networks. This perspective
                  make sit possible to consider novel generalizations
                  to hidden Markov models with multiple hidden state
                  variables, multiscale representations, and mixed
                  discrete and continuous variables. Although exact
                  inference in these generalizations is usually
                  intractable, one can use approximate inference in
                  these generalizations is usually intractable, one
                  can use approximate inference algorithms such as
                  Markov chain sampling and variational methods. We
                  describe how such methods are applied to these
                  generalized hidden Markov models. We conclude this
                  review with a discussion of Bayesian methods for
                  model selection in generalized HMMs.  }
}

@inproceedings{Gha03a,
  author =	 {Ghahramani, Zoubin},
  booktitle =	 {Advanced Lectures on Machine Learning},
  cat =		 {review},
  editor =	 {Bousquet, Olivier and von Luxburg, Ulrike and
                  R{\"a}tsch, Gunnar},
  isbn =	 {3-540-23122-6},
  pages =	 {72-112},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {Unsupervised {L}earning},
  url =		 {.},
  volume =	 3176,
  year =	 2003,
  abstract =	 {We give a tutorial and overview of the field of
                  unsupervised learning from the perspective of
                  statistical modelling. Unsupervised learning can be
                  motivated from information theoretic and Bayesian
                  principles. We briefly review basic models in
                  unsupervised learning, including factor analysis,
                  PCA, mixtures of Gaussians, ICA, hidden Markov
                  models, state-space models, and many variants and
                  extensions. We derive the EM algorithm and give an
                  overview of fundamental concepts in graphical
                  models, and inference algorithms on graphs. This is
                  followed by a quick tour of approximate Bayesian
                  inference, including Markov chain Monte Carlo
                  (MCMC), Laplace approximation, BIC, variational
                  approximations, and expectation propagation
                  (EP). The aim of this chapter is to provide a
                  high-level view of the field. Along the way, many
                  state-of-the-art ideas and future directions are
                  also reviewed. }
}

@article{Gha12,
  cat =		 {np review},
  author =	 {Zoubin Ghahramani},
  title =	 {{B}ayesian nonparametrics and the probabilistic
                  approach to modelling},
  journal =	 {Philosophical Transactions of the Royal Society A},
  url =		 {.},
  year =	 2013,
  abstract =	 {Modelling is fundamental to many fields of science
                  and engineering. A model can be thought of as a
                  representation of possible data one could predict
                  from a system. The probabilistic approach to
                  modelling uses probability theory to express all
                  aspects of uncertainty in the model. The
                  probabilistic approach is synonymous with Bayesian
                  modelling, which simply uses the rules of
                  probability theory in order to make predictions,
                  compare alternative models, and learn model
                  parameters and structure from data. This simple and
                  elegant framework is most powerful when coupled with
                  flexible probabilistic models. Flexibility is
                  achieved through the use of Bayesian
                  nonparametrics. This article provides an overview of
                  probabilistic modelling and an accessible survey of
                  some of the main tools in Bayesian
                  nonparametrics. The survey covers the use of
                  Bayesian nonparametrics for modelling unknown
                  functions, density estimation, clustering, time
                  series modelling, and representing sparsity,
                  hierarchies, and covariance structure.  More
                  specifically it gives brief non-technical overviews
                  of Gaussian processes, Dirichlet processes, infinite
                  hidden Markov models, Indian buffet processes,
                  Kingman's coalescent, Dirichlet diffusion tress, and
                  Wishart processes.}
}

@article{Gha15,
  cat =		 {np review},
  author =	 {Zoubin Ghahramani},
  title =	 {Probabilistic machine learning and artificial
                  intelligence},
  journal =	 {Nature},
  url =
                  {http://www.nature.com/nature/journal/v521/n7553/full/nature14541.html},
  year =	 2015,
  volume =	 521,
  pages =	 {452–459},
  doi =		 {doi:10.1038/nature14541},
  abstract =	 {How can a machine learn from experience?
                  Probabilistic modelling provides a framework for
                  understanding what learning is, and has therefore
                  emerged as one of the principal theoretical and
                  practical approaches for designing machines that
                  learn from data acquired through experience. The
                  probabilistic framework, which describes how to
                  represent and manipulate uncertainty about models
                  and predictions, has a central role in scientific
                  data analysis, machine learning, robotics, cognitive
                  science and artificial intelligence. This Review
                  provides an introduction to this framework, and
                  discusses some of the state-of-the-art advances in
                  the field, namely, probabilistic programming,
                  Bayesian optimization, data compression and
                  automatic model discovery.}
}

@inproceedings{Gha94a,
  author =	 {Ghahramani, Zoubin},
  booktitle =	 nips7,
  cat =		 {clust},
  editor =	 {Tesauro, Gerald and Touretzky, David S. and Leen,
                  Todd K.},
  pages =	 {617-624},
  publisher =	 {MIT Press},
  title =	 {Factorial Learning and the {EM} Algorithm},
  url =		 {.},
  year =	 1994,
  abstract =	 {Many real world learning problems are best
                  characterized by an interaction of multiple
                  independent causes or factors. Discovering such
                  causal structure from the data is the focus of this
                  paper. Based on Zemel and Hinton's cooperative
                  vector quantizer (CVQ) architecture, an unsupervised
                  learning algorithm is derived from the
                  Expectation--Maximization (EM) framework. Due to the
                  combinatorial nature of the data generation process,
                  the exact E-step is computationally intractable. Two
                  alternative methods for computing the E-step are
                  proposed: Gibbs sampling and mean-field
                  approximation, and some promising empirical results
                  are presented.  }
}

@inproceedings{Gha97a,
  author =	 {Ghahramani, Zoubin},
  booktitle =	 {Adaptive Processing of Sequences and Data
                  Structures},
  cat =		 {time, gm},
  editor =	 {Giles, C. Lee and Gori, Marco},
  isbn =	 {3-540-64341-9},
  pages =	 {168-197},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {Learning Dynamic {B}ayesian Networks},
  url =		 {.},
  volume =	 1387,
  year =	 1997,
  abstract =	 {Bayesian networks are a concise graphical formalism
                  for describing probabilistic models. We have
                  provided a brief tutorial of methods for learning
                  and inference in dynamic Bayesian networks. In many
                  of the interesting models, beyond the simple linear
                  dynamical system or hidden Markov model, the
                  calculations required for inference are
                  intractable. Two different approaches for handling
                  this intractability are Monte Carlo methods such as
                  Gibbs sampling, and variational methods. An
                  especially promising variational approach is based
                  on exploiting tractable substructures in the
                  Bayesian network.  }
}

@inproceedings{GhaBea00a,
  author =	 {Ghahramani, Zoubin and Beal, Matthew J.},
  booktitle =	 {NIPS},
  cat =		 {approx},
  editor =	 {Kearns, Michael J. and Solla, Sara A. and Cohn,
                  David A.},
  isbn =	 {0-262-11245-0},
  pages =	 {507-513},
  publisher =	 {The MIT Press},
  title =	 {Propagation Algorithms for Variational {B}ayesian
                  Learning},
  url =		 {.},
  year =	 2000,
  abstract =	 {Variational approximations are becoming a widespread
                  tool for Bayesian learning of graphical models. We
                  provide some theoretical results for the variational
                  updates in a very general family of
                  conjugate-exponential graphical models. We show how
                  the belief propagation and the junction tree
                  algorithms can be used in the inference step of
                  variational Bayesian learning. Applying these
                  results to the Bayesian analysis of linear-Gaussian
                  state-space models we obtain a learning procedure
                  that exploits the Kalman smoothing propagation,
                  while integrating over all model parameters. We
                  demonstrate how this can be used to infer the hidden
                  state dimensionality of the state-space model in a
                  variety of synthetic problems and one real
                  high-dimensional data set.  }
}

@inproceedings{GhaBea99a,
  author =	 {Ghahramani, Zoubin and Beal, Matthew J.},
  booktitle =	 {NIPS},
  editor =	 {Kearns, Michael J. and Solla, Sara A. and Cohn,
                  David A.},
  isbn =	 {0-262-11245-0},
  pages =	 {449-455},
  publisher =	 {The MIT Press},
  title =	 {Variational Inference for {B}ayesian Mixtures of
                  Factor Analysers},
  url =		 {.},
  year =	 1999,
  abstract =	 {We present an algorithm that infers the model
                  structure of a mixture of factor analysers using an
                  efficient and deterministic variational
                  approximation to full Bayesian integration over
                  model parameters. This procedure can automatically
                  determine the optimal number of components and the
                  local dimensionality of each component (i.e. the
                  number of factors in each factor
                  analyser). Alternatively it can be used to infer
                  posterior distributions over number of components
                  and dimensionalities. Since all parameters are
                  integrated out the method is not prone to
                  overfitting. Using a stochastic procedure for adding
                  components it is possible to perform the variational
                  optimisation incrementally and to avoid local
                  maxima. Results show that the method works very well
                  in practice and correctly infers the number and
                  dimensionality of nontrivial synthetic examples.  By
                  importance sampling from the variational
                  approximation we show how to obtain unbiased
                  estimates of the true evidence, the exact predictive
                  density, and the KL divergence between the
                  variational posterior and the true posterior, not
                  only in this model but for variational
                  approximations in general.}
}

@inproceedings{GhaGriSol07,
  cat =		 {np},
  month =	 {July},
  author =	 {Z. Ghahramani and T.L. Griffiths and P. Sollich},
  annote =	 {Includes discussion by David Dunson, and rejoinder.},
  booktitle =	 {Bayesian Statistics 8},
  editor =	 {J.M. Bernardo and M.J. Bayarri and J.O. Berger and
                  A.P. Dawid and D. Heckerman and A.F.M. Smith and
                  M. West},
  address =	 {Oxford, UK},
  title =	 {Bayesian nonparametric latent feature models (with
                  discussion)},
  publisher =	 oup,
  pages =	 {201--226},
  year =	 2007,
  url =		 {.},
  abstract =	 {We describe a flexible nonparametric approach to
                  latent variable modelling in which the number of
                  latent variables is unbounded. This approach is
                  based on a probability distribution over equivalence
                  classes of binary matrices with a finite number of
                  rows, corresponding to the data points, and an
                  unbounded number of columns, corresponding to the
                  latent variables. Each data point can be associated
                  with a subset of the possible latent variables,
                  which we refer to as the latent features of that
                  data point. The binary variables in the matrix
                  indicate which latent feature is possessed by which
                  data point, and there is a potentially infinite
                  array of features. We derive the distribution over
                  unbounded binary matrices by taking the limit of a
                  distribution over N&times;K binary matrices as
                  K&rarr;&infin;. We define a simple generative
                  processes for this distribution which we call the
                  Indian buffet process (IBP; Griffiths and
                  Ghahramani, 2005, <a href="/pub/#GriGha06">2006</a>)
                  by analogy to the Chinese restaurant process
                  (Aldous, 1985; Pitman, 2002). The IBP has a single
                  hyperparameter which controls both the number of
                  feature per ob ject and the total number of
                  features. We describe a two-parameter generalization
                  of the IBP which has additional flexibility,
                  independently controlling the number of features per
                  object and the total number of features in the
                  matrix. The use of this distribution as a prior in
                  an infinite latent feature model is illustrated, and
                  Markov chain Monte Carlo algorithms for inference
                  are described.}
}

@inproceedings{GhaHel06,
  cat =		 {ir clust},
  author =	 {Zoubin Ghahramani and Katherine A. Heller},
  title =	 {{B}ayesian Sets},
  booktitle =	 nips18,
  year =	 2006,
  month =	 {December},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  editor =	 {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
  pages =	 {435--442},
  url =		 {.},
  abstract =	 {Inspired by "Google&trade; Sets", we consider the
                  problem of retrieving items from a concept or
                  cluster, given a query consisting of a few items
                  from that cluster. We formulate this as a Bayesian
                  inference problem and describe a very simple
                  algorithm for solving it. Our algorithm uses a
                  model-based concept of a cluster and ranks items
                  using a score which evaluates the marginal
                  probability that each item belongs to a cluster
                  containing the query items. For exponential family
                  models with conjugate priors this marginal
                  probability is a simple function of sufficient
                  statistics. We focus on sparse binary data and show
                  that our score can be evaluated exactly using a
                  single sparse matrix multiplication, making it
                  possible to apply our algorithm to very large
                  datasets. We evaluate our algorithm on three
                  datasets: retrieving movies from EachMovie, finding
                  completions of author sets from the NIPS dataset,
                  and finding completions of sets of words appearing
                  in the Grolier encyclopedia. We compare to
                  Google&trade; Sets and show that Bayesian Sets gives
                  very reasonable set completions.}
}

@article{GhaHin00a,
  author =	 {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  cat =		 {approx, time},
  journal =	 {Neural Computation},
  number =	 4,
  pages =	 {831-864},
  title =	 {Variational Learning for Switching State-Space
                  Models},
  url =		 {.},
  volume =	 12,
  year =	 2000,
  abstract =	 {We introduce a new statistical model for time series
                  which iteratively segments data into regimes with
                  approximately linear dynamics and learns the
                  parameters of each of these linear regimes. This
                  model combines and generalizes two of the most
                  widely used stochastic time series models---hidden
                  Markov models and linear dynamical systems---and is
                  closely related to models that are widely used in
                  the control and econometrics literatures. It can
                  also be derived by extending the mixture of experts
                  neural network (Jacobs et al, 1991) to its fully
                  dynamical version, in which both expert and gating
                  networks are recurrent. Inferring the posterior
                  probabilities of the hidden states of this model is
                  computationally intractable, and therefore the exact
                  Expectation Maximization (EM) algorithm cannot be
                  applied. However, we present a variational
                  approximation that maximizes a lower bound on the
                  log likelihood and makes use of both the
                  forward--backward recursions for hidden Markov
                  models and the Kalman filter recursions for linear
                  dynamical systems. We tested the algorithm both on
                  artificial data sets and on a natural data set of
                  respiration force from a patient with sleep
                  apnea. The results suggest that variational
                  approximations are a viable method for inference and
                  learning in switching state-space models.}
}

@inproceedings{GhaHin97a,
  author =	 {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  booktitle =	 {NIPS},
  cat =		 {deep},
  editor =	 {Jordan, Michael I. and Kearns, Michael J. and Solla,
                  Sara A.},
  isbn =	 {0-262-10076-2},
  publisher =	 {The MIT Press},
  title =	 {Hierarchical Non-linear Factor Analysis and
                  Topographic Maps},
  url =		 {.},
  year =	 1997,
  abstract =	 {We first describe a hierarchical, generative model
                  that can be viewed as a non-linear generalisation of
                  factor analysis and can be implemented in a neural
                  network. The model performs perceptual inference in
                  a probabilistically consistent manner by using
                  top-down, bottom-up and lateral connections. These
                  connections can be learned using simple rules that
                  require only locally available information. We then
                  show how to incorporate lateral connections into the
                  generative model. The model extracts a sparse,
                  distributed, hierarchical representation of depth
                  from simplified random-dot stereograms and the
                  localised disparity detectors in the first hidden
                  layer form a topographic map. When presented with
                  image patches from natural scenes, the model
                  develops topographically organised local feature
                  detectors.}
}

@inproceedings{GhaJor93a,
  author =	 {Ghahramani, Zoubin and Jordan, Michael I.},
  booktitle =	 {NIPS},
  cat =		 {clust},
  editor =	 {Cowan, Jack D. and Tesauro, Gerald and Alspector,
                  Joshua},
  isbn =	 {1-55860-322-0},
  pages =	 {120-127},
  publisher =	 {Morgan Kaufmann},
  title =	 {Supervised learning from incomplete data via an {EM}
                  approach},
  url =		 {.},
  year =	 1993,
  abstract =	 {Real-world learning tasks may involve
                  high-dimensional data sets with arbitrary patterns
                  of missing data. In this paper we present a
                  framework based on maximum likelihood density
                  estimation for learning from such data sets. We use
                  mixture models for the density estimates and make
                  two distinct appeals to the ExpectationMaximization
                  (EM) principle (Dempster et al., 1977) in deriving a
                  learning algorithm---EM is used both for the
                  estimation of mixture components and for coping with
                  missing data. The resulting algorithm is applicable
                  to a wide range of supervised as well as
                  unsupervised learning problems. Results from a
                  classification benchmark---the iris data set---are
                  presented.}
}

@inproceedings{GhaJor95a,
  author =	 {Ghahramani, Zoubin and Jordan, Michael I.},
  booktitle =	 {NIPS},
  cat =		 {time},
  editor =	 {Touretzky, David S. and Mozer, Michael and Hasselmo,
                  Michael E.},
  isbn =	 {0-262-20107-0},
  pages =	 {472-478},
  publisher =	 {MIT Press},
  title =	 {Factorial Hidden {M}arkov Models},
  url =		 {.},
  year =	 1995,
  abstract =	 {Hidden Markov models (HMMs) have proven to be one of
                  the most widely used tools for learning
                  probabilistic models of time series data. In an HMM,
                  information about the past is conveyed through a
                  single discrete variable---the hidden state. We
                  discuss a generalization of HMMs in which this state
                  is factored into multiple state variables and is
                  therefore represented in a distributed manner. We
                  describe an exact algorithm for inferring the
                  posterior probabilities of the hidden state
                  variables given the observations, and relate it to
                  the forward--backward algorithm for HMMs and to
                  algorithms for more general graphical models. Due to
                  the combinatorial nature of the hidden state
                  representation, this exact algorithm is
                  intractable. As in other intractable systems,
                  approximate inference can be carried out using Gibbs
                  sampling or variational methods. Within the
                  variational framework, we present a structured
                  approximation in which the the state variables are
                  decoupled, yielding a tractable algorithm for
                  learning the parameters of the model. Empirical
                  comparisons suggest that these approximations are
                  efficient and provide accurate alternatives to the
                  exact methods. Finally, we use the structured
                  approximation to model Bach's chorales and show that
                  factorial HMMs can capture statistical structure in
                  this data set which an unconstrained HMM cannot.}
}

@article{GhaJor97a,
  author =	 {Ghahramani, Zoubin and Jordan, Michael I.},
  cat =		 {time},
  journal =	 {Machine Learning},
  number =	 {2-3},
  pages =	 {245-273},
  title =	 {Factorial Hidden {M}arkov Models},
  url =		 {.},
  volume =	 29,
  year =	 1997,
  abstract =	 {Hidden Markov models (HMMs) have proven to be one of
                  the most widely used tools for learning
                  probabilistic models of time series data. In an HMM,
                  information about the past is conveyed through a
                  single discrete variable---the hidden state. We
                  discuss a generalization of HMMs in which this state
                  is factored into multiple state variables and is
                  therefore represented in a distributed manner. We
                  describe an exact algorithm for inferring the
                  posterior probabilities of the hidden state
                  variables given the observations, and relate it to
                  the forward--backward algorithm for HMMs and to
                  algorithms for more general graphical models. Due to
                  the combinatorial nature of the hidden state
                  representation, this exact algorithm is
                  intractable. As in other intractable systems,
                  approximate inference can be carried out using Gibbs
                  sampling or variational methods. Within the
                  variational framework, we present a structured
                  approximation in which the the state variables are
                  decoupled, yielding a tractable algorithm for
                  learning the parameters of the model. Empirical
                  comparisons suggest that these approximations are
                  efficient and provide accurate alternatives to the
                  exact methods. Finally, we use the structured
                  approximation to model Bach's chorales and show that
                  factorial HMMs can capture statistical structure in
                  this data set which an unconstrained HMM cannot.}
}

@inproceedings{GhaKorHin99a,
  author =	 {Ghahramani, Zoubin and Korenberg, Alexander T and
                  Hinton, Geoffrey E},
  booktitle =	 {Artificial Neural Networks, 1999. ICANN 99. Ninth
                  International Conference on (Conf. Publ. No. 470)},
  cat =		 {deep},
  organization = {IET},
  pages =	 {13--18},
  title =	 {Scaling in a hierarchical unsupervised network},
  volume =	 1,
  year =	 1999,
  url =		 {.},
  abstract =	 {A persistent worry with computational models of
                  unsupervised learning is that learning will become
                  more difficult as the problem is scaled.  We examine
                  this issue in the context of a novel hierarchical,
                  generative model that can be viewed as a non-linear
                  generalization of factor analysis and can be
                  implemented in a neural network.  The model performs
                  perceptual inference in a probabilistically
                  consistent manner by using top-down, bottom-up and
                  lateral connections.  These connections can be
                  learned using simple rules that require only locally
                  available information.  We first demonstrate that
                  the model can extract a sparse, distributed,
                  hierarchical representation of global disparity from
                  simplified random-dot stereograms.  We then
                  investigate some of the scaling properties of the
                  algorithm on this problem and find that : (1)
                  increasing the image size leads to faster and more
                  reliable learning; (2) Increasing the depth of the
                  network from one to two hidden layers leads to
                  better representations at the first hidden layer,
                  and (3) Once one part of the network has discovered
                  how to represent disparity, it 'supervises' other
                  parts of the network, greatly speeding up their
                  learning.}
}

@inproceedings{GhaRow98a,
  author =	 {Ghahramani, Zoubin and Roweis, Sam T.},
  booktitle =	 {NIPS},
  cat =		 {time, rl},
  editor =	 {Kearns, Michael J. and Solla, Sara A. and Cohn,
                  David A.},
  isbn =	 {0-262-11245-0},
  pages =	 {431-437},
  publisher =	 {The MIT Press},
  title =	 {Learning Nonlinear Dynamical Systems Using an {EM}
                  Algorithm},
  url =		 {.},
  year =	 1998,
  abstract =	 {The Expectation Maximization (EM) algorithm is an
                  iterative procedure for maximum likelihood parameter
                  estimation from data sets with missing or hidden
                  variables. It has been applied to system
                  identification in linear stochastic state-space
                  models, where the state variables are hidden from
                  the observer and both the state and the parameters
                  of the model have to be estimated simultaneously
                  [9]. We present a generalization of the EM algorithm
                  for parameter estimation in nonlinear dynamical
                  systems. The ``expectation'' step makes use of
                  Extended Kalman Smoothing to estimate the state,
                  while the ``maximization'' step re-estimates the
                  parameters using these uncertain state estimates. In
                  general, the nonlinear maximization step is
                  difficult because it requires integrating out the
                  uncertainty in the states. However, if Gaussian
                  radial basis function (RBF) approximators are used
                  to model the nonlinearities, the integrals become
                  tractable and the maximization step can be solved
                  via systems of linear equations.  }
}

@inproceedings{GhaWolJor94a,
  author =	 {Ghahramani, Zoubin and Wolpert, Daniel M. and
                  Jordan, Michael I.},
  booktitle =	 nips7,
  cat =		 {neuro},
  editor =	 {Tesauro, Gerald and Touretzky, David S. and Leen,
                  Todd K.},
  pages =	 {1125-1132},
  publisher =	 {MIT Press},
  title =	 {Computational Structure of coordinate
                  transformations: A generalization study},
  url =		 {.},
  year =	 1994,
  abstract =	 {One of the fundamental properties that both neural
                  networks and the central nervous system share is the
                  ability to learn and generalize from examples. While
                  this property has been studied extensively in the
                  neural network literature it has not been thoroughly
                  explored in human perceptual and motor learning. We
                  have chosen a coordinate transformation system---the
                  visuomotor map which transforms visual coordinates
                  into motor coordinates---to study the generalization
                  effects of learning new input--output pairs. Using a
                  paradigm of computer controlled altered visual
                  feedback, we have studied the generalization of the
                  visuomotor map subsequent to both local and
                  context-dependent remappings. A local remapping of
                  one or two input-output pairs induced a significant
                  global, yet decaying, change in the visuomotor map,
                  suggesting a representation for the map composed of
                  units with large functional receptive fields. Our
                  study of context-dependent remappings indicated that
                  a single point in visual space can be mapped to two
                  different finger locations depending on a context
                  variable---the starting point of the
                  movement. Furthermore, as the context is varied
                  there is a gradual shift between the two remappings,
                  consistent with two visuomotor modules being learned
                  and gated smoothly with the context.}
}

@article{GibSaa08,
  author =	 {Richard J. Gibbens and Yunus Saat\c{c}i},
  title =	 {Data, modelling and inference in road traffic
                  networks},
  journal =	 {Philosophical Transactions of the Royal Society A:
                  Mathematical, Physical and Engineering Sciences},
  year =	 2008,
  volume =	 366,
  pages =	 {1907--1919},
  number =	 1872,
  month =	 {June},
  abstract =	 {In this paper, we study UK road traffic data and
                  explore a range of modelling and inference questions
                  that arise from them. For example, loop detectors on
                  the M25 motorway record speed and flow measurements
                  at regularly spaced locations as well as the entry
                  and exit lanes of junctions. An exploratory study of
                  these data helps us to better understand and
                  quantify the nature of congestion on the road
                  network.  From a traveller's perspective it is
                  crucially important to understand the overall
                  journey times and we look at methods to improve our
                  ability to predict journey times given access
                  jointly to both real-time and historical loop
                  detector data. Throughout this paper we will comment
                  on related work derived from US freeway data.},
  doi =		 {10.1098/rsta.2008.0020},
  eprint =
                  {http://rsta.royalsocietypublishing.org/content/366/1872/1907.full.pdf+html},
  url =
                  {http://rsta.royalsocietypublishing.org/content/366/1872/1907.abstract}
}

@inproceedings{GilSaaCun13,
  cat =		 {approx gp},
  author =	 {E.~Gilboa and Yunus Saat\c{c}i and John
                  P.~Cunningham},
  title =	 {Scaling Multidimensional {G}aussian Processes using
                  Projected Additive Approximations},
  year =	 2013,
  booktitle =	 icml30,
  abstract =	 {Exact Gaussian Process (GP) regression has
                  O(N<sup>3</sup>) runtime for data size N, making it
                  intractable for large N. Many algorithms for
                  improving GP scaling approximate the covariance with
                  lower rank matrices. Other work has exploited
                  structure inherent in particular covariance
                  functions, including GPs with implied Markov
                  structure, and equispaced inputs (both enable O(N)
                  runtime). However, these GP advances have not been
                  extended to the multidimensional input setting,
                  despite the preponderance of multidimensional
                  applications. This paper introduces and tests novel
                  extensions of structured GPs to multidimensional
                  inputs. We present new methods for additive GPs,
                  showing a novel connection between the classic
                  backﬁtting method and the Bayesian framework.  To
                  achieve optimal accuracy-complexity tradeoff, we
                  extend this model with a novel variant of projection
                  pursuit regression. Our primary result – projection
                  pursuit Gaussian Process Regression – shows orders
                  of magnitude speedup while preserving high
                  accuracy. The natural second and third steps include
                  non-Gaussian observations and higher dimensional
                  equispaced grid methods. We introduce novel
                  techniques to address both of these necessary
                  directions. We thoroughly illustrate the power of
                  these three advances on several datasets, achieving
                  close performance to the naive Full GP at orders of
                  magnitude less cost.},
  url =		 {.}
}

@article{GilSaaCun15,
  cat =		 {gp},
  title =	 {Scaling Multidimensional Inference for Structured
                  {G}aussian Processes},
  author =	 {E.~Gilboa and Yunus Saat\c{c}i and John
                  P.~Cunningham},
  year =	 2015,
  pages =	 {424--436},
  Journal =	 PAMI,
  doi =		 {10.1109/TPAMI.2013.192},
  abstract =	 {Exact Gaussian process (GP) regression has
                  O(N<sup>3</sup> runtime for data size N, making it
                  intractable for large N. Many algorithms for
                  improving GP scaling approximate the covariance with
                  lower rank matrices. Other work has exploited
                  structure inherent in particular covariance
                  functions, including GPs with implied Markov
                  structure, and inputs on a lattice (both enable O(N)
                  or O(N log N) runtime). However, these GP advances
                  have not been well extended to the multidimensional
                  input setting, despite the preponderance of
                  multidimensional applications. This paper introduces
                  and tests three novel extensions of structured GPs
                  to multidimensional inputs, for models with additive
                  and multiplicative kernels. First we present a new
                  method for inference in additive GPs, showing a
                  novel connection between the classic backfitting
                  method and the Bayesian framework. We extend this
                  model using two advances: a variant of projection
                  pursuit regression, and a Laplace approximation for
                  non-Gaussian observations. Lastly, for
                  multiplicative kernel structure, we present a novel
                  method for GPs with inputs on a multidimensional
                  grid. We illustrate the power of these three
                  advances on several data sets, achieving performance
                  equal to or very close to the naive GP at orders of
                  magnitude less cost.},
  annote =	 {<a href="http://arxiv.org/abs/1209.4120">arXiv</a>}
}

@inproceedings{GirRasQuiMur03,
  cat =		 {gp time},
  author =	 {Agathe Girard and Carl Edward Rasmussen and Joaquin
                  Qui{\~n}onero-Candela and Roderick Murray-Smith},
  title =	 {Gaussian Process priors with uncertain inputs ---
                  application to multiple-step ahead time series
                  forecasting},
  booktitle =	 nips15,
  pages =	 {529--536},
  year =	 2003,
  month =	 {December},
  address =	 {Cambridge, MA, USA},
  editor =	 {S.~Becker and S.~Thrun and K.~Obermayer},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {We consider the problem of multi-step ahead
                  prediction in time series analysis using the
                  non-parametric Gaussian process model. k-step ahead
                  forecasting of a discrete-time non-linear dynamic
                  system can be performed by doing repeated one-step
                  ahead predictions. For a state-space model of the
                  form y<sub>t</sub> =
                  f(y<sub>t-1</sub>,...,y<sub>t-L</sub>), the
                  prediction of y at time t + k is based on the point
                  estimates of the previous outputs. In this paper, we
                  show how, using an analytical Gaussian
                  approximation, we can formally incorporate the
                  uncertainty about intermediate regressor values,
                  thus updating the uncertainty on the current
                  prediction.}
}

@article{GlaParKnoetal10,
  cat =		 {bioinf},
  author =	 {Daniel Glass and Leopold Parts and David A. Knowles
                  and Abraham Aviv and and Tim D. Spector},
  year =	 2010,
  title =	 {No Correlation Between Childhood Maltreatment and
                  Telomere Length.},
  journal =	 {Biological Psychiatry},
  volume =	 68,
  number =	 6,
  pages =	 {21--22},
  abstract =	 {Telomeres are lengths of repetitive DNA that cap the
                  ends of chromosomes. They protect the ends of the
                  chromosome and shorten with each cell
                  division. Short leukocyte telomere length has been
                  related to a number of age-related diseases. In
                  addition, shorter telomere length has been
                  associated with environmental factors such as
                  smoking and lack of exercise. In a recent issue of
                  Biological Psychiatry, Tyrka et al. (4) published a
                  report suggesting a link between maltreatment in
                  childhood and telomere shortening in 31
                  subjects. Individuals who had suffered maltreatment
                  had telomere length .70 +/- .24 compared with 1.02
                  +/- .52 in individuals who had not been abused.},
}

@inproceedings{GoeJaeRas06,
  cat =		 {np},
  author =	 {Dilan G{\"o}r{\"u}r and Frank J{\"a}kel and Carl
                  Edward Rasmussen},
  title =	 {A Choice Model with Infinitely Many Latent Features},
  year =	 2006,
  publisher =	 {ACM Press},
  pages =	 {361--368},
  month =	 {June},
  booktitle =	 icml23,
  editor =	 {W.~W.~Cohen and Andrew Moore},
  address =	 {New York, NY, USA},
  doi =		 {10.1145/1143844.1143890},
  abstract =	 {Elimination by aspects (EBA) is a probabilistic
                  choice model describing how humans decide between
                  several options.  The options from which the choice
                  is made are characterized by binary features and
                  associated weights. For instance, when choosing
                  which mobile phone to buy the features to consider
                  may be: long lasting battery, color screen,
                  etc. Existing methods for inferring the parameters
                  of the model assume pre-specified features. However,
                  the features that lead to the observed choices are
                  not always known.  Here, we present a non-parametric
                  Bayesian model to infer the features of the options
                  and the corresponding weights from choice data. We
                  use the Indian buffet process (IBP) as a prior over
                  the features. Inference using Markov chain Monte
                  Carlo (MCMC) in conjugate IBP models has been
                  previously described. The main contribution of this
                  paper is an MCMC algorithm for the EBA model that
                  can also be used in inference for other
                  non-conjugate IBP models---this may broaden the use
                  of IBP priors considerably.},
  location =	 {Pittsburgh, PA, USA},
  url =		 {.}
}

@article{GoeRas10,
  cat =		 {np},
  title =	 {{D}irichlet Process {G}aussian Mixture Models:
                  Choice of the base distribution},
  author =	 {Dilan G{\"o}r{\"u}r and Carl Edward Rasmussen},
  journal =	 jcst,
  publisher =	 {Science Press},
  address =	 {Beijing, China},
  volume =	 25,
  pages =	 {615--625},
  month =	 {July},
  year =	 2010,
  number =	 4,
  url =		 {.},
  doi =		 {10.1007/s11390-010-9355-8},
  abstract =	 {In the Bayesian mixture modeling framework it is
                  possible to infer the necessary number of components
                  to model the data and therefore it is unnecessary to
                  explicitly restrict the number of
                  components. Nonparametric mixture models sidestep
                  the problem of finding the "correct" number of
                  mixture components by assuming infinitely many
                  components. In this paper Dirichlet process mixture
                  (DPM) models are cast as infinite mixture models and
                  inference using Markov chain Monte Carlo is
                  described. The specification of the priors on the
                  model parameters is often guided by mathematical and
                  practical convenience. The primary goal of this
                  paper is to compare the choice of conjugate and
                  non-conjugate base distributions on a particular
                  class of DPM models which is widely used in
                  applications, the Dirichlet process Gaussian mixture
                  model (DPGMM). We compare computational efficiency
                  and modeling performance of DPGMM defined using a
                  conjugate and a conditionally conjugate base
                  distribution. We show that better density models can
                  result from using a wider class of priors with no or
                  only a modest increase in computational effort.}
}

@inproceedings{GoeRasToletal04,
  author =	 {Dilan G{\"o}r{\"u}r and Carl Edward Rasmussen and
                  Andreas S.~Tolias and Fabian Sinz and Nikos
                  K.~Logothetis},
  title =	 {Modelling Spikes with Mixtures of Factor Analysers},
  year =	 2004,
  series =	 lncs,
  publisher =	 {Springer},
  pages =	 {391--398},
  month =	 09,
  volume =	 3175,
  journal =	 {Pattern Recognition: Proceedings of the 26th DAGM
                  Symposium},
  editor =	 {C.~E.~Rasmussen and H.~H.~B{\"u}lthoff and
                  B.~Sch{\"o}lkopf and M.~A.~Giese},
  address =	 {Berlin, Germany},
  abstract =	 {Identifying the action potentials of individual
                  neurons from extracellular recordings, known as
                  spike sorting, is a challenging problem. We consider
                  the spike sorting problem using a generative
                  model,mixtures of factor analysers, which
                  concurrently performs clustering and feature
                  extraction. The most important advantage of this
                  method is that it quantifies the certainty with
                  which the spikes are classified. This can be used as
                  a means for evaluating the quality of clustering and
                  therefore spike isolation. Using this method, nearly
                  simultaneously occurring spikes can also be modelled
                  which is a hard task for many of the spike sorting
                  methods. Furthermore, modelling the data with a
                  generative model allows us to generate simulated
                  data.},
  booktitle =	 {DAGM 2004},
  location =	 {T{\"u}bingen, Germany},
  url =		 {.}
}

@inproceedings{GolAndVanSetetal06,
  author =	 {Goldberg, A. B. and Andrzejewski, D. and {Van Gael},
                  J. and Settles, B. and Zhu, X. and Craven, M.},
  shorttitle =	 {Ranking Biomedical Passages for Relevance and Dive},
  title =	 {Ranking Biomedical Passages for Relevance and
                  Diversity: {U}niversity of {W}isconsin, {M}adison at
                  {TREC} {G}enomics 2006},
  booktitle =	 {Proceedings of the Fifteenth Text REtrieval
                  Conference (TREC 2006)},
  url =		 {.},
  year =	 2006,
  month =	 {November},
  address =	 {Gaithersburg, MD, USA},
  abstract =	 {We report on the University of Wisconsin, Madison's
                  experience in the TREC Genomics 2006 track, which
                  asks participants to retrieve passages from
                  scientific articles that satisfy biologists'
                  information needs. An emphasis is placed on
                  returning relevant passages that discuss different
                  aspects of the topic. Using an off-the-shelf
                  information retrieval (IR) engine, we focused on
                  query generation and reranking query results to
                  encourage relevance and diversity. For query
                  generation, we automatically identify noun phrases
                  from the topic descriptions, and use online
                  resources to gather synonyms as expansion terms. Our
                  first submission uses the baseline IR engine
                  results. We rerank the passages using a naive
                  clustering-based approach in our second run, and we
                  test GRASSHOPPER, a novel graph-theoretic algorithm
                  based on absorbing random walks, in our third
                  run. While our aspect-level results appear to
                  compare favorably with other participants on
                  average, our query generation techniques failed to
                  produce adequate query results for several topics,
                  causing our passage and document-level evaluation
                  scores to suffer.  Furthermore, we surprisingly
                  achieved higher aspect-level scores using the
                  initial ranking than our methods aimed specifically
                  at promoting diversity. While this sounds
                  discouraging, we have several ideas as to why this
                  happened and hope to produce new methods that
                  correct these shortcomings.}
}

@inproceedings{GolGe22,
  title =	 {Learning Deep Neural Networks Through Iterative
                  Linearisation},
  cat =		 {deep},
  author =	 {Goldwaser, Adrian and Ge, Hong},
  url =		 {https://arxiv.org/abs/2211.12345},
  booktitle =	 {Neurips 2022 Workshop Optimisation in Machine
                  Learning},
  year =	 2022,
  abstract =	 {The excellent real-world performance of deep neural
                  networks has received increasing attention. Despite
                  the capacity to overfit significantly, such large
                  models work better than smaller ones.  This
                  phenomenon is often referred to as the scaling law
                  by practitioners. It is of fundamental interest to
                  study why the scaling law exists and how it
                  avoids/controls overfitting. One approach has been
                  looking at infinite width limits of neural networks
                  (e.g., Neural Tangent Kernels, Gaussian Processes);
                  however, in practise, these do not fully explain
                  finite networks as their infinite counterparts do
                  not learn features.  Furthermore, the empirical
                  kernel for finite networks (i.e., the inner product
                  of feature vectors), changes significantly during
                  training in contrast to infinite width networks. In
                  this work we derive an iterative linearised training
                  method. We justify iterative lineralisation as an
                  interpolation between finite analogs of the infinite
                  width regime, which do not learn features, and
                  standard gradient descent training which does. We
                  show some preliminary results where iterative
                  linearised training works well, noting in particular
                  how much feature learning is required to achieve
                  comparable performance. We also provide novel
                  insights into the training behaviour of neural
                  networks.}
}

@inproceedings{GolZhuVanAnd07,
  author =	 {Goldberg, A.B. and Zhu, X. and {Van Gael}, J. and
                  Andrzejewski, D.},
  booktitle =	 {Proceedings of NAACL HLT},
  pages =	 {97--104},
  address =	 {Rochester, NY, USA},
  title =	 {Improving diversity in ranking using absorbing
                  random walks},
  url =		 {.},
  year =	 2007,
  month =	 {April}
}

@inproceedings{GorBroBauetal19,
  cat =		 {approx deep},
  title =	 {Meta-learning probabilistic inference for
                  prediction},
  booktitle =	 iclr07,
  year =	 2019,
  month =	 {April},
  address =	 {New Orleans},
  author =	 {Jonathan Gordon and John Bronskill and Matthias
                  Bauer and S<!>ebastian Nowozin and Richard Turner},
  url =		 {https://openreview.net/forum?id=HkxStoC5F7},
  abstract =	 {This paper introduces a new framework for data
                  efficient and versatile learning. Specifically: 1)
                  We develop ML-PIP, a general framework for
                  Meta-Learning approximate Probabilistic Inference
                  for Prediction. ML-PIP extends existing
                  probabilistic interpretations of meta-learning to
                  cover a broad class of methods. 2) We introduce
                  \Versa{}, an instance of the framework employing a
                  flexible and versatile amortization network that
                  takes few-shot learning datasets as inputs, with
                  arbitrary numbers of shots, and outputs a
                  distribution over task-specific parameters in a
                  single forward pass. Versa substitutes optimization
                  at test time with forward passes through inference
                  networks, amortizing the cost of inference and
                  relieving the need for second derivatives during
                  training.  3) We evaluate \Versa{} on benchmark
                  datasets where the method sets new state-of-the-art
                  results, and can handle arbitrary number of shots,
                  and for classification, arbitrary numbers of classes
                  at train and test time. The power of the approach is
                  then demonstrated through a challenging few-shot
                  ShapeNet view reconstruction task.}
}

@inproceedings{GorBruFooetal20,
  cat =		 {approx deep},
  title =	 {Convolutional Conditional Neural Processes},
  booktitle =	 iclr08,
  year =	 2020,
  month =	 {April},
  address =	 {Adis Ababa},
  author =	 {Jonathan Gordon and Wessel Bruinsma and Andrew
                  Y. K. Foong and James Requeima and Yann Dubois and
                  Richard Turner},
  url =		 {https://openreview.net/forum?id=Skey4eBYPS},
  abstract =	 {We introduce the Convolutional Conditional Neural
                  Process (ConvCNP), a new member of the Neural
                  Process family that models translation equivariance
                  in the data. Translation equivariance is an
                  important inductive bias for many learning problems
                  including time series modelling, spatial data, and
                  images. The model embeds data sets into an
                  infinite-dimensional function space, as opposed to
                  finite-dimensional vector spaces. To formalize this
                  notion, we extend the theory of neural
                  representations of sets to include functional
                  representations, and demonstrate that any
                  translation-equivariant embedding can be represented
                  using a convolutional deep-set.  We evaluate
                  ConvCNPs in several settings, demonstrating that
                  they achieve state-of-the-art performance compared
                  to existing NPs. We demonstrate that building in
                  translation equivariance enables zero-shot
                  generalization to challenging, out-of-domain tasks.}
}

@inproceedings{Greseleetal21,
  cat =		 {causal},
  title =	 {Independent mechanisms analysis, a new concept?},
  author =	 {Gresele*, L. and von K{\"u}gelgen*, J. and Stimper,
                  V. and Sch{\"o}lkopf, B. and Besserve, M.},
  booktitle =	 nips34,
  pages =	 {28233--28248},
  editors =	 {M. Ranzato and A. Beygelzimer and Y. Dauphin and
                  P.S. Liang and J. Wortman Vaughan},
  publisher =	 {Curran Associates, Inc.},
  year =	 2021,
  note =	 {*equal contribution},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/edc27f139c3b4e4bb29d1cdbc45663f9-Paper.pdf},
  abstract =	 {Independent component analysis provides a principled
                  framework for unsupervised representation learning,
                  with solid theory on the identifiability of the
                  latent code that generated the data, given only
                  observations of mixtures thereof. Unfortunately,
                  when the mixing is nonlinear, the model is provably
                  nonidentifiable, since statistical independence
                  alone does not sufficiently constrain the
                  problem. Identifiability can be recovered in
                  settings where additional, typically observed
                  variables are included in the generative process. We
                  investigate an alternative path and consider instead
                  including assumptions reflecting the principle of
                  independent causal mechanisms exploited in the field
                  of causality. Specifically, our approach is
                  motivated by thinking of each source as
                  independently influencing the mixing process. This
                  gives rise to a framework which we term independent
                  mechanism analysis. We provide theoretical and
                  empirical evidence that our approach circumvents a
                  number of nonidentifiability issues arising in
                  nonlinear blind source separation.},
}

@inproceedings{Greseleetal22,
  cat =		 {causal},
  title =	 {Causal Inference Through the Structural Causal
                  Marginal Problem},
  author =	 {Gresele*, L. and von K{\"u}gelgen*, J. and
                  K{\"u}bler*, J. M. and Kirschbaum, E. and
                  Sch{\"o}lkopf, B. and Janzing, D.},
  booktitle =	 icml39,
  volume =	 162,
  pages =	 {7793--7824},
  editors =	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song,
                  Le and Szepesvari, Csaba and Niu, Gang and Sabato,
                  Sivan},
  publisher =	 {PMLR},
  year =	 2022,
  note =	 {*equal contribution},
  url =		 {https://proceedings.mlr.press/v162/gresele22a.html},
  abstract =	 {We introduce an approach to counterfactual inference
                  based on merging information from multiple
                  datasets. We consider a causal reformulation of the
                  statistical marginal problem: given a collection of
                  marginal structural causal models (SCMs) over
                  distinct but overlapping sets of variables,
                  determine the set of joint SCMs that are
                  counterfactually consistent with the marginal
                  ones. We formalise this approach for categorical
                  SCMs using the response function formulation and
                  show that it reduces the space of allowed marginal
                  and joint SCMs. Our work thus highlights a new mode
                  of falsifiability through additional variables, in
                  contrast to the statistical one via additional
                  data.},
}

@inproceedings{GrgRedGumetal18,
  cat =		 {fairness},
  title =	 {Human perceptions of fairness in algorithmic
                  decision making: A case study of criminal risk
                  prediction},
  booktitle =	 www2018,
  year =	 2018,
  month =	 {April},
  address =	 {Lyon},
  abstract =	 {As algorithms are increasingly used to make
                  important decisions that affect human lives, ranging
                  from social benefit assignment to predicting risk of
                  criminal recidivism, concerns have been raised about
                  the fairness of algorithmic decision making. Most
                  prior works on algorithmic fairness normatively
                  prescribe how fair decisions ought to be made. In
                  contrast, here, we descriptively survey users for
                  how they perceive and reason about fairness in
                  algorithmic decision making.  A key contribution of
                  this work is the framework we propose to understand
                  why people perceive certain features as fair or
                  unfair to be used in algorithms. Our framework
                  identifies eight properties of features, such as
                  relevance, volitionality and reliability, as latent
                  considerations that inform people’s moral judgments
                  about the fairness of feature use in decision-making
                  algorithms. We validate our framework through a
                  series of scenario-based surveys with 576 people. We
                  find that, based on a person’s assessment of the
                  eight latent properties of a feature in our exemplar
                  scenario, we can accurately (> 85%) predict if the
                  person will judge the use of the feature as fair.
                  Our findings have important implications. At a
                  high-level, we show that people’s unfairness
                  concerns are multi-dimensional and argue that future
                  studies need to address unfairness concerns beyond
                  discrimination. At a low-level, we find considerable
                  disagreements in people’s fairness judgments. We
                  identify root causes of the disagreements, and note
                  possible pathways to resolve them.  },
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/WWW18-HumanPerceptions.pdf},
  author =	 {Nina Grgi\'c-Hla{\v c}a and Elissa Redmiles and
                  Krishna P. Gummadi and Adrian Weller}
}

@inproceedings{GrgZafGumetal18,
  cat =		 {fairness},
  title =	 {Beyond Distributive Fairness in Algorithmic Decision
                  Making: Feature Selection for Procedurally Fair
                  Learning},
  booktitle =	 aaai32,
  year =	 2018,
  month =	 {February},
  address =	 {New Orleans},
  author =	 {N. Grgi\'c-Hla{\v c}a and M. B. Zafar and
                  K. P. Gummadi and A. Weller},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/AAAI18-BeyondDistributiveFairness.pdf},
  abstract =	 {With wide-spread usage of machine learning methods
                  in numerous domains involving human subjects,
                  several studies have raised questions about the
                  potential for unfairness towards certain individuals
                  or groups. A number of recent works have proposed
                  methods to measure and eliminate unfairness from
                  machine learning methods. However, most of this work
                  on fair learning has focused on only one dimension
                  of fair decision making: distributive fairness,
                  i.e., the fairness of the decision outcomes. In this
                  work, we leverage the rich literature on
                  organizational justice and focus on another
                  dimension of fair decision making: procedural
                  fairness, i.e., the fairness of the decision making
                  process. We propose measures for procedural fairness
                  that consider the input features used in the
                  decision process, and evaluate the moral judgments
                  of humans regarding the use of these features. We
                  operationalize these measures on two real world
                  datasets using human surveys on the Amazon
                  Mechanical Turk (AMT) platform, demonstrating that
                  we capture important properties of procedurally fair
                  decision making. We provide fast submodular
                  mechanisms to optimize the tradeoff between
                  procedural fairness and prediction accuracy. On our
                  datasets, we observe empirically that procedural
                  fairness may be achieved with little cost to outcome
                  fairness, but that some loss of accuracy is
                  unavoidable.}
}

@inproceedings{GriGha06,
  cat =		 {np},
  booktitle =	 nips18,
  year =	 2006,
  month =	 {December},
  title =	 {Infinite Latent Feature Models and the {I}ndian
                  {B}uffet {P}rocess},
  author =	 {T. L. Griffiths and Z. Ghahramani},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  editor =	 {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
  pages =	 {475--482},
  url =		 {.},
  abstract =	 {We define a probability distribution over
                  equivalence classes of binary matrices with a finite
                  number of rows and an unbounded number of
                  columns. This distribution is suitable for use as a
                  prior in probabilistic models that represent objects
                  using a potentially infinite array of features. We
                  identify a simple generative process that results in
                  the same distribution over equivalence classes,
                  which we call the Indian buffet process. We
                  illustrate the use of this distribution as a prior
                  in an infinite latent feature model, deriving a
                  Markov chain Monte Carlo algorithm for inference in
                  this model and applying the algorithm to an image
                  dataset.}
}

@article{GriGha11,
  cat =		 {np review},
  volume =	 12,
  month =	 {April},
  author =	 {Thomas L. Griffiths and Zoubin Ghahramani},
  title =	 {The {Indian} buffet process: {An} introduction and
                  review},
  journal =	 jmlr,
  pages =	 {1185--1224},
  year =	 2011,
  url =		 {.},
  abstract =	 {The Indian buffet process is a stochastic process
                  defining a probability distribution over equivalence
                  classes of sparse binary matrices with a finite
                  number of rows and an unbounded number of
                  columns. This distribution is suitable for use as a
                  prior in probabilistic models that represent objects
                  using a potentially infinite array of features, or
                  that involve bipartite graphs in which the size of
                  at least one class of nodes is unknown. We give a
                  detailed derivation of this distribution, and
                  illustrate its use as a prior in an infinite latent
                  feature model. We then review recent applications of
                  the Indian buffet process in machine learning,
                  discuss its extensions, and summarize its
                  connections to other stochastic processes.}
}

@inproceedings{GuGhaTur15,
  cat =		 {approx gm deep time},
  title =	 {Neural Adaptive Sequential {M}onte {C}arlo},
  author =	 {Shixiang Gu and Zoubin Ghahramani and Richard
                  E.~Turner},
  booktitle =	 nips29,
  year =	 2015,
  month =	 {Dec},
  address =	 {Montréal CANADA},
  url =		 {http://arxiv.org/abs/1506.03338},
  abstract =	 {Sequential Monte Carlo (SMC), or particle filtering,
                  is a popular class of methods for sampling from an
                  intractable target distribution using a sequence of
                  simpler intermediate distributions. Like other
                  importance sampling-based methods, performance is
                  critically dependent on the proposal distribution: a
                  bad proposal can lead to arbitrarily inaccurate
                  estimates of the target distribution. This paper
                  presents a new method for automatically adapting the
                  proposal using an approximation of the
                  Kullback-Leibler divergence between the true
                  posterior and the proposal distribution. The method
                  is very flexible, applicable to any parameterised
                  proposal distribution and it supports online and
                  batch variants. We use the new framework to adapt
                  powerful proposal distributions with rich
                  parameterisations based upon neural networks leading
                  to Neural Adaptive Sequential Monte Carlo
                  (NASMC). Experiments indicate that NASMC
                  significantly improves inference in a non-linear
                  state space model outperforming adaptive proposal
                  methods including the Extended Kalman and Unscented
                  Particle Filters. Experiments also indicate that
                  improved inference translates into improved
                  parameter learning when NASMC is used as a
                  subroutine of Particle Marginal Metropolis
                  Hastings. Finally we show that NASMC is able to
                  train a neural network-based deep recurrent
                  generative model achieving results that compete with
                  the state-of-the-art for polymorphic music
                  modelling. NASMC can be seen as bridging the gap
                  between adaptive SMC methods and the recent work in
                  scalable, black-box variational inference.}
}

@inproceedings{GuHerZou14,
  cat =		 {approx gm},
  title =	 {Gaussian Process Volatility Model},
  author =	 {Yue Wu and Jos\'e Miguel Hern\'andez-Lobato and
                  Zoubin Ghahramani},
  booktitle =	 nips28,
  year =	 2014,
  month =	 {December},
  address =	 {Montreal, Canada},
  url =
                  {http://jmhldotorg.files.wordpress.com/2014/10/nips2014gpvol1.pdf},
  pages =	 {1--9},
  abstract =	 { The prediction of time-changing variances is an
                  important task in the modeling of financial
                  data. Standard econometric models are often limited
                  as they assume rigid functional relationships for
                  the evolution of the variance. Moreover, functional
                  parameters are usually learned by maximum
                  likelihood, which can lead to overfitting. To
                  address these problems we introduce GP-Vol, a novel
                  non-parametric model for time-changing variances
                  based on Gaussian Processes. This new model can
                  capture highly flexible functional relationships for
                  the variances. Furthermore, we introduce a new
                  online algorithm for fast inference in GP-Vol. This
                  method is much faster than current offline inference
                  procedures and it avoids overfitting problems by
                  following a fully Bayesian approach. Experiments
                  with financial data show that GP-Vol performs
                  significantly better than current standard
                  alternatives. }
}

@inproceedings{GuHolLilLev17,
  cat =		 {rl deep},
  title =	 {Deep Reinforcement Learning for Robotic Manipulation
                  with Asynchronous Off-Policy Updates},
  author =	 {Shixiang Gu and Ethan Holly and Timothy Lillicrap
                  and Sergey Levine},
  booktitle =	 icra2017,
  year =	 2017,
  month =	 {May},
  address =	 {SINGAPORE},
  url =		 {https://arxiv.org/abs/1610.00633},
  abstract =	 {Reinforcement learning holds the promise of enabling
                  autonomous robots to learn large repertoires of
                  behavioral skills with minimal human
                  intervention. However, robotic applications of
                  reinforcement learning often compromise the autonomy
                  of the learning process in favor of achieving
                  training times that are practical for real physical
                  systems. This typically involves introducing
                  hand-engineered policy representations and
                  human-supplied demonstrations. Deep reinforcement
                  learning alleviates this limitation by training
                  general-purpose neural network policies, but
                  applications of direct deep reinforcement learning
                  algorithms have so far been restricted to simulated
                  settings and relatively simple tasks, due to their
                  apparent high sample complexity. In this paper, we
                  demonstrate that a recent deep reinforcement
                  learning algorithm based on off-policy training of
                  deep Q-functions can scale to complex 3D
                  manipulation tasks and can learn deep neural network
                  policies efficiently enough to train on real
                  physical robots. We demonstrate that the training
                  times can be further reduced by parallelizing the
                  algorithm across multiple robots which pool their
                  policy updates asynchronously. Our experimental
                  evaluation shows that our method can learn a variety
                  of 3D manipulation skills in simulation and a
                  complex door opening skill on real robots without
                  any prior demonstrations or manually designed
                  representations.},
  annote =	 {[<a
                  href="https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html">Google
                  Blogpost</a>] [<a
                  href="https://www.technologyreview.com/s/602529/google-builds-a-robotic-hive-mind-kindergarten/">MIT
                  Technology Review</a>] [<a
                  href="https://www.youtube.com/watch?v=ZhsEKTo7V04">Video</a>]
                  }
}

@inproceedings{GuLevSutMni16,
  cat =		 {approx deep gm},
  title =	 {MuProp: Unbiased Backpropagation for Stochastic
                  Neural Networks},
  author =	 {Shixiang Gu and Sergey Levine and Ilya Sutskever and
                  Andriy Mnih},
  booktitle =	 iclr04,
  year =	 2016,
  month =	 {May},
  address =	 {San Juan PUERTO RICO},
  url =		 {https://arxiv.org/abs/1511.05176},
  abstract =	 {Deep neural networks are powerful parametric models
                  that can be trained efficiently using the
                  backpropagation algorithm. Stochastic neural
                  networks combine the power of large parametric
                  functions with that of graphical models, which makes
                  it possible to learn very complex
                  distributions. However, as backpropagation is not
                  directly applicable to stochastic networks that
                  include discrete sampling operations within their
                  computational graph, training such networks remains
                  difficult. We present MuProp, an unbiased gradient
                  estimator for stochastic networks, designed to make
                  this task easier. MuProp improves on the
                  likelihood-ratio estimator by reducing its variance
                  using a control variate based on the first-order
                  Taylor expansion of a mean-field network. Crucially,
                  unlike prior attempts at using backpropagation for
                  training stochastic networks, the resulting
                  estimator is unbiased and well behaved. Our
                  experiments on structured output prediction and
                  discrete latent variable modeling demonstrate that
                  MuProp yields consistently good performance across a
                  range of difficult tasks.}
}

@inproceedings{GuLilGhaTuretal17a,
  cat =		 {rl deep},
  title =	 {Q-Prop: Sample-Efficient Policy Gradient with An
                  Off-Policy Critic},
  author =	 {Shixiang Gu and Timothy Lillicrap and Zoubin
                  Ghahramani and Richard E. Turner and Sergey Levine},
  booktitle =	 iclr05,
  year =	 2017,
  month =	 {April},
  address =	 {Toulon France},
  url =		 {https://arxiv.org/abs/1611.02247},
  abstract =	 {Model-free deep reinforcement learning (RL) methods
                  have been successful in a wide variety of simulated
                  domains. However, a major obstacle facing deep RL in
                  the real world is their high sample
                  complexity. Batch policy gradient methods offer
                  stable learning, but at the cost of high variance,
                  which often requires large batches. TD-style
                  methods, such as off-policy actor-critic and
                  Q-learning, are more sample-efficient but biased,
                  and often require costly hyperparameter sweeps to
                  stabilize. In this work, we aim to develop methods
                  that combine the stability of policy gradients with
                  the efficiency of off-policy RL. We present Q-Prop,
                  a policy gradient method that uses a Taylor
                  expansion of the off-policy critic as a control
                  variate. Q-Prop is both sample efficient and stable,
                  and effectively combines the benefits of on-policy
                  and off-policy methods. We analyze the connection
                  between Q-Prop and existing model-free algorithms,
                  and use control variate theory to derive two
                  variants of Q-Prop with conservative and aggressive
                  adaptation. We show that conservative Q-Prop
                  provides substantial gains in sample efficiency over
                  trust region policy optimization (TRPO) with
                  generalized advantage estimation (GAE), and improves
                  stability over deep deterministic policy gradient
                  (DDPG), the state-of-the-art on-policy and
                  off-policy methods, on OpenAI Gym's MuJoCo
                  continuous control environments.}
}

@inproceedings{GuLilGhaTuretal17b,
  cat =		 {rl deep},
  title =	 {Interpolated Policy Gradient: Merging On-Policy and
                  Off-Policy Policy Gradient Estimation for Deep
                  Reinforcement Learning},
  author =	 {Shixiang Gu and Timothy Lillicrap and Zoubin
                  Ghahramani and Richard E. Turner and Bernhard
                  Sch\"{o}lkopf and Sergey Levine},
  booktitle =	 nips31,
  year =	 2017,
  month =	 {Dec},
  address =	 {Long Beach USA},
  url =		 {https://arxiv.org/abs/1706.00387},
  abstract =	 {Off-policy model-free deep reinforcement learning
                  methods using previously collected data can improve
                  sample efficiency over on-policy policy gradient
                  techniques. On the other hand, on-policy algorithms
                  are often more stable and easier to use. This paper
                  examines, both theoretically and empirically,
                  approaches to merging on- and off-policy updates for
                  deep reinforcement learning. Theoretical results
                  show that off-policy updates with a value function
                  estimator can be interpolated with on-policy policy
                  gradient updates whilst still satisfying performance
                  bounds. Our analysis uses control variate methods to
                  produce a family of policy gradient algorithms, with
                  several recently proposed algorithms being special
                  cases of this family. We then provide an empirical
                  comparison of these techniques with the remaining
                  algorithmic details fixed, and show how different
                  mixing of off-policy gradient estimates with
                  on-policy samples contribute to improvements in
                  empirical performance. The final algorithm provides
                  a generalization and unification of existing deep
                  policy gradient techniques, has theoretical
                  guarantees on the bias introduced by off-policy
                  updates, and improves on the state-of-the-art
                  model-free deep RL methods on a number of OpenAI Gym
                  continuous control benchmarks.}
}

@inproceedings{GuLilSutLev16,
  cat =		 {deep rl},
  title =	 {Continuous Deep Q-Learning with Model-based
                  Acceleration},
  author =	 {Shixiang Gu and Timothy Lillicrap and Ilya Sutskever
                  and Sergey Levine},
  booktitle =	 icml33,
  year =	 2016,
  month =	 {June},
  address =	 {New York USA},
  url =		 {https://arxiv.org/abs/1603.00748},
  abstract =	 {Model-free reinforcement learning has been
                  successfully applied to a range of challenging
                  problems, and has recently been extended to handle
                  large neural network policies and value
                  functions. However, the sample complexity of
                  model-free algorithms, particularly when using
                  high-dimensional function approximators, tends to
                  limit their applicability to physical systems. In
                  this paper, we explore algorithms and
                  representations to reduce the sample complexity of
                  deep reinforcement learning for continuous control
                  tasks. We propose two complementary techniques for
                  improving the efficiency of such algorithms. First,
                  we derive a continuous variant of the Q-learning
                  algorithm, which we call normalized adantage
                  functions (NAF), as an alternative to the more
                  commonly used policy gradient and actor-critic
                  methods. NAF representation allows us to apply
                  Q-learning with experience replay to continuous
                  tasks, and substantially improves performance on a
                  set of simulated robotic control tasks. To further
                  improve the efficiency of our approach, we explore
                  the use of learned models for accelerating
                  model-free reinforcement learning. We show that
                  iteratively refitted local linear models are
                  especially effective for this, and demonstrate
                  substantially faster learning on domains where such
                  models are applicable.}
}

@inproceedings{GuaDyNiuetal10,
  cat =		 {np clust},
  author =	 {Y. Guan and J. G. Dy and D. Niu and Z. Ghahramani},
  title =	 {Variational inference for nonparametric multiple
                  clustering},
  booktitle =	 {KDD10 Workshop on Discovering, Summarizing, and
                  Using Multiple Clusterings},
  address =	 {Washington, DC, USA},
  year =	 2010,
  month =	 {July},
  url =		 {.},
  abstract =	 {Most clustering algorithms produce a single
                  clustering solution.  Similarly, feature selection
                  for clustering tries to find one feature subset
                  where one interesting clustering solution
                  resides. However, a single data set may be
                  multi-faceted and can be grouped and interpreted in
                  many different ways, especially for high dimensional
                  data, where feature selection is typically
                  needed. Moreover, different clustering solutions are
                  interesting for different purposes. Instead of
                  committing to one clustering solution, in this paper
                  we introduce a probabilistic nonparametric Bayesian
                  model that can discover several possible clustering
                  solutions and the feature subset views that
                  generated each cluster partitioning
                  simultaneously. We provide a variational inference
                  approach to learn the features and clustering
                  partitions in each view. Our model allows us not
                  only to learn the multiple clusterings and views but
                  also allows us to automatically learn the number of
                  views and the number of clusters in each view.}
}

@inproceedings{HalRasMac11,
  cat =		 {rl},
  author =	 {Joseph Hall and Carl Edward Rasmussen and Jan
                  Maciejowski},
  title =	 {Reinforcement Learning with Reference Tracking
                  Control in Continuous State Spaces},
  url =		 {.},
  booktitle =	 {Proceedings of 50th IEEE Conference on Decision and
                  Control and European Control Conference},
  year =	 2011,
  abstract =	 {The contribution described in this paper is an
                  algorithm for learning nonlinear, reference
                  tracking, control policies given no prior knowledge
                  of the dynamical system and limited interaction with
                  the system through the learning process. Concepts
                  from the field of reinforcement learning, Bayesian
                  statistics and classical control have been brought
                  together in the formulation of this algorithm which
                  can be viewed as a form indirect self tuning
                  regulator. On the task of reference tracking using
                  the inverted pendulum it was shown to yield
                  generally improved performance on the best
                  controller derived from the standard linear
                  quadratic method using only 30 s of total
                  interaction with the system. Finally, the algorithm
                  was shown to work on the double pendulum proving its
                  ability to solve nontrivial control tasks.}
}

@inproceedings{HalRasMac12,
  cat =		 {rl gp},
  author =	 {Joseph Hall and Carl Edward Rasmussen and Jan
                  Maciejowski},
  title =	 {Modelling and Control of Nonlinear Systems using
                  {G}aussian Processes with Partial Model Information},
  url =		 {.},
  booktitle =	 {51st IEEE Conference on Decision and Control},
  year =	 2012,
  abstract =	 {Gaussian processes are gaining increasing popularity
                  among the control community, in particular for the
                  modelling of discrete time state space
                  systems. However, it has not been clear how to
                  incorporate model information, in the form of known
                  state relationships, when using a Gaussian process
                  as a predictive model. An obvious example of known
                  prior information is position and velocity related
                  states.  Incorporation of such information would be
                  beneficial both computationally and for faster
                  dynamics learning. This paper introduces a method of
                  achieving this, yielding faster dynamics learning
                  and a reduction in computational effort from
                  O(Dn<sup>2</sup>) to O((D-F)n<sup>2</sup>) in the
                  prediction stage for a system with D states, F known
                  state relationships and n observations.  The
                  effectiveness of the method is demonstrated through
                  its inclusion in the PILCO learning algorithm with
                  application to the swing-up and balance of a
                  torque-limited pendulum and the balancing of a
                  robotic unicycle in simulation.}
}

@article{HanRas94,
  cat =		 {approx},
  author =	 {Lars Kai Hansen and Carl Edward Rasmussen},
  title =	 {Pruning from adaptive regularization},
  journal =	 nc,
  number =	 6,
  volume =	 6,
  pages =	 {1222--1231},
  year =	 1994,
  abstract =	 {Inspired by the recent upsurge of interest in
                  Bayesian methods we consider adaptive
                  regularization. A generalization based scheme for
                  adaptation of regularization parameters is
                  introduced and compared to Bayesian
                  regularization. We show that pruning arises
                  naturally within both adaptive regularization
                  schemes. As model example we have chosen the
                  simplest possible: estimating the mean of a random
                  variable with known variance. Marked similarities
                  are found between the two methods in that they both
                  involve a "noise limit", below which they regularize
                  with infinite weight decay, i.e., they
                  prune. However, pruning is not always beneficial. We
                  show explicitly that both methods in some cases may
                  increase the generalization error. This corresponds
                  to situations where the underlying assumptions of
                  the regularizer are poorly matched to the
                  environment.},
  url =		 {.},
  publisher =	 mit
}

@inproceedings{HeaGha13,
  author =	 {Creighton Heaukulani and Zoubin Ghahramani},
  cat =		 {time network},
  title =	 {Dynamic probabilistic models for latent feature
                  propagation in social networks},
  booktitle =	 icml30,
  year =	 2013,
  month =	 {June},
  address =	 {Atlanta, Georgia, USA},
  url =
                  {http://jmlr.csail.mit.edu/proceedings/papers/v28/heaukulani13.pdf},
  abstract =	 {Current Bayesian models for dynamic social network
                  data have focused on modelling the influence of
                  evolving unobserved structure on observed social
                  interactions. However, an understanding of how
                  observed social relationships from the past affect
                  future unobserved structure in the network has been
                  neglected. In this paper, we introduce a new
                  probabilistic model for capturing this phenomenon,
                  which we call latent feature propagation, in social
                  networks. We demonstrate our model's capability for
                  inferring such latent structure in varying types of
                  social network datasets, and experimental studies
                  show this structure achieves higher predictive
                  performance on link prediction and forecasting
                  tasks.}
}

@inproceedings{HeaKnoGha14,
  author =	 {Creighton Heaukulani and David A. Knowles and Zoubin
                  Ghahramani},
  cat =		 {np clust},
  title =	 {Beta diffusion trees},
  booktitle =	 icml31,
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =
                  {http://mlg.eng.cam.ac.uk/heaukulani/BDT_ICML2014.pdf},
  abstract =	 {We define the beta diffusion tree, a random tree
                  structure with a set of leaves that defines a
                  collection of overlapping subsets of objects, known
                  as a feature allocation. The generative process for
                  the tree is defined in terms of particles
                  (representing the objects) diffusing in some
                  continuous space, analogously to the Dirichlet and
                  Pitman–Yor diffusion trees (Neal, 2003b; Knowles &
                  Ghahramani, 2011), both of which define tree
                  structures over clusters of the particles. With the
                  beta diffusion tree, however, multiple copies of a
                  particle may exist and diffuse to multiple locations
                  in the continuous space, resulting in (a random
                  number of) possibly overlapping clusters of the
                  objects. We demonstrate how to build a
                  hierarchically-clustered factor analysis model with
                  the beta diffusion tree and how to perform inference
                  over the random tree structures with a Markov chain
                  Monte Carlo algorithm. We conclude with several
                  numerical experiments on missing data problems with
                  data sets of gene expression arrays, international
                  development statistics, and intranational
                  socioeconomic measurements.}
}

@techreport{HeaKnoGha14b,
  author =	 {Creighton Heaukulani and David A. Knowles and Zoubin
                  Ghahramani},
  cat =		 {np clust},
  title =	 {Beta diffusion trees and hierarchical feature
                  allocations},
  institution =	 {Dept. of Engineering, University of Cambridge},
  year =	 2014,
  month =	 {August},
  url =		 {http://arxiv.org/abs/1408.3378},
  abstract =	 {We define the beta diffusion tree, a random tree
                  structure with a set of leaves that defines a
                  collection of overlapping subsets of objects, known
                  as a feature allocation.  A generative process for
                  the tree structure is defined in terms of particles
                  (representing the objects) diffusing in some
                  continuous space, analogously to the Dirichlet
                  diffusion tree (Neal, 2003b), which defines a tree
                  structure over partitions (i.e., non-overlapping
                  subsets) of the objects.  Unlike in the Dirichlet
                  diffusion tree, multiple copies of a particle may
                  exist and diffuse along multiple branches in the
                  beta diffusion tree, and an object may therefore
                  belong to multiple subsets of particles.  We
                  demonstrate how to build a hierarchically-clustered
                  factor analysis model with the beta diffusion tree
                  and how to perform inference over the random tree
                  structures with a Markov chain Monte Carlo
                  algorithm.  We conclude with several numerical
                  experiments on missing data problems with data sets
                  of gene expression microarrays, international
                  development statistics, and intranational
                  socioeconomic measurements.}
}

@techreport{HeaRoy14,
  author =	 {Creighton Heaukulani and Daniel M. Roy},
  cat =		 {np clust},
  title =	 {The combinatorial structure of beta negative
                  binomial processes},
  institution =	 {Dept. of Engineering, University of Cambridge},
  year =	 2014,
  month =	 {March},
  url =		 {http://arxiv.org/abs/1401.0062},
  abstract =	 {We characterize the combinatorial structure of
                  conditionally-i.i.d. sequences of negative binomial
                  processes with a common beta process base
                  measure. In Bayesian nonparametric applications,
                  such processes have served as models for unknown
                  multisets of a measurable space. Previous work has
                  characterized random subsets arising from
                  conditionally-i.i.d. sequences of Bernoulli
                  processes with a common beta process base
                  measure. In this case, the combinatorial structure
                  is described by the Indian buffet process. Our
                  results give a count analogue of the Indian buffet
                  process, which we call a negative binomial Indian
                  buffet process. As an intermediate step toward this
                  goal, we provide constructions for the beta negative
                  binomial process that avoid a representation of the
                  underlying beta process base measure.}
}

@inproceedings{HelGha05a,
  author =	 {Heller, Katherine A. and Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {clust},
  editor =	 {Raedt, Luc De and Wrobel, Stefan},
  isbn =	 {1-59593-180-5},
  pages =	 {297-304},
  publisher =	 acm,
  series =	 {ACM International Conference Proceeding Series},
  title =	 {Bayesian hierarchical clustering},
  url =		 {.},
  volume =	 119,
  year =	 2005,
  abstract =	 {We present a novel algorithm for agglomerative
                  hierarchical clustering based on evaluating marginal
                  likelihoods of a probabilistic model. This algorithm
                  has several advantages over traditional
                  distance-based agglomerative clustering
                  algorithms. (1) It defines a probabilistic model of
                  the data which can be used to compute the predictive
                  distribution of a test point and the probability of
                  it belonging to any of the existing clusters in the
                  tree. (2) It uses a model-based criterion to decide
                  on merging clusters rather than an ad-hoc distance
                  metric. (3) Bayesian hypothesis testing is used to
                  decide which merges are advantageous and to output
                  the recommended depth of the tree. (4) The algorithm
                  can be interpreted as a novel fast bottom-up
                  approximate inference method for a Dirichlet process
                  (i.e. countably infinite) mixture model (DPM). It
                  provides a new lower bound on the marginal
                  likelihood of a DPM by summing over exponentially
                  many clusterings of the data in polynomial time. We
                  describe procedures for learning the model
                  hyperpa-rameters, computing the predictive
                  distribution, and extensions to the
                  algorithm. Experimental results on synthetic and
                  real-world data sets demonstrate useful properties
                  of the algorithm.  }
}

@inproceedings{HelGha06a,
  author =	 {Heller, Katherine A. and Ghahramani, Zoubin},
  booktitle =	 {CVPR},
  cat =		 {mvision, ir},
  isbn =	 {0-7695-2597-0},
  pages =	 {2110-2117},
  publisher =	 {IEEE Computer Society},
  title =	 {A Simple {B}ayesian Framework for Content-Based
                  Image Retrieval},
  url =		 {.},
  year =	 2006,
  abstract =	 {We present a Bayesian framework for content-based
                  image retrieval which models the distribution of
                  color and texture features within sets of related
                  images. Given a userspecified text query
                  (e.g. "penguins") the system first extracts a set of
                  images, from a labelled corpus, corresponding to
                  that query. The distribution over features of these
                  images is used to compute a Bayesian score for each
                  image in a large unlabelled corpus. Unlabelled
                  images are then ranked using this score and the top
                  images are returned. Although the Bayesian score is
                  based on computing marginal likelihoods, which
                  integrate over model parameters, in the case of
                  sparse binary data the score reduces to a single
                  matrix-vector multiplication and is therefore
                  extremely efficient to compute. We show that our
                  method works surprisingly well despite its
                  simplicity and the fact that no relevance feedback
                  is used. We compare different choices of features,
                  and evaluate our results using human subjects.}
}

@inproceedings{HelGha07a,
  author =	 {Heller, Katherine A. and Ghahramani, Zoubin},
  booktitle =	 {AISTATS},
  cat =		 {clust, np},
  editor =	 {Meila, Marina and Shen, Xiaotong},
  pages =	 {187-194},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {A Nonparametric {B}ayesian Approach to Modeling
                  Overlapping Clusters},
  url =		 {.},
  volume =	 2,
  year =	 2007,
  abstract =	 {Although clustering data into mutually exclusive
                  partitions has been an extremely successful approach
                  to unsupervised learning, there are many situations
                  in which a richer model is needed to fully represent
                  the data. This is the case in problems where data
                  points actually simultaneously belong to multiple,
                  overlapping clusters. For example a particular gene
                  may have several functions, therefore belonging to
                  several distinct clusters of genes, and a biologist
                  may want to discover these through unsupervised
                  modeling of gene expression data. We present a new
                  nonparametric Bayesian method, the Infinite
                  Overlapping Mixture Model (IOMM), for modeling
                  overlapping clusters. The IOMM uses exponential
                  family distributions to model each cluster and forms
                  an overlapping mixture by taking products of such
                  distributions, much like products of experts
                  (Hinton, 2002). The IOMM allows an unbounded number
                  of clusters, and assignments of points to (multiple)
                  clusters is modeled using an Indian Buffet Process
                  (IBP), (Griffiths and Ghahramani, 2006). The IOMM
                  has the desirable properties of being able to focus
                  in on overlapping regions while maintaining the
                  ability to model a potentially infinite number of
                  clusters which may overlap. We derive MCMC inference
                  algorithms for the IOMM and show that these can be
                  used to cluster movies into multiple genres.}
}

@inproceedings{HelWilGha08,
  author =	 {Katherine A. Heller and Sinead Williamson and Zoubin
                  Ghahramani},
  cat =		 {clust},
  title =	 {Statistical models for partial membership},
  booktitle =	 icml25,
  year =	 2008,
  month =	 {July},
  address =	 {Helsinki, Finland},
  publisher =	 {Omnipress},
  editor =	 {Andrew McCallum and Sam Roweis},
  pages =	 {392--399},
  url =		 {.},
  abstract =	 {We present a principled Bayesian framework for
                  modeling partial memberships of data points to
                  clusters. Unlike a standard mixture model which
                  assumes that each data point belongs to one and only
                  one mixture component, or cluster, a partial
                  membership model allows data points to have
                  fractional membership in multiple
                  clusters. Algorithms which assign data points
                  partial memberships to clusters can be useful for
                  tasks such as clustering genes based on microarray
                  data (Gasch \& Eisen, 2002). Our Bayesian Partial
                  Membership Model (BPM) uses exponential family
                  distributions to model each cluster, and a product
                  of these distibtutions, with weighted parameters, to
                  model each datapoint. Here the weights correspond to
                  the degree to which the datapoint belongs to each
                  cluster. All parameters in the BPM are continuous,
                  so we can use Hybrid Monte Carlo to perform
                  inference and learning. We discuss relationships
                  between the BPM and Latent Dirichlet Allocation,
                  Mixed Membership models, Exponential Family PCA, and
                  fuzzy clustering. Lastly, we show some experimental
                  results and discuss nonparametric extensions to our
                  model.}
}

@inproceedings{HenMatFilGha15,
  cat =		 {gp np approx},
  title =	 {{MCMC} for {V}ariationally {S}parse {G}aussian
                  {P}rocesses},
  author =	 {James Hensman and Alexander G D G Matthews and
                  Maurizio Filippone and Zoubin Ghahramani},
  booktitle =	 nips28,
  year =	 2015,
  month =	 {December},
  address =	 {Montreal, Canada},
  url =
                  {https://papers.nips.cc/paper/5875-mcmc-for-variationally-sparse-gaussian-processes},
  pages =	 {1--9},
  abstract =	 {Gaussian process (GP) models form a core part of
                  probabilistic machine learning. Considerable
                  research effort has been made into attacking three
                  issues with GP models: how to compute efficiently
                  when the number of data is large; how to approximate
                  the posterior when the likelihood is not Gaussian
                  and how to estimate covariance function parameter
                  posteriors. This paper simultaneously addresses
                  these, using a variational approximation to the
                  posterior which is sparse in support of the function
                  but otherwise free-form. The result is a Hybrid
                  Monte-Carlo sampling scheme which allows for a
                  non-Gaussian approximation over the function values
                  and covariance parameters simultaneously, with
                  efficient computations based on inducing-point
                  sparse GPs. Code to replicate each experiment in
                  this paper will be available shortly.}
}

@inproceedings{HenMatGha15,
  cat =		 {gp np approx},
  title =	 {{S}calable {V}ariational {G}aussian {P}rocess
                  {C}lassification},
  author =	 {James Hensman and Alexander G D G Matthews and
                  Zoubin Ghahramani},
  booktitle =	 aistats18,
  year =	 2015,
  month =	 {May},
  address =	 {San Diego, California, USA},
  url =
                  {http://jmlr.org/proceedings/papers/v38/hensman15.pdf},
  pages =	 {1--9},
  abstract =	 {Gaussian process classification is a popular method
                  with a number of appealing properties. We show how
                  to scale the model within a variational inducing
                  point framework, out-performing the state of the art
                  on benchmark datasets. Importantly, the variational
                  formulation an be exploited to allow classification
                  in problems with millions of data points, as we
                  demonstrate in experiments.}
}

@inproceedings{HerHer13,
  cat =		 {approx gm},
  title =	 {Learning Feature Selection Dependencies in
                  Multi-task Learning},
  author =	 {Daniel Hern\'andez-Lobato and Jos\'e Miguel
                  Hern\'andez-Lobato},
  booktitle =	 nips27,
  year =	 2013,
  month =	 {December},
  address =	 {Lake Tahoe, California, USA},
  url =
                  {http://jmhldotorg.files.wordpress.com/2013/10/learningfeatureselectiondependenciesinmulti-tasklearning.pdf},
  pages =	 {1--9},
  abstract =	 { A probabilistic model based on the horseshoe prior
                  is proposed for learning de- pendencies in the
                  process of identifying relevant features for
                  prediction. Exact inference is intractable in this
                  model. However, expectation propagation offers an
                  approximate alternative. Because the process of
                  estimating feature selection dependencies may suffer
                  from over-fitting in the model proposed, additional
                  data from a multi-task learning scenario are
                  considered for induction. The same model can be used
                  in this setting with few modifications. Furthermore,
                  the assumptions made are less restrictive than in
                  other multi-task methods: The different tasks must
                  share feature selection dependencies, but can have
                  different relevant features and model
                  coefficients. Experiments with real and synthetic
                  data show that this model performs better than other
                  multi-task alternatives from the literature. The
                  experiments also show that the model is able to
                  induce suitable feature selection dependencies for
                  the problems considered, only from the training
                  data. }
}

@inproceedings{HerHerDup11,
  cat =		 {np clust},
  author =	 {Daniel Hern\'andez-Lobato and Jos\'e Miguel
                  Hern\'andez-Lobato and Pierre Dupont},
  title =	 {Robust Multi-Class {G}aussian Process
                  Classification},
  booktitle =	 nips25,
  year =	 2011,
  url =		 {.},
  abstract =	 {Multi-class Gaussian Processs Classifiers (MGPCs)
                  are often affected by overfitting problems when
                  labeling errors occur far from the decision
                  boundaries.  To prevent this, we investigate a
                  robust MGPC (RMGPC) which considers labeling errors
                  independently of their distance to the decision
                  boundaries. Expectation propagation is used for
                  approximate inference. Experiments with several
                  datasets in which noise is injected in the labels
                  illustrate the benefits of RMGPC. This method
                  performs better than other Gaussian process
                  alternatives based on considering latent Gaussian
                  noise or heavy-tailed processes. When no noise is
                  injected in the labels, RMGPC still performs equal
                  or better than the other methods. Finally, we show
                  how RMGPC can be used for successfully indentifying
                  data instances which are difficult to classify
                  correctly in practice.}
}

@article{HerHerDup13,
  cat =		 {gm approx},
  title =	 {Generalized Spike-and-Slab Priors for {B}ayesian
                  Group Feature Selection Using Expectation
                  Propagation},
  author =	 {Daniel Hern\'andez-Lobato and Jos\'e Miguel
                  Hern\'andez-Lobato and Pierre Dupont},
  volume =	 14,
  month =	 {July},
  journal =	 jmlr,
  pages =	 {1891--1945},
  year =	 2013,
  url =
                  {http://jmlr.org/papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf},
  abstract =	 { We describe a {B}ayesian method for group feature
                  selection in linear regression problems.  The method
                  is based on a generalized version of the standard
                  spike-and-slab prior distribution which is often
                  used for individual feature selection. Exact
                  {B}ayesian inference under the prior considered is
                  infeasible for typical regression problems. However,
                  approximate inference can be carried out efficiently
                  using {E}xpectation {P}ropagation (EP). A detailed
                  analysis of the generalized spike-and-slab prior
                  shows that it is well suited for regression problems
                  that are sparse at the group level. Furthermore,
                  this prior can be used to introduce prior knowledge
                  about specific groups of features that are a priori
                  believed to be more relevant. An experimental
                  evaluation compares the performance of the proposed
                  method with those of group {LASSO}, {B}ayesian group
                  {LASSO}, automatic relevance determination and
                  additional variants used for group feature
                  selection.  The results of these experiments show
                  that a model based on the generalized spike-and-slab
                  prior and the {EP} algorithm has state-of-the-art
                  prediction performance in the problems
                  analyzed. Furthermore, this model is also very
                  useful to carry out sequential experimental design
                  (also known as active learning), where the data
                  instances that are most informative are iteratively
                  included in the training set, reducing the number of
                  instances needed to obtain a particular level of
                  prediction accuracy.}
}

@inproceedings{HerHofZou14,
  cat =		 {approx gm},
  title =	 {Predictive Entropy Search for Efficient Global
                  Optimization of Black-box Functions},
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and Matthew
                  W. Hoffman and Zoubin Ghahramani},
  booktitle =	 nips28,
  year =	 2014,
  month =	 {December},
  address =	 {Montreal, Canada},
  url =
                  {https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf},
  pages =	 {1--9},
  abstract =	 { We propose a novel information-theoretic approach
                  for Bayesian optimization called Predictive Entropy
                  Search (PES). At each iteration, PES selects the
                  next evaluation point that maximizes the expected
                  information gained with respect to the global
                  maximum. PES codifies this intractable acquisition
                  function in terms of the expected reduction in the
                  differential entropy of the predictive distribution.
                  This reformulation allows PES to obtain
                  approximations that are both more accurate and
                  efficient than other alternatives such as Entropy
                  Search (ES). Furthermore, PES can easily perform a
                  fully Bayesian treatment of the model
                  hyperparameters while ES cannot. We evaluate PES in
                  both synthetic and realworld applications, including
                  optimization problems in machine learning, finance,
                  biotechnology, and robotics. We show that the
                  increased accuracy of PES leads to significant gains
                  in optimization performance. }
}

@inproceedings{HerHouGha13,
  cat =		 {gm approx},
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and Neil Houlsby
                  and Zoubin Ghahramani},
  title =	 {Stochastic Inference for Scalable Probabilistic
                  Modeling of Binary Matrices},
  booktitle =	 {NIPS Workshop on Randomized Methods for Machine
                  Learning},
  year =	 2013,
  url =
                  {http://jmhldotorg.files.wordpress.com/2013/11/svi_nipswshop20131.pdf},
  abstract =	 { Fully observed large binary matrices appear in a
                  wide variety of contexts. To model them,
                  probabilistic matrix factorization ({PMF}) methods
                  are an attractive solution.  However, current batch
                  algorithms for PMF can be inefficient since they
                  need to analyze the entire data matrix before
                  producing any parameter updates. We derive an
                  efficient stochastic inference algorithm for {PMF}
                  models of fully observed binary matrices. Our method
                  exhibits faster convergence rates than more
                  expensive batch approaches and has better predictive
                  performance than scalable alternatives. The proposed
                  method includes new data subsampling strategies
                  which produce large gains over standard uniform
                  subsampling. We also address the task of
                  automatically selecting the size of the minibatches
                  of data and we propose an algorithm that adjusts
                  this hyper-parameter in an online manner.  }
}

@inproceedings{HerHouGha14,
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and Neil Houlsby
                  and Zoubin Ghahramani},
  cat =		 {gm},
  title =	 {Probabilistic Matrix Factorization with Non-random
                  Missing Data},
  booktitle =	 icml31,
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =
                  {http://jmhldotorg.files.wordpress.com/2014/05/missingdataicml2014.pdf},
  abstract =	 {We propose a probabilistic matrix factorization
                  model for collaborative filtering that learns from
                  data that is missing not at random (MNAR).  Matrix
                  factorization models exhibit state-of-the-art
                  predictive performance in collaborative
                  filtering. However, these models usually assume that
                  the data is missing at random (MAR), and this is
                  rarely the case. For example, the data is not MAR if
                  users rate items they like more than ones they
                  dislike. When the MAR assumption is incorrect,
                  inferences are biased and predictive performance can
                  suffer. Therefore, we model both the generative
                  process for the data and the missing data
                  mechanism. By learning these two models jointly we
                  obtain improved performance over state-of-the-art
                  methods when predicting the ratings and when
                  modeling the data observation process. We present
                  the first viable MF model for MNAR data. Our results
                  are promising and we expect that further research on
                  NMAR models will yield large gains in collaborative
                  filtering.  }
}

@inproceedings{HerHouGha14b,
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and Neil Houlsby
                  and Zoubin Ghahramani},
  cat =		 {gm},
  title =	 {Stochastic Inference for Scalable Probabilistic
                  Modeling of Binary Matrices},
  booktitle =	 icml31,
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =
                  {http://jmhldotorg.files.wordpress.com/2014/05/stochasticinferencebinarymatricesicml2014.pdf},
  abstract =	 { Fully observed large binary matrices appear in a
                  wide variety of contexts.  To model them,
                  probabilistic matrix factorization (PMF) methods are
                  an attractive solution. However, current batch
                  algorithms for PMF can be inefficient because they
                  need to analyze the entire data matrix before
                  producing any parameter updates. We derive an
                  efficient stochastic inference algorithm for PMF
                  models of fully observed binary matrices. Our method
                  exhibits faster convergence rates than more
                  expensive batch approaches and has better predictive
                  performance than scalable alternatives. The proposed
                  method includes new data subsampling strategies
                  which produce large gains over standard uniform
                  subsampling. We also address the task of
                  automatically selecting the size of the minibatches
                  of data used by our method. For this, we derive an
                  algorithm that adjusts this hyper-parameter online.
                  }
}

@inproceedings{HerLiRowetal16,
  cat =		 {approx deep},
  title =	 {{B}lack-Box {A}lpha {D}ivergence {M}inimization},
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and Yingzhen Li and
                  Mark Rowland and Thang D.~Bui and Daniel
                  Hern\'andez-Lobato and Richard E.~Turner},
  booktitle =	 icml33,
  year =	 2016,
  month =	 {June},
  address =	 {New York USA},
  url =
                  {http://proceedings.mlr.press/v48/hernandez-lobatob16.html},
  abstract =	 {Black-box alpha (BB-$\alpha$) is a new approximate
                  inference method based on the minimization of
                  $\alpha$-divergences. BB-$\alpha$ scales to large
                  datasets because it can be implemented using
                  stochastic gradient descent. BB-$\alpha$ can be
                  applied to complex probabilistic models with little
                  effort since it only requires as input the
                  likelihood function and its gradients. These
                  gradients can be easily obtained using automatic
                  differentiation. By changing the divergence
                  parameter α, the method is able to interpolate
                  between variational Bayes (VB) ($\alpha \rightarrow
                  0$) and an algorithm similar to expectation
                  propagation (EP) ($\alpha = 1$). Experiments on
                  probit regression and neural network regression and
                  classification problems show that BB-αwith
                  non-standard settings of $\alpha$, such as $\alpha =
                  0.5$, usually produces better predictions than with
                  $\alpha \rightarrow 0$ (VB) or $\alpha = 1$ (EP). }
}

@inproceedings{HerLloHer13,
  cat =		 {gp np approx},
  title =	 {{G}aussian Process Conditional Copulas with
                  Applications to Financial Time Series},
  author =	 {Jos\'e Miguel Hern\'andez-Lobato and James Robert
                  Lloyds and Daniel Hern\'andez-Lobato},
  booktitle =	 nips27,
  year =	 2013,
  month =	 {December},
  address =	 {Lake Tahoe, California, USA},
  url =		 {http://arxiv.org/pdf/1307.0373v1.pdf},
  pages =	 {1--9},
  abstract =	 { The estimation of dependencies between multiple
                  variables is a central problem in the analysis of
                  financial time series. A common approach is to
                  express these dependencies in terms of a copula
                  function. Typically the copula function is assumed
                  to be constant but this may be inaccurate when there
                  are covariates that could have a large influence on
                  the dependence structure of the data. To account for
                  this, a {Bayesian} framework for the estimation of
                  conditional copulas is proposed. In this framework
                  the parameters of a copula are non-linearly related
                  to some arbitrary conditioning variables.  We
                  evaluate the ability of our method to predict
                  time-varying dependencies on several equities and
                  currencies and observe consistent performance gains
                  compared to static copula models and other
                  time-varying copula methods. }
}

@inproceedings{Heretal15,
  title =	 {Predictive Entropy Search for {B}ayesian
                  Optimization with Unknown Constraints},
  author =	 {Hern\'andez-Lobato, Jos\'e Miguel and Gelbart,
                  Michael A. and Hoffman, Matthew W. and Adams,
                  R<!>yan P. and Ghahramani, Zoubin},
  booktitle =	 icml32,
  abstract =	 {Unknown constraints arise in many types of expensive
                  black-box optimization problems. Several methods
                  have been proposed recently for performing Bayesian
                  optimization with constraints, based on the expected
                  improvement (EI) heuristic. However, EI can lead to
                  pathologies when used with constraints. For example,
                  in the case of decoupled constraints—i.e., when one
                  can independently evaluate the objective or the
                  constraints—EI can encounter a pathology that
                  prevents exploration. Additionally, computing EI
                  requires a current best solution, which may not
                  exist if none of the data collected so far satisfy
                  the constraints. By contrast, information-based
                  approaches do not suffer from these failure
                  modes. In this paper, we present a new
                  information-based method called Predictive Entropy
                  Search with Constraints (PESC). We analyze the
                  performance of PESC and show that it compares
                  favorably to EI-based approaches on synthetic and
                  benchmark problems, as well as several real-world
                  examples. We demonstrate that PESC is an effective
                  algorithm that provides a promising direction
                  towards a unified solution for constrained Bayesian
                  optimization.},
  pages =	 {1699--1707},
  url =
                  {http://jmlr.org/proceedings/papers/v37/hernandez-lobatob15.pdf},
  year =	 2015
}

@article{HinGha97a,
  author =	 {Hinton, Geoffrey E and Ghahramani, Zoubin},
  journal =	 {Philosophical Transactions of the Royal Society of
                  London. Series B: Biological Sciences},
  cat =		 {deep},
  number =	 1358,
  pages =	 {1177--1190},
  publisher =	 {The Royal Society},
  title =	 {Generative models for discovering sparse distributed
                  representations},
  volume =	 352,
  year =	 1997,
  url =		 {.},
  abstract =	 {We describe a hierarchical, generative model that
                  can be viewed as a nonlinear generalization of
                  factor analysis and can be implemented in a neural
                  network. The model uses bottom–up, top–down and
                  lateral connections to perform Bayesian perceptual
                  inference correctly. Once perceptual inference has
                  been performed the connection strengths can be
                  updated using a very simple learning rule that only
                  requires locally available information. We
                  demonstrate that the network learns to extract
                  sparse, distributed, hierarchical representations.}
}

@inproceedings{HinGhaTeh99a,
  author =	 {Hinton, Geoffrey E. and Ghahramani, Zoubin and Teh,
                  Yee Whye},
  booktitle =	 {NIPS},
  cat =		 {mvision deep},
  editor =	 {Kearns, Michael J. and Solla, Sara A. and Cohn,
                  David A.},
  isbn =	 {0-262-11245-0},
  pages =	 {463-469},
  publisher =	 {The MIT Press},
  title =	 {Learning to Parse Images},
  url =		 {.},
  year =	 1999,
  abstract =	 {We describe a class of probabilistic models that we
                  call credibility networks. Using parse trees as
                  internal representations of images, credibility
                  networks are able to perform segmentation and
                  recognition simultaneously, removing the need for ad
                  hoc segmentation heuristics. Promising results in
                  the problem of segmenting handwritten digits were
                  obtained.  }
}

@inproceedings{HoeRasHan00,
  cat =		 {time},
  author =	 {Pedro A.~d.~F.~R.~H{\o}jen-S{\o}rensen and Carl
                  Edward Rasmussen and Lars Kai Hansen},
  title =	 {{B}ayesian modelling of {fMRI} time series},
  booktitle =	 nips12,
  year =	 2000,
  pages =	 {754--760},
  url =		 {.},
  editors =	 {Sara A. Solla, Todd K. Leen and Klaus-Robert
                  M{\"u}ller},
  publisher =	 mit,
  abstract =	 {We present a Hidden Markov Model (HMM) for inferring
                  the hidden psychological state (or neural activity)
                  during single trial fMRI activation experiments with
                  blocked task paradigms. Inference is based on
                  Bayesian methodology, using a combination of
                  analytical and a variety of Markov Chain Monte Carlo
                  (MCMC) sampling techniques. The advantage of this
                  method is that detection of short time learning
                  effects between repeated trials is possible since
                  inference is based only on single trial
                  experiments.}
}

@inproceedings{HofShaDef14,
  booktitle =	 aistats17,
  year =	 2014,
  month =	 {April},
  cat =		 {active},
  author =	 {Hoffman, Matthew W and Shahriari, Bobak and de
                  Freitas, Nando},
  title =	 {On correlation and budget constraints in model-based
                  bandit optimization with application to automatic
                  machine learning},
  pages =	 {365--374},
  address =	 {Reykjavik, Iceland},
  url =
                  {http://jmlr.org/proceedings/papers/v33/hoffman14.pdf},
  abstract =	 {We address the problem of finding the maximizer of a
                  nonlinear function that can only be evaluated,
                  subject to noise, at a finite number of query
                  locations. Further, we will assume that there is a
                  constraint on the total number of permitted function
                  evaluations.  We introduce a Bayesian approach for
                  this problem and show that it empirically
                  outperforms both the existing frequentist
                  counterpart and other Bayesian optimization methods.
                  The Bayesian approach places emphasis on detailed
                  modelling, including the modelling of correlations
                  among the arms. As a result, it can perform well in
                  situations where the number of arms is much larger
                  than the number of allowed function evaluation,
                  whereas the frequentist counterpart is inapplicable.
                  This feature enables us to develop and deploy
                  practical applications, such as automatic machine
                  learning toolboxes.  The paper presents
                  comprehensive comparisons of the proposed approach
                  with many Bayesian and bandit optimization
                  techniques, the first comparison of many of these
                  methods in the literature.  }
}

@incollection{HouCia14,
  cat =		 {ir approx},
  title =	 {A Scalable {G}ibbs Sampler for Probabilistic Entity
                  Linking},
  author =	 {Houlsby, Neil and Ciaramita, Massimiliano},
  booktitle =	 {36th European Conference on Information Retrieval},
  pages =	 {335--346},
  year =	 2014,
  publisher =	 {Springer},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-319-06028-6_28#page-1},
  abstract =	 {Entity linking involves labeling phrases in text
                  with their referent entities, such as Wikipedia or
                  Freebase entries.  This task is challenging due to
                  the large number of possible entities, in the
                  millions, and heavy-tailed mention ambiguity.  We
                  formulate the problem in terms of probabilistic
                  inference within a topic model, where each topic is
                  associated with a Wikipedia article.  To deal with
                  the large number of topics we propose a novel
                  efficient Gibbs sampling scheme which can also
                  incorporate side information, such as the Wikipedia
                  graph.  This conceptually simple probabilistic
                  approach achieves state-of-the-art performance in
                  entity-linking on the Aida-CoNLL dataset.}
}

@inproceedings{HouHerGha14,
  author =	 {Neil Houlsby and Jos\'e Miguel Hern\'andez-Lobato
                  and Zoubin Ghahramani},
  cat =		 {gm, active},
  title =	 {Cold-start Active Learning with Robust Ordinal
                  Matrix Factorization},
  booktitle =	 icml31,
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =
                  {http://jmhldotorg.files.wordpress.com/2014/05/coldstarticml2014.pdf},
  abstract =	 { We present a new matrix factorization model for
                  rating data and a corresponding active learning
                  strategy to address the cold-start
                  problem. Cold-start is one of the most challenging
                  tasks for recommender systems: what to recommend
                  with new users or items for which one has little or
                  no data. An approach is to use active learning to
                  collect the most useful initial ratings. However,
                  the performance of active learning depends strongly
                  upon having accurate estimates of i) the uncertainty
                  in model parameters and ii) the intrinsic noisiness
                  of the data. To achieve these estimates we propose a
                  heteroskedastic Bayesian model for ordinal matrix
                  factorization.  We also present a computationally
                  efficient framework for Bayesian active learning
                  with this type of complex probabilistic model. This
                  algorithm successfully distinguishes between
                  informative and noisy data points.  Our model yields
                  state-of-the-art predictive performance and, coupled
                  with our active learning strategy, enables us to
                  gain useful information in the cold-start setting
                  from the very first active sample.  }
}

@incollection{HouHerHusGha12,
  cat =		 {gp active},
  title =	 {Collaborative {G}aussian Processes for Preference
                  Learning},
  author =	 {Neil Houlsby and Jose Miguel Hern\'{a}ndez-Lobato
                  and Ferenc Husz\'{a}r and Zoubin Ghahramani},
  booktitle =	 nips26,
  year =	 2012,
  publisher =	 curran,
  pages =	 {2096-2104},
  url =
                  {http://books.nips.cc/papers/files/nips25/NIPS2012_1031.pdf},
  abstract =	 {We present a new model based on Gaussian processes
                  (GPs) for learning pairwise preferences expressed by
                  multiple users.  Inference is simplified by using a
                  \emph{preference kernel} for GPs which allows us to
                  combine supervised GP learning of user preferences
                  with unsupervised dimensionality reduction for
                  multi-user systems.  The model not only exploits
                  collaborative information from the shared structure
                  in user behavior, but may also incorporate user
                  features if they are available.  Approximate
                  inference is implemented using a combination of
                  expectation propagation and variational Bayes.
                  Finally, we present an efficient active learning
                  strategy for querying preferences.  The proposed
                  technique performs favorably on real-world data
                  against state-of-the-art multi-user preference
                  learning algorithms.}
}

@article{HouHou13,
  title =	 {Statistical Fitting of Undrained Strength Data},
  author =	 {Neil Houlsby and Guy Houlsby},
  journal =	 {Geotechnique},
  volume =	 63,
  number =	 13,
  pages =	 {1253-1263},
  year =	 2013,
  doi =		 {10.1680/geot.13.P.007},
  publisher =	 {Telford},
  url =
                  {http://www.icevirtuallibrary.com/content/article/10.1680/geot.13.P.007},
  abstract =	 { We describe an approach, based on Bayesian
                  statistical methods, that allows the fitting of a
                  design profile to a set of measurements of undrained
                  strengths. In particular we allow for the automatic
                  determination of not only the positions of
                  boundaries between geological units, but also the
                  selection of the number of units to model the data
                  in an appropriate way. }
}

@article{HouHusGha11a,
  author =	 {Houlsby, Neil and Huszar, Ferenc and Ghahramani,
                  Zoubin and Lengyel, M{\'a}t{\'e}},
  cat =		 {active},
  journal =	 {arXiv},
  title =	 {Bayesian Active Learning for Classification and
                  Preference Learning},
  url =		 {.},
  volume =	 {abs/1112.5745},
  year =	 2011,
  abstract =	 {Information theoretic active learning has been
                  widely studied for probabilistic models. For simple
                  regression an optimal myopic policy is easily
                  tractable. However, for other tasks and with more
                  complex models, such as classification with
                  nonparametric models, the optimal solution is harder
                  to compute. Current approaches make approximations
                  to achieve tractability. We propose an approach that
                  expresses information gain in terms of predictive
                  entropies, and apply this method to the Gaussian
                  Process Classifier (GPC). Our approach makes minimal
                  approximations to the full information theoretic
                  objective. Our experimental performance compares
                  favourably to many popular active learning
                  algorithms, and has equal or lower computational
                  complexity. We compare well to decision theoretic
                  approaches also, which are privy to more information
                  and require much more computational time. Secondly,
                  by developing further a reformulation of binary
                  preference learning to a classification problem, we
                  extend our algorithm to Gaussian Process preference
                  learning.  }
}

@article{HouHusGhaetal13,
  cat =		 {neuro},
  title =	 {Cognitive tomography reveals complex
                  task-independent mental representations},
  author =	 {Neil Houlsby and Ferenc Husz\'ar and Mohammad M
                  Ghassemi and Gerg\H{o} Orb\'{a}n and Daniel M
                  Wolpert and M\'at\'e Lengyel},
  journal =	 {Current Biology},
  volume =	 23,
  number =	 21,
  pages =	 {2169-2175},
  year =	 2013,
  doi =		 {10.1016/j.cub.2013.09.012},
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0960982213011287},
  abstract =	 { Humans develop rich mental representations that
                  guide their behavior in a variety of every-day
                  tasks.  However, it is unknown whether these
                  representations, often formalized as priors in
                  Bayesian inference, are specific for each task or
                  subserve multiple tasks. Current approaches cannot
                  distinguish between these two possibilities because
                  they cannot extract comparable representations
                  across different tasks.  Here, we develop a novel
                  method, termed cognitive tomography, that can
                  extract complex, multi-dimensional priors across
                  tasks. We apply this method to human judgments in
                  two qualitatively different tasks, familiarity and
                  odd-one-out, involving an ecologically relevant set
                  of stimuli, human faces. We show that priors over
                  faces are structurally complex and vary dramatically
                  across subjects, but are invariant across the tasks
                  within each subject. The priors we extract from each
                  task allow us to predict with high precision the
                  behavior of subjects for novel stimuli both in the
                  same task as well as in the other task.  Our results
                  provide the first evidence for a single
                  high-dimensional structured representation of a
                  naturalistic stimulus set that guides behavior in
                  multiple tasks. Moreover, the representations
                  estimated by cognitive tomography can provide
                  independent, behavior-based regressors for
                  elucidating the neural correlates of complex
                  naturalistic priors. }
}

@article{HroBahNovetal20,
  cat =		 {network},
  author =	 {Hron, Jiri and Bahri, Yasaman and Novak, Roman and
                  Pennington, Jeffrey and Sohl-{D}ickstein, Jascha},
  title =	 {Exact posteriors of wide {B}ayesian neural networks},
  journal =	 {UDL (ICML workshop)},
  year =	 2020,
  url =		 {https://arxiv.org/abs/2006.10541},
  abstract =	 { Recent work has shown that the prior over functions
                  induced by a deep Bayesian neural network (BNN)
                  behaves as a Gaussian process (GP) as the width of
                  all layers becomes large. However, many BNN
                  applications are concerned with the BNN function
                  space posterior. While some empirical evidence of
                  the posterior convergence was provided in the
                  original works of Neal (1996) and Matthews et
                  al. (2018), it is limited to small datasets or
                  architectures due to the notorious difficulty of
                  obtaining and verifying exactness of BNN posterior
                  approximations. We provide the missing theoretical
                  proof that the exact BNN posterior converges
                  (weakly) to the one induced by the GP limit of the
                  prior. For empirical validation, we show how to
                  generate exact samples from a finite BNN on a small
                  dataset via rejection sampling.  }
}

@article{HroBahSohetal20,
  cat =		 {network},
  title =	 {Infinite attention: {NNGP} and {NTK} for deep
                  attention networks},
  author =	 {Hron, Jiri and Bahri, Yasaman and Sohl-{D}ickstein,
                  Jascha and Novak, Roman},
  journal =	 {ICML},
  year =	 2020,
  url =		 {https://arxiv.org/abs/2006.10540},
  abstract =	 { There is a growing amount of literature on the
                  relationship between wide neural networks (NNs) and
                  Gaussian processes (GPs), identifying an equivalence
                  between the two for a variety of NN
                  architectures. This equivalence enables, for
                  instance, accurate approximation of the behaviour of
                  wide Bayesian NNs without MCMC or variational
                  approximations, or characterisation of the
                  distribution of randomly initialised wide NNs
                  optimised by gradient descent without ever running
                  an optimiser. We provide a rigorous extension of
                  these results to NNs involving attention layers,
                  showing that unlike single-head attention, which
                  induces non-Gaussian behaviour, multi-head attention
                  architectures behave as GPs as the number of heads
                  tends to infinity. We further discuss the effects of
                  positional encodings and layer normalisation, and
                  propose modifications of the attention mechanism
                  which lead to improved results for both finite and
                  infinitely wide NNs. We evaluate attention kernels
                  empirically, leading to a moderate improvement upon
                  the previous state-of-the-art on CIFAR-10 for GPs
                  without trainable kernels and advanced data
                  preprocessing. Finally, we introduce new features to
                  the Neural Tangents library (Novak et al., 2020)
                  allowing applications of NNGP/NTK models, with and
                  without attention, to variable-length sequences,
                  with an example on the IMDb reviews dataset.  }
}

@article{HroKraJoretal20,
  cat =		 {ir},
  title =	 {Exploration in two-stage recommender systems},
  author =	 {Hron, Jiri and Krauth, Karl and Jordan, Michael
                  I. and Kilbertus, Niki},
  journal =	 {REVEAL (ACM RecSys workshop)},
  year =	 2020,
  url =		 {https://arxiv.org/abs/2009.08956},
  abstract =	 { Two-stage recommender systems are widely adopted in
                  industry due to their scalability and
                  maintainability. These systems produce
                  recommendations in two steps: (i) multiple
                  nominators preselect a small number of items from a
                  large pool using cheap-to-compute item embeddings;
                  (ii) with a richer set of features, a ranker
                  rearranges the nominated items and serves them to
                  the user. A key challenge of this setup is that
                  optimal performance of each stage in isolation does
                  not imply optimal global performance. In response to
                  this issue, Ma et al. (2020) proposed a nominator
                  training objective importance weighted by the
                  ranker's probability of recommending each item. In
                  this work, we focus on the complementary issue of
                  exploration. Modeled as a contextual bandit problem,
                  we find LinUCB (a near optimal exploration strategy
                  for single-stage systems) may lead to linear regret
                  when deployed in two-stage recommenders. We
                  therefore propose a method of synchronising the
                  exploration strategies between the ranker and the
                  nominators. Our algorithm only relies on quantities
                  already computed by standard LinUCB at each stage
                  and can be implemented in three lines of additional
                  code. We end by demonstrating the effectiveness of
                  our algorithm experimentally.  }
}

@article{HroKraJoretal21,
  cat =		 {ir},
  title =	 {On component interactions in two-stage recommender
                  systems},
  author =	 {Hron, Jiri and Krauth, Karl and Jordan, Michael
                  I. and Kilbertus, Niki},
  journal =	 {NeurIPS},
  year =	 2021,
  url =		 {https://arxiv.org/abs/2106.14979},
  abstract =	 { Thanks to their scalability, two-stage recommenders
                  are used by many of today's largest online
                  platforms, including YouTube, LinkedIn, and
                  Pinterest. These systems produce recommendations in
                  two steps: (i) multiple nominators, tuned for low
                  prediction latency, preselect a small subset of
                  candidates from the whole item pool; (ii) a slower
                  but more accurate ranker further narrows down the
                  nominated items, and serves to the user. Despite
                  their popularity, the literature on two-stage
                  recommenders is relatively scarce, and the
                  algorithms are often treated as mere sums of their
                  parts. Such treatment presupposes that the two-stage
                  performance is explained by the behavior of the
                  individual components in isolation. This is not the
                  case: using synthetic and real-world data, we
                  demonstrate that interactions between the ranker and
                  the nominators substantially affect the overall
                  performance. Motivated by these findings, we derive
                  a generalization lower bound which shows that
                  independent nominator training can lead to
                  performance on par with uniformly random
                  recommendations. We find that careful design of item
                  pools, each assigned to a different nominator,
                  alleviates these issues. As manual search for a good
                  pool allocation is difficult, we propose to learn
                  one instead using a Mixture-of-Experts based
                  approach. This significantly improves both precision
                  and recall at K.  }
}

@article{HroKraJoretal22,
  cat =		 {ir},
  author =	 {Hron, Jiri and Krauth, Karl and Jordan, Michael
                  I. and Kilbertus, Niki and Dean, Sarah},
  title =	 {Modelling content creator incentives on
                  algorithm-curated platforms},
  journal =	 {arXiv},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2206.13102},
  abstract =	 { Content creators compete for user attention. Their
                  reach crucially depends on algorithmic choices made
                  by developers on online platforms. To maximize
                  exposure, many creators adapt strategically, as
                  evidenced by examples like the sprawling search
                  engine optimization industry. This begets
                  competition for the finite user attention pool. We
                  formalize these dynamics in what we call an exposure
                  game, a model of incentives induced by algorithms
                  including modern factorization and (deep) two-tower
                  architectures. We prove that seemingly innocuous
                  algorithmic choices—e.g., non-negative
                  vs. unconstrained factorization—significantly affect
                  the existence and character of (Nash) equilibria in
                  exposure games. We proffer use of creator behavior
                  models like ours for an (ex-ante) pre-deployment
                  audit.  Such an audit can identify misalignment
                  between desirable and incentivized content, and thus
                  complement post-hoc measures like content filtering
                  and moderation. To this end, we propose tools for
                  numerically finding equilibria in exposure games,
                  and illustrate results of an audit on the MovieLens
                  and LastFM datasets. Among else, we find that the
                  strategically produced content exhibits strong
                  dependence between algorithmic exploration and
                  content diversity, and between model expressivity
                  and bias towards gender-based user and creator
                  groups.  }
}

@article{HroMatGha18,
  cat =		 {network},
  title =	 {Variational {B}ayesian dropout: pitfalls and fixes},
  author =	 {Hron, Jiri and Matthews, Alexander G. D. G. and
                  Ghahramani, Zoubin},
  journal =	 {ICML},
  year =	 2018,
  url =		 {https://arxiv.org/abs/1807.01969},
  abstract =	 { Dropout, a stochastic regularisation technique for
                  training of neural networks, has recently been
                  reinterpreted as a specific type of approximate
                  inference algorithm for Bayesian neural
                  networks. The main contribution of the
                  reinterpretation is in providing a theoretical
                  framework useful for analysing and extending the
                  algorithm. We show that the proposed framework
                  suffers from several issues; from undefined or
                  pathological behaviour of the true posterior related
                  to use of improper priors, to an ill-defined
                  variational objective due to singularity of the
                  approximating distribution relative to the true
                  posterior. Our analysis of the improper log uniform
                  prior used in variational Gaussian dropout suggests
                  the pathologies are generally irredeemable, and that
                  the algorithm still works only because the
                  variational formulation annuls some of the
                  pathologies. To address the singularity issue, we
                  proffer Quasi-KL (QKL) divergence, a new approximate
                  inference objective for approximation of
                  high-dimensional distributions. We show that
                  motivations for variational Bernoulli dropout based
                  on discretisation and noise have QKL as a limit.
                  Properties of QKL are studied both theoretically and
                  on a simple practical example which shows that the
                  QKL-optimal approximation of a full rank Gaussian
                  with a degenerate one naturally leads to the
                  Principal Component Analysis solution.  }
}

@article{HroNovPenetal22,
  cat =		 {network},
  author =	 {Hron, Jiri and Novak, Roman and Pennington, Jeffrey
                  and Sohl-{D}ickstein, Jascha},
  title =	 {{B}ayesian neural networks have a simple weight
                  posterior: theory and accelerated sampling},
  journal =	 {ICML},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2206.07673},
  abstract =	 { We introduce repriorisation, a data-dependent
                  reparameterisation which transforms a Bayesian
                  neural network (BNN) posterior to a distribution
                  whose KL divergence to the BNN prior vanishes as
                  layer widths grow. The repriorisation map acts
                  directly on parameters, and its analytic simplicity
                  complements the known neural network Gaussian
                  process (NNGP) behaviour of wide BNNs in function
                  space. Exploiting the repriorisation, we develop a
                  Markov chain Monte Carlo (MCMC) posterior sampling
                  algorithm which mixes faster the wider the BNN. This
                  contrasts with the typically poor performance of
                  MCMC in high dimensions. We observe up to 50x higher
                  effective sample size relative to no
                  reparametrisation for both fully-connected and
                  residual networks. Improvements are achieved at all
                  widths, with the margin between reparametrised and
                  standard BNNs growing with layer width.  }
}

@inproceedings{HueBorKriGha08,
  cat =		 {mcmc},
  month =	 {December},
  address =	 {Pisa, Italy},
  author =	 {C. H\"ubler and K. Borgwardt and H.-P. Kriegel and
                  Z. Ghahramani},
  note =	 {ISSN: 1550-4786},
  booktitle =	 {Proceedings of 8th IEEE International Conference on
                  Data Mining (ICDM 2008)},
  title =	 {{M}etropolis algorithms for representative subgraph
                  sampling},
  publisher =	 {IEEE},
  pages =	 {283--292},
  year =	 2008,
  url =		 {.},
  abstract =	 {While data mining in chemoinformatics studied graph
                  data with dozens of nodes, systems biology and the
                  Internet are now generating graph data with
                  thousands and millions of nodes. Hence data mining
                  faces the algorithmic challenge of coping with this
                  significant increase in graph size: Classic
                  algorithms for data analysis are often too expensive
                  and too slow on large graphs.<br> While one strategy
                  to overcome this problem is to design novel
                  efficient algorithms, the other is to 'reduce' the
                  size of the large graph by sampling. This is the
                  scope of this paper: We will present novel
                  Metropolis algorithms for sampling a
                  'representative' small subgraph from the original
                  large graph, with 'representative' describing the
                  requirement that the sample shall preserve crucial
                  graph properties of the original graph. In our
                  experiments, we improve over the pioneering work of
                  Leskovec and Faloutsos (KDD 2006), by producing
                  representative subgraph samples that are both
                  smaller and of higher quality than those produced by
                  other methods from the literature.}
}

@inproceedings{HusDuv2012,
  cat =		 {np gp mcmc active approx},
  author =	 {Ferenc Husz\'{a}r and David Duvenaud},
  title =	 {Optimally-Weighted Herding is {B}ayesian Quadrature},
  booktitle =	 uai28,
  address =	 {Catalina Island, California},
  abstract =	 {Herding and kernel herding are deterministic methods
                  of choosing samples which summarise a probability
                  distribution.  A related task is choosing samples
                  for estimating integrals using Bayesian quadrature.
                  We show that the criterion minimised when selecting
                  samples in kernel herding is equivalent to the
                  posterior variance in Bayesian quadrature.  We then
                  show that sequential Bayesian quadrature can be
                  viewed as a weighted version of kernel herding which
                  achieves performance superior to any other weighted
                  herding method. We demonstrate empirically a rate of
                  convergence faster than O(1/N).  Our results also
                  imply an upper bound on the empirical error of the
                  Bayesian quadrature estimate.},
  month =	 {July},
  year =	 2012,
  pages =	 {377--385},
  url =		 {http://arxiv.org/pdf/1204.1664.pdf}
}

@article{HusHou11,
  cat =		 {active},
  title =	 {Adaptive {Bayesian} Quantum Tomography},
  author =	 {Ferenc Husz\'{a}r and Neil Houlsby},
  journal =	 {Physical Review A},
  volume =	 85,
  number =	 5,
  pages =	 052120,
  year =	 2012,
  publisher =	 {APS},
  url =		 {http://pra.aps.org/abstract/PRA/v85/i5/e052120},
  abstract =	 {In this paper we revisit the problem of optimal
                  design of quantum tomographic experiments. In
                  contrast to previous approaches where an optimal set
                  of measurements is decided in advance of the
                  experiment, we allow for measurements to be
                  adaptively and efficiently re-optimised depending on
                  data collected so far. We develop an adaptive
                  statistical framework based on Bayesian inference
                  and Shannon's information, and demonstrate a
                  ten-fold reduction in the total number of
                  measurements required as compared to non-adaptive
                  methods, including mutually unbiased bases.}
}

@techreport{HusLac11,
  title =	 {A Kernel Approach to Tractable {Bayesian}
                  Nonparametrics},
  author =	 {Ferenc Husz\'{a}r and Simon Lacoste-Julien},
  year =	 2011,
  institution =	 {University of Cambridge},
  annote =	 {arXiv:<a
                  href="http://arxiv.org/abs/1103.1761">1103.1761</a>},
  url =		 {.},
  abstract =	 { Inference in popular nonparametric Bayesian models
                  typically relies on sampling or other
                  approximations. This paper presents a general
                  methodology for constructing novel tractable
                  nonparametric Bayesian methods by applying the
                  kernel trick to inference in a parametric Bayesian
                  model. For example, Gaussian process regression can
                  be derived this way from Bayesian linear
                  regression. Despite the success of the Gaussian
                  process framework, the kernel trick is rarely
                  explicitly considered in the Bayesian literature. In
                  this paper, we aim to fill this gap and demonstrate
                  the potential of applying the kernel trick to
                  tractable Bayesian parametric models in a wider
                  context than just regression. As an example, we
                  present an intuitive Bayesian kernel machine for
                  density estimation that is obtained by applying the
                  kernel trick to a Gaussian generative model in
                  feature space.}
}

@inproceedings{HusNopLen10,
  author =	 {Ferenc Husz\'{a}r and Uta Noppeney and M\'{a}t\'{e}
                  Lengyel},
  title =	 {Mind reading by machine learning: {A} doubly
                  {Bayesian} method for inferring mental
                  representations},
  booktitle =	 cogsci32,
  year =	 2010,
  month =	 {August},
  editor =	 {S. Ohlsson and R. Catrambone},
  publisher =	 {The Cognitive Science Society},
  address =	 {Austin, TX, USA},
  abstract =	 {A central challenge in cognitive science is to
                  measure and quantify the mental representations
                  humans develop --- in other words, to 'read'
                  subject's minds. In order to eliminate potential
                  biases in reporting mental contents due to verbal
                  elaboration, subjects' responses in experiments are
                  often limited to binary decisions or discrete
                  choices that do not require conscious reflection
                  upon their mental contents. However, it is unclear
                  what such impoverished data can tell us about the
                  potential richness and dynamics of subjects' mental
                  representations. To address this problem, we used
                  ideal observer models that formalise choice
                  behaviour as (quasi-)Bayes-optimal, given subjects'
                  representations in long-term memory, acquired
                  through prior learning, and the stimuli currently
                  available to them. Bayesian inversion of such ideal
                  observer models allowed us to infer subjects' mental
                  representation from their choice behaviour in a
                  variety of psychophysical tasks. The inferred mental
                  representations also allowed us to predict future
                  choices of subjects with reasonable accuracy, even
                  in tasks that were different from those in which the
                  representations were estimated. These results
                  demonstrate a significant potential in standard
                  binary decision tasks to recover detailed
                  information about subjects' mental representations},
  url =		 {.},
  annote =	 {Supplementary material available <a href =
                  "http://mlg.eng.cam.ac.uk/ferenc/mindreading">here</a>.}
}

@phdthesis{Ial22,
  author =	 {Alessandro Davide Ialongo},
  cat =		 {gp time approx},
  title =	 {Variational Inference in Dynamical Systems},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2022,
  address =	 {Cambridge, UK},
  doi =		 {https://doi.org/10.17863/CAM.91368},
  url =
                  {https://www.repository.cam.ac.uk/bitstream/handle/1810/343944/PhD%20Thesis%20-%20Alessandro%20Davide%20Ialongo%20%28Final%20Version%20-%202022-12-06%29.pdf?sequence=3&isAllowed=y},
  abstract =	 {Dynamical systems are a powerful formalism to
                  analyse the world around us. Many datasets are
                  sequential in nature, and can be described by a
                  discrete time evolution law. We are interested in
                  approaching the analysis of such datasets from a
                  probabilistic perspective. We would like to maintain
                  justified beliefs about quantities which, though
                  useful in explaining the behaviour of a system, may
                  not be observable, as well as about the system's
                  evolution itself, especially in regimes we have not
                  yet observed in our data. The framework of
                  statistical inference gives us the tools to do so,
                  yet, for many systems of interest, performing
                  inference exactly is not computationally or
                  analytically tractable. The contribution of this
                  thesis, then, is twofold: first, we uncover two
                  sources of bias in existing variational inference
                  methods applied to dynamical systems in general, and
                  state space models whose transition function is
                  drawn from a Gaussian process (GPSSM) in
                  particular. We show bias can derive from assuming
                  posteriors in non-linear systems to be jointly
                  Gaussian, and from assuming that we can sever the
                  dependence between latent states and transition
                  function in state space model posteriors. Second, we
                  propose methods to address these issues, undoing the
                  resulting biases. We do this without compromising on
                  computational efficiency or on the ability to scale
                  to larger datasets and higher dimensions, compared
                  to the methods we rectify. One method, the Markov
                  Autoregressive Flow (Markov AF) addresses the
                  Gaussian assumption, by providing a more flexible
                  class of posteriors, based on normalizing flows,
                  which can be easily evaluated, sampled, and
                  optimised. The other method, Variationally Coupled
                  Dynamics and Trajectories (VCDT), tackles the
                  factorisation assumption, leveraging sparse Gaussian
                  processes and their variational representation to
                  reintroduce dependence between latent states and the
                  transition function at no extra computational
                  cost. Since the objective of inference is to
                  maintain calibrated beliefs, if we employed
                  approximations which are significantly biased in
                  non-linear, noisy systems, or when there is little
                  data available, we would have failed in our
                  objective, as those are precisely the regimes in
                  which uncertainty quantification is all the more
                  important. Hence we think it is essential, if we
                  wish to act optimally on such beliefs, to uncover,
                  and, if possible, to correct, all sources of
                  systematic bias in our inference methods.}
}

@inproceedings{IalWilHenRas18,
  cat =		 {gp approx time},
  title =	 {Non-Factorised Variational Inference in Dynamical
                  Systems},
  author =	 {Alessandro Davide Ialongo and Mark van der Wilk and
                  James Hensman and Carl Edward Rasmussen},
  booktitle =	 {First Symposium on Advances in Approximate Bayesian
                  Inference},
  year =	 2018,
  month =	 {December},
  address =	 {Montreal},
  url =		 {https://arxiv.org/pdf/1812.06067.pdf},
  abstract =	 {We focus on variational inference in dynamical
                  systems where the discrete time transition function
                  (or evolution rule) is modelled by a Gaussian
                  process. The dominant approach so far has been to
                  use a factorised posterior distribution, decoupling
                  the transition function from the system states.
                  This is not exact in general and can lead to an
                  overconfident posterior over the transition function
                  as well as an overestimation of the intrinsic
                  stochasticity of the system (process noise). We
                  propose a new method that addresses these issues and
                  incurs no additional computational costs.}
}

@inproceedings{IalWilHenRas19,
  cat =		 {gp approx time},
  title =	 {Overcoming Mean-Field Approximations in Recurrent
                  {G}aussian Process Models},
  author =	 {Alessandro Davide Ialongo and Mark van der Wilk and
                  James Hensman and Carl Edward Rasmussen},
  booktitle =	 icml36,
  year =	 2019,
  month =	 {June},
  address =	 {Long Beach},
  url =		 {https://arxiv.org/pdf/1906.05828.pdf},
  abstract =	 {We identify a new variational inference scheme for
                  dynamical systems whose transition function is
                  modelled by a Gaussian process. Inference in this
                  setting has either employed computationally
                  intensive MCMC methods, or relied on factorisations
                  of the variational posterior.  As we demonstrate in
                  our experiments, the factorisation between latent
                  system states and transition function can lead to a
                  miscalibrated posterior and to learning
                  unnecessarily large noise terms.  We eliminate this
                  factorisation by explicitly modelling the dependence
                  between state trajectories and the Gaussian process
                  posterior. Samples of the latent states can then be
                  tractably generated by conditioning on this
                  representation. The method we obtain (VCDT:
                  variationally coupled dynamics and trajectories)
                  gives better predictive performance and more
                  calibrated estimates of the transition function, yet
                  maintains the same time and space complexities as
                  mean-field methods.  Code is available at:
                  https://github.com/ialong/GPt.}
}

@inproceedings{IalWilRas17,
  cat =		 {gp approx time},
  title =	 {Closed-form Inference and Prediction in {G}aussian
                  Process State-Space Models},
  author =	 {Alessandro Davide Ialongo and Mark van der Wilk and
                  Carl Edward Rasmussen},
  booktitle =	 {NIPS Time Series Workshop 2017},
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach},
  url =		 {https://arxiv.org/pdf/1812.03580.pdf},
  abstract =	 {We examine an analytic variational inference scheme
                  for the Gaussian Process State Space Model (GPSSM) -
                  a probabilistic model for system identification and
                  time-series modelling.  Our approach performs
                  variational inference over both the system states
                  and the transition function.  We exploit Markov
                  structure in the true posterior, as well as an
                  inducing point approximation to achieve linear time
                  complexity in the length of the time
                  series. Contrary to previous approaches, no Monte
                  Carlo sampling is required: inference is cast as a
                  deterministic optimisation problem. In a number of
                  experiments, we demonstrate the ability to model
                  non-linear dynamics in the presence of both process
                  and observation noise as well as to impute missing
                  information (e.g. velocities from raw positions
                  through time), to de-noise, and to estimate the
                  underlying dimensionality of the system. Finally, we
                  also introduce a closed-form method for multi-step
                  prediction, and a novel criterion for assessing the
                  quality of our approximate posterior.}
}

@inproceedings{IwaDuvGha12,
  cat =		 {np gp clust},
  author =	 {Tomoharu Iwata and David Duvenaud and Zoubin
                  Ghahramani},
  title =	 {Warped Mixtures for Nonparametric Cluster Shapes},
  booktitle =	 uai29,
  address =	 {Bellevue, Washington},
  abstract =	 {A mixture of Gaussians fit to a single curved or
                  heavy-tailed cluster will report that the data
                  contains many clusters.  To produce more appropriate
                  clusterings, we introduce a model which warps a
                  latent mixture of Gaussians to produce nonparametric
                  cluster shapes.  The possibly low-dimensional latent
                  mixture model allows us to summarize the properties
                  of the high-dimensional clusters (or density
                  manifolds) describing the data.  The number of
                  manifolds, as well as the shape and dimension of
                  each manifold is automatically inferred. We derive a
                  simple inference scheme for this model which
                  analytically integrates out both the mixture
                  parameters and the warping function.  We show that
                  our model is effective for density estimation,
                  performs better than infinite Gaussian mixture
                  models at recovering the true number of clusters,
                  and produces interpretable summaries of
                  high-dimensional datasets.},
  month =	 {July},
  year =	 2013,
  url =		 {http://arxiv.org/pdf/1206.1846}
}

@inproceedings{IwaHouGha13,
  cat =		 {active},
  author =	 {Tomoharu Iwata and Neil Houlsby and Zoubin
                  Ghahramani},
  year =	 2013,
  title =	 {Active Learning for Interactive Visualization},
  booktitle =	 aistats16,
  abstract =	 {Many automatic visualization methods have been
                  proposed.  However, a visualization that is
                  automatically generated might be different to how a
                  user wants to arrange the objects in visualization
                  space. By allowing users to re-locate objects in the
                  embedding space of the visualization, they can
                  adjust the visualization to their preference.  We
                  propose an active learning framework for interactive
                  visualization which selects objects for the user to
                  re-locate so that they can obtain their desired
                  visualization by re-locating as few as possible. The
                  framework is based on an information theoretic
                  criterion, which favors objects that reduce the
                  uncertainty of the visualization.  We present a
                  concrete application of the proposed framework to
                  the Laplacian eigenmap visualization method.  We
                  demonstrate experimentally that the proposed
                  framework yields the desired visualization with
                  fewer user interactions than existing methods.},
  url =
                  {http://jmlr.org/proceedings/papers/v31/iwata13a.html}
}

@article{IwaLloGha15,
  cat =		 {clust network ssl np},
  title =	 {Unsupervised Many-to-Many Object Matching for
                  Relational Data},
  author =	 {Tomoharu Iwata and James Robert Lloyd and Zoubin
                  Ghahramani},
  abstract =	 {We propose a method for unsupervised many-to-many
                  object matching from multiple networks, which is the
                  task of finding correspondences between groups of
                  nodes in different networks. For example, the
                  proposed method can discover shared word groups from
                  multi-lingual document-word networks without
                  cross-language alignment information. We assume that
                  multiple networks share groups, and each group has
                  its own interaction pattern with other groups. Using
                  infinite relational models with this assumption,
                  objects in different networks are clustered into
                  common groups depending on their interaction
                  patterns, discovering a matching. The effectiveness
                  of the proposed method is experimentally
                  demonstrated by using synthetic and real relational
                  data sets, which include applications to
                  cross-domain recommendation without shared user/item
                  identifiers and multi-lingual word clustering.},
  url =
                  {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7208879&tag=1},
  journal =	 PAMI,
  year =	 2015
}

@inproceedings{IwaShaGha13a,
  author =	 {Iwata, Tomoharu and Shah, Amar and Ghahramani,
                  Zoubin},
  booktitle =	 {KDD},
  cat =		 {network},
  isbn =	 {978-1-4503-2174-7},
  pages =	 {266-274},
  publisher =	 acm,
  title =	 {Discovering latent influence in online social
                  activities via shared cascade poisson processes},
  url =		 {.},
  year =	 2013,
  abstract =	 {Many people share their activities with others
                  through online communities. These shared activities
                  have an impact on other users' activities. For
                  example, users are likely to become interested in
                  items that are adopted (e.g. liked, bought and
                  shared) by their friends. In this paper, we propose
                  a probabilistic model for discovering latent
                  influence from sequences of item adoption events. An
                  inhomogeneous Poisson process is used for modeling a
                  sequence, in which adoption by a user triggers the
                  subsequent adoption of the same item by other
                  users. For modeling adoption of multiple items, we
                  employ multiple inhomogeneous Poisson processes,
                  which share parameters, such as influence for each
                  user and relations between users. The proposed model
                  can be used for finding influential users,
                  discovering relations between users and predicting
                  item popularity in the future. We present an
                  efficient Bayesian inference procedure of the
                  proposed model based on the stochastic EM
                  algorithm. The effectiveness of the proposed model
                  is demonstrated by using real data sets in a social
                  bookmark sharing service.}
}

@inproceedings{JanBurGon2020,
  title =	 {Bandit optimisation of functions in the {Matérn}
                  kernel {RKHS}},
  author =	 {Janz, David and Burt, David and Gonzalez, Javier},
  booktitle =	 aistats23,
  year =	 2020,
  url =		 {https://proceedings.mlr.press/v108/janz20a.html},
  abstract =	 {We consider the problem of optimising functions in
                  the reproducing kernel Hilbert space (RKHS) of a
                  Matérn kernel with smoothness parameter $u$ over the
                  domain $[0,1]^d$ under noisy bandit feedback. Our
                  contribution, the $\pi$-GP-UCB algorithm, is the
                  first practical approach with guaranteed sublinear
                  regret for all $u&gt;1$ and $d \geq 1$. Empirical
                  validation suggests better performance and
                  drastically improved computational scalablity
                  compared with its predecessor, Improved GP-UCB.},
  cat =		 {gp}
}

@inproceedings{JanGuPoo17,
  cat =		 {approx gm deep},
  title =	 {Categorical Reparametrization with Gumble-Softmax},
  author =	 {Eric Jang and Shixiang Gu and Ben Poole},
  booktitle =	 iclr05,
  year =	 2017,
  month =	 {April},
  address =	 {Toulon FRANCE},
  url =		 {https://arxiv.org/abs/1611.01144},
  abstract =	 {Categorical variables are a natural choice for
                  representing discrete structure in the
                  world. However, stochastic neural networks rarely
                  use categorical latent variables due to the
                  inability to backpropagate through samples. In this
                  work, we present an efficient gradient estimator
                  that replaces the non-differentiable sample from a
                  categorical distribution with a differentiable
                  sample from a novel Gumbel-Softmax
                  distribution. This distribution has the essential
                  property that it can be smoothly annealed into a
                  categorical distribution. We show that our
                  Gumbel-Softmax estimator outperforms
                  state-of-the-art gradient estimators on structured
                  output prediction and unsupervised generative
                  modeling tasks with categorical latent variables,
                  and enables large speedups on semi-supervised
                  classification.}
}

@article{JanHroMazetal19,
  cat =		 {rl},
  author =	 {Janz, David and Hron, Jiri and Mazur, Przemyslaw and
                  Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Hofmann,
                  Katja and Tschiatschek, Sebastian},
  title =	 {Successor {U}ncertainties: exploration and
                  uncertainty in temporal difference learning},
  journal =	 {NeurIPS},
  year =	 2019,
  url =		 {https://arxiv.org/abs/1810.06530},
  abstract =	 { Posterior sampling for reinforcement learning
                  (PSRL) is an effective method for balancing
                  exploration and exploitation in reinforcement
                  learning.  Randomised value functions (RVF) can be
                  viewed as a promising approach to scaling
                  PSRL. However, we show that most contemporary
                  algorithms combining RVF with neural network
                  function approximation do not possess the properties
                  which make PSRL effective, and provably fail in
                  sparse reward problems.  Moreover, we find that
                  propagation of uncertainty, a property of PSRL
                  previously thought important for exploration, does
                  not preclude this failure. We use these insights to
                  design Successor Uncertainties (SU), a cheap and
                  easy to implement RVF algorithm that retains key
                  properties of PSRL. SU is highly effective on hard
                  tabular exploration benchmarks.  Furthermore, on the
                  Atari 2600 domain, it surpasses human performance on
                  38 of 49 games tested (achieving a median human
                  normalised score of 2.09), and outperforms its
                  closest RVF competitor, Bootstrapped DQN, on 36 of
                  those.  }
}

@inproceedings{JaqGuBahLobetal17,
  cat =		 {deep time rl},
  title =	 {Sequence Tutor: Conservative fine-tuning of sequence
                  generation models with KL-control},
  author =	 {Natasha Jaques and Shixiang Gu and Dzmitry Bahdanau
                  and Jose Miguel Hernndez Lobato and Richard
                  E. Turner and Douglas Eck},
  booktitle =	 icml34,
  year =	 2017,
  month =	 {Aug},
  address =	 {Sydney AUSTRALIA},
  url =		 {https://arxiv.org/abs/1611.02796},
  abstract =	 {This paper proposes a general method for improving
                  the structure and quality of sequences generated by
                  a recurrent neural network (RNN), while maintaining
                  information originally learned from data, as well as
                  sample diversity. An RNN is first pre-trained on
                  data using maximum likelihood estimation (MLE), and
                  the probability distribution over the next token in
                  the sequence learned by this model is treated as a
                  prior policy. Another RNN is then trained using
                  reinforcement learning (RL) to generate
                  higher-quality outputs that account for
                  domain-specific incentives while retaining proximity
                  to the prior policy of the MLE RNN. To formalize
                  this objective, we derive novel off-policy RL
                  methods for RNNs from KL-control. The effectiveness
                  of the approach is demonstrated on two applications;
                  1) generating novel musical melodies, and 2)
                  computational molecular generation. For both
                  problems, we show that the proposed method improves
                  the desired properties and structure of the
                  generated sequences, while maintaining information
                  learned from data.},
  annote =	 {[<a
                  href="https://www.technologyreview.com/s/603003/ai-songsmith-cranks-out-surprisingly-catchy-tunes/">MIT
                  Technology Review</a>] [<a
                  href="https://www.youtube.com/watch?v=abBfZB5DlSY">Video</a>]}
}

@inproceedings{JazAshForetal,
  cat =		 {gp approx},
  title =	 { Scalable {G}aussian Process Variational
                  Autoencoders },
  author =	 {Jazbec, Metod and Ashman, Matt and Fortuin, Vincent
                  and Pearce, Michael and Mandt, Stephan and
                  R{\"a}tsch, Gunnar},
  booktitle =	 {Proceedings of The 24th International Conference on
                  Artificial Intelligence and Statistics},
  pages =	 {3511--3519},
  year =	 2021,
  editor =	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume =	 130,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--15 Apr},
  publisher =	 pmlr,
  url =		 {https://proceedings.mlr.press/v130/jazbec21a.html},
  abstract =	 { Conventional variational autoencoders fail in
                  modeling correlations between data points due to
                  their use of factorized priors. Amortized Gaussian
                  process inference through GP-VAEs has led to
                  significant improvements in this regard, but is
                  still inhibited by the intrinsic complexity of exact
                  GP inference. We improve the scalability of these
                  methods through principled sparse inference
                  approaches. We propose a new scalable GP-VAE model
                  that outperforms existing approaches in terms of
                  runtime and memory footprint, is easy to implement,
                  and allows for joint end-to-end optimization of all
                  components. }
}

@inproceedings{JinGha02a,
  author =	 {Jin, Rong and Ghahramani, Zoubin},
  booktitle =	 {NIPS},
  editor =	 {Becker, Suzanna and Thrun, S<!>ebastian and
                  Obermayer, Klaus},
  isbn =	 {0-262-02550-7},
  pages =	 {897-904},
  publisher =	 {MIT Press},
  title =	 {Learning with Multiple Labels},
  url =		 {.},
  year =	 2002,
  abstract =	 {In this paper, we study a special kind of learning
                  problem in which each training instance is given a
                  set of (or distribution over) candidate class labels
                  and only one of the candidate labels is the correct
                  one. Such a problem can occur, e.g., in an
                  information retrieval setting where a set of words
                  is associated with an image, or if classes labels
                  are organized hierarchically. We propose a novel
                  discriminative approach for handling the ambiguity
                  of class labels in the training examples. The
                  experiments with the proposed approach over five
                  different UCI datasets show that our approach is
                  able to find the correct label among the set of
                  candidate labels and actually achieve performance
                  close to the case when each training instance is
                  given a single correct label. In contrast,
                  na{\"\i}ve methods degrade rapidly as more ambiguity
                  is introduced into the labels.  }
}

@inproceedings{Jinetal21,
  cat =		 {causal nlp},
  title =	 {Causal Direction of Data Collection Matters:
                  Implications of Causal and Anticausal Learning for
                  NLP},
  author =	 {Jin*, Z. and von K{\"u}gelgen*, J. and Ni, J. and
                  Vaidhya, T. and Kaushal, A. and Sachan, M. and
                  Sch{\"o}lkopf, B.},
  booktitle =	 {Proceedings of the 2021 Conference on Empirical
                  Methods in Natural Language Processing (EMNLP)},
  pages =	 {9499--9513},
  editors =	 {Marie{-}Francine Moens and Xuanjing Huang and Lucia
                  Specia and Scott Wen{-}tau Yih},
  publisher =	 {Association for Computational Linguistics},
  year =	 2021,
  note =	 {*equal contribution},
  doi =		 {10.18653/v1/2021.emnlp-main.748},
  url =		 {https://aclanthology.org/2021.emnlp-main.748},
  abstract =	 {The principle of independent causal mechanisms (ICM)
                  states that generative processes of real world data
                  consist of independent modules which do not
                  influence or inform each other. While this idea has
                  led to fruitful developments in the field of causal
                  inference, it is not widely-known in the NLP
                  community. In this work, we argue that the causal
                  direction of the data collection process bears
                  nontrivial implications that can explain a number of
                  published NLP findings, such as differences in
                  semi-supervised learning (SSL) and domain adaptation
                  (DA) performance across different settings. We
                  categorize common NLP tasks according to their
                  causal direction and empirically assay the validity
                  of the ICM principle for text data using minimum
                  description length. We conduct an extensive
                  meta-analysis of over 100 published SSL and 30 DA
                  studies, and find that the results are consistent
                  with our expectations based on causal insights. This
                  work presents the first attempt to analyze the ICM
                  principle in NLP, and provides constructive
                  suggestions for future modeling choices.},
}

@article{JorGhaJaa99a,
  author =	 {Jordan, Michael I. and Ghahramani, Zoubin and
                  Jaakkola, Tommi and Saul, Lawrence K.},
  cat =		 {approx, review},
  journal =	 {Machine Learning},
  number =	 2,
  pages =	 {183-233},
  title =	 {An Introduction to Variational Methods for Graphical
                  Models},
  url =		 {.},
  volume =	 37,
  year =	 1999,
  abstract =	 {This paper presents a tutorial introduction to the
                  use of variational methods for inference and
                  learning in graphical models (Bayesian networks and
                  Markov random fields). We present a number of
                  examples of graphical models, including the QMR-DT
                  database, the sigmoid belief network, the Boltzmann
                  machine, and several variants of hidden Markov
                  models, in which it is infeasible to run exact
                  inference algorithms. We then introduce variational
                  methods, which exploit laws of large numbers to
                  transform the original graphical model into a
                  simplified graphical model in which inference is
                  efficient. Inference in the simpified model provides
                  bounds on probabilities of interest in the original
                  model. We describe a general framework for
                  generating variational transformations based on
                  convex duality. Finally we return to the examples
                  and demonstrate how variational algorithms can be
                  formulated in each case.}
}

@inproceedings{JorGhaSau96a,
  author =	 {Jordan, Michael I. and Ghahramani, Zoubin and Saul,
                  Lawrence K.},
  booktitle =	 {NIPS},
  cat =		 {time},
  editor =	 {Mozer, Michael and Jordan, Michael I. and Petsche,
                  Thomas},
  pages =	 {501-507},
  publisher =	 {MIT Press},
  title =	 {Hidden {M}arkov Decision Trees},
  url =		 {.},
  year =	 1996,
  abstract =	 {We study a time series model that can be viewed as a
                  decision tree with Markov temporal structure. The
                  model is intractable for exact calculations, thus we
                  utilize variational approximations. We consider
                  three different distributions for the approximation:
                  one in which the Markov calculations are performed
                  exactly and the layers of the decision tree are
                  decoupled, one in which the decision tree
                  calculations are performed exactly and the time
                  steps of the Markov chain are decoupled, and one in
                  which a Viterbi-like assumption is made to pick out
                  a single most likely state sequence. We present
                  simulation results for artificial data and the Bach
                  chorales.}
}

@inproceedings{KarKugSchVal20,
  cat =		 {causal interpretability},
  title =	 {Algorithmic recourse under imperfect causal
                  knowledge: a probabilistic approach},
  author =	 {Karimi*, A.-H. and von K\"{u}gelgen*, J. and
                  Sch{\"o}lkopf, B. and Valera, I.},
  booktitle =	 nips33,
  pages =	 {265--277},
  editors =	 {H. Larochelle and M. Ranzato and R. Hadsell and
                  M. F. Balcan and H. Lin},
  publisher =	 {Curran Associates, Inc.},
  year =	 2020,
  note =	 {*equal contribution},
  url =
                  {https://proceedings.neurips.cc/paper/2020/file/02a3c7fb3f489288ae6942498498db20-Paper.pdf},
  abstract =	 {Recent work has discussed the limitations of
                  counterfactual explanations to recommend actions for
                  algorithmic recourse, and argued for the need of
                  taking causal relationships between features into
                  consideration. Unfortunately, in practice, the true
                  underlying structural causal model is generally
                  unknown. In this work, we first show that it is
                  impossible to guarantee recourse without access to
                  the true structural equations. To address this
                  limitation, we propose two probabilistic approaches
                  to select optimal actions that achieve recourse with
                  high probability given limited causal knowledge
                  (e.g., only the causal graph). The first captures
                  uncertainty over structural equations under additive
                  Gaussian noise, and uses Bayesian model averaging to
                  estimate the counterfactual distribution. The second
                  removes any assumptions on the structural equations
                  by instead computing the average effect of recourse
                  actions on individuals similar to the person who
                  seeks recourse, leading to a novel
                  subpopulation-based interventional notion of
                  recourse. We then derive a gradient-based procedure
                  for selecting optimal recourse actions, and
                  empirically show that the proposed approaches lead
                  to more reliable recommendations under imperfect
                  causal knowledge than non-probabilistic baselines.},
}

@inproceedings{KasSobHer12,
  author =	 {Michael Kaschesky and Pawel Sobkowicz and Jos\'e
                  Miguel Hern\'andez-Lobato and Guillaume Bouchard and
                  Cedric Archambeau and Nicolas Scharioth and Robert
                  Manchin and Adrian Gschwend and Reinhard Riedl},
  title =	 {Bringing Representativeness into Social Media
                  Monitoring and Analysis},
  booktitle =	 {46th {Hawaii} International Conference on System
                  Sciences},
  year =	 2013,
  address =	 {Manoa, Hawaii},
  url =
                  {http://www.computer.org/csdl/proceedings/hicss/2013/4892/00/4892c003.pdf},
  abstract =	 {The opinions, expectations and behavior of citizens
                  are increasingly reflected online --- therefore,
                  mining the internet for such data can enhance
                  decision-making in public policy, communications,
                  marketing, finance and other fields.  However, to
                  come closer to the representativeness of classic
                  opinion surveys there is a lack of knowledge about
                  the sociodemographic characteristics of those
                  voicing opinions on the internet. This paper
                  proposes to calibrate online opinions aggregated
                  from multiple and heterogeneous data sources with
                  traditional surveys enhanced with rich
                  socio-demographic information to enable insights
                  into which opinions are expressed on the internet by
                  specific segments of society. The goal of this
                  research is to provide professionals in citizen- and
                  consumer- centered domains with more concise near
                  real-time intelligence on online opinions. To become
                  effective, the methodologies presented in this paper
                  must be integrated into a coherent decision support
                  system.}
}

@inproceedings{KasVanGraHer10,
  author =	 {Kasneci, G. and {Van Gael}, J. and Graepel, T. and
                  Herbrich, R.},
  booktitle =	 {European Conference on Machine Learning (ECML)},
  title =	 {Bayesian Knowledge Corroboration with Logical Rules
                  and User Feedback},
  year =	 2010,
  month =	 {September},
  address =	 {Barcelona, Spain},
  url =		 {.},
  abstract =	 {Current knowledge bases suffer from either low
                  coverage or low accuracy. The underlying hypothesis
                  of this work is that user feedback can greatly
                  improve the quality of automatically extracted
                  knowledge bases. The feedback could help quantify
                  the uncertainty associated with the stored
                  statements and would enable mechanisms for
                  searching, ranking and reasoning at
                  entity-relationship level. Most importantly, a
                  principled model for exploiting user feedback to
                  learn the truth values of statements in the
                  knowledge base would be a major step forward in
                  addressing the issue of knowledge base curation.  We
                  present a family of probabilistic graphical models
                  that builds on user feedback and logical inference
                  rules derived from the popular Semantic-Web
                  formalism of RDFS <a
                  href="http://www.w3.org/TR/rdf-schema/">[1]</a>. Through
                  internal inference and belief propagation, these
                  models can learn both, the truth values of the
                  statements in the knowledge base and the
                  reliabilities of the users who give feedback. We
                  demonstrate the viability of our approach in
                  extensive experiments on real-world datasets, with
                  feedback collected from Amazon Mechanical Turk.}
}

@article{KerFreLinKro14,
  cat =		 {binf},
  author =	 {Peter Kerpedjiev and Jes Frellsen and Stinus
                  Lindgreen and Anders Krogh},
  title =	 {Adaptable probabilistic mapping of short reads using
                  position specific scoring matrices},
  journal =	 {BMC bioinformatics},
  volume =	 15,
  pages =	 100,
  year =	 2014,
  abstract =	 {BACKGROUND: Modern DNA sequencing methods produce
                  vast amounts of data that often requires mapping to
                  a reference genome. Most existing programs use the
                  number of mismatches between the read and the genome
                  as a measure of quality. This approach is without a
                  statistical foundation and can for some data types
                  result in many wrongly mapped reads. Here we present
                  a probabilistic mapping method based on
                  position-specific scoring matrices, which can take
                  into account not only the quality scores of the
                  reads but also user-specified models of evolution
                  and data-specific biases.RESULTS:We show how
                  evolution, data-specific biases, and sequencing
                  errors are naturally dealt with
                  probabilistically. Our method achieves better
                  results than Bowtie and BWA on simulated and real
                  ancient and PAR-CLIP reads, as well as on simulated
                  reads from the AT rich organism P. falciparum, when
                  modeling the biases of these data. For simulated
                  Illumina reads, the method has consistently higher
                  sensitivity for both single-end and paired-end
                  data. We also show that our probabilistic approach
                  can limit the problem of random matches from short
                  reads of contamination and that it improves the
                  mapping of real reads from one organism
                  (D.~melanogaster) to a related genome
                  (D.~simulans). CONCLUSION: The presented work is an
                  implementation of a novel approach to short read
                  mapping where quality scores, prior mismatch
                  probabilities and mapping qualities are handled in a
                  statistically sound manner. The resulting
                  implementation provides not only a tool for
                  biologists working with low quality and/or biased
                  sequencing data but also a demonstration of the
                  feasibility of using a probability based alignment
                  method on real and simulated data sets.},
  doi =		 {10.1186/1471-2105-15-100},
  annote =	 {Peter Kerpedjiev and Jes Frellsen contributed
                  equally. Additional resources are available at <a
                  href="http://bwa-pssm.binf.ku.dk">bwa-pssm.binf.ku.dk</a>}
}

@inproceedings{KhaRezBabHofetal20,
  cat =		 {fairness deep approx},
  title =	 {Adversarial Graph Embeddings for Fair Influence
                  Maximization over Social Networks},
  author =	 {Moein Khajehnejad and Ahmad Asgharian Rezaei and
                  Mahmoudreza Babaei and Jessica H<!>offmann and Mahdi
                  Jalili and Adrian Weller},
  booktitle =	 ijcai,
  year =	 2020,
  url =		 {https://arxiv.org/abs/2005.04074},
  abstract =	 {Influence maximization is a widely studied topic in
                  network science, where the aim is to reach the
                  maximum possible number of nodes, while only
                  targeting a small initial set of individuals. It has
                  critical applications in many fields, including
                  viral marketing, information propagation, news
                  dissemination, and vaccinations. However, the
                  objective does not usually take into account whether
                  the final set of influenced nodes is fair with
                  respect to sensitive attributes, such as race or
                  gender. Here we address fair influence maximization,
                  aiming to reach minorities more equitably. We
                  introduce Adversarial Graph Embeddings: we co-train
                  an auto-encoder for graph embedding and a
                  discriminator to discern sensitive attributes. This
                  leads to embeddings which are similarly distributed
                  across sensitive attributes. We then find a good
                  initial set by clustering the embeddings. We believe
                  we are the first to use embeddings for the task of
                  fair influence maximization. While there are
                  typically trade-offs between fairness and influence
                  maximization objectives, our experiments on
                  synthetic and real-world datasets show that our
                  approach dramatically reduces disparity while
                  remaining competitive with state-of-the-art
                  influence maximization methods.}
}

@inproceedings{KilBalKus19,
  cat =		 {fairness},
  title =	 {The sensitivity of counterfactual fairness to
                  unmeasured confounding},
  author =	 {Niki Kilbertus and Phil Ball and Matt Kusner and
                  Adrian Weller and Ricardo Silva},
  booktitle =	 uai35,
  year =	 2019,
  month =	 {July},
  address =	 {Tel Aviv},
  url =		 {https://arxiv.org/abs/1907.01040},
  abstract =	 {Causal approaches to fairness have seen substantial
                  recent interest, both from the machine learning
                  community and from wider parties interested in
                  ethical prediction algorithms. In no small part,
                  this has been due to the fact that causal models
                  allow one to simultaneously leverage data and expert
                  knowledge to remove discriminatory effects from
                  predictions. However, one of the primary assumptions
                  in causal modeling is that you know the causal
                  graph. This introduces a new opportunity for bias,
                  caused by misspecifying the causal model. One common
                  way for misspecification to occur is via unmeasured
                  confounding: the true causal effect between
                  variables is partially described by unobserved
                  quantities. In this work we design tools to assess
                  the sensitivity of fairness measures to this
                  confounding for the popular class of non-linear
                  additive noise models (ANMs). Specifically, we give
                  a procedure for computing the maximum difference
                  between two counterfactually fair predictors, where
                  one has become biased due to confounding. For the
                  case of bivariate confounding our technique can be
                  swiftly computed via a sequence of closed-form
                  updates. For multivariate confounding we give an
                  algorithm that can be efficiently solved via
                  automatic differentiation. We demonstrate our new
                  sensitivity analysis tools in real-world fairness
                  scenarios to assess the bias arising from
                  confounding.}
}

@inproceedings{KilGasKusetal18,
  cat =		 {fairness},
  title =	 {Blind justice: Fairness with encrypted sensitive
                  attributes},
  author =	 {Niki Kilbertus and Adria Gascon and Matt Kusner and
                  Michael Veale and Krishna P. Gummadi and Adrian
                  Weller},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm Sweden},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/ICML18-BlindJustice.pdf},
  abstract =	 {Recent work has explored how to train machine
                  learning models which do not discriminate against
                  any subgroup of the population as determined by
                  sensitive attributes such as gender or race.  To
                  avoid disparate treatment, sensitive attributes
                  should not be considered. On the other hand, in
                  order to avoid disparate impact, sensitive
                  attributes must be examined — e.g., in order to
                  learn a fair model, or to check if a given model is
                  fair. We introduce methods from secure multi-party
                  computation which allow us to avoid both. By
                  encrypting sensitive attributes, we show how an
                  outcome based fair model may be learned, checked, or
                  have its outputs verified and held to account,
                  without users revealing their sensitive attributes.
                  }
}

@inproceedings{KilGomSchetal20,
  cat =		 {fairness},
  title =	 {Fair Decisions Despite Imperfect Predictions},
  author =	 {Kilbertus, Niki and Rodriguez, Manuel Gomez and
                  Sch\"olkopf, Bernhard and Muandet, Krikamol and
                  Valera, Isabel},
  booktitle =	 aistats23,
  pages =	 {277--287},
  year =	 2020,
  editor =	 {Chiappa, Silvia and Calandra, Roberto},
  volume =	 108,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {26--28 Aug},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v108/kilbertus20a/kilbertus20a.pdf},
  url =		 {http://proceedings.mlr.press/v108/kilbertus20a.html},
  abstract =	 {Consequential decisions are increasingly informed by
                  sophisticated data-driven predictive
                  models. However, consistently learning accurate
                  predictive models requires access to ground truth
                  labels. Unfortunately, in practice, labels may only
                  exist conditional on certain decisions—if a loan is
                  denied, there is not even an option for the
                  individual to pay back the loan. In this paper, we
                  show that, in this selective labels setting,
                  learning to predict is suboptimal in terms of both
                  fairness and utility. To avoid this undesirable
                  behavior, we propose to directly learn stochastic
                  decision policies that maximize utility under
                  fairness constraints. In the context of fair machine
                  learning, our results suggest the need for a
                  paradigm shift from "learning to predict" to
                  "learning to decide". Experiments on synthetic and
                  real-world data illustrate the favorable properties
                  of learning to decide, in terms of both utility and
                  fairness.}
}

@inproceedings{KilRojGiaetal17,
  cat =		 {gm, fairness, causality},
  title =	 {Avoiding Discrimination through Causal Reasoning},
  booktitle =	 nips30,
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California},
  author =	 {Niki Kilbertus and Mateo Rojas-Carulla and
                  Giambattista Parascandolo and Moritz Hardt and
                  Dominik J<!>anzing and Bernhard Sch\"olkopf},
  url =		 {https://arxiv.org/pdf/1706.02744.pdf},
  abstract =	 {Recent work on fairness in machine learning has
                  focused on various statistical discrimination
                  criteria and how they trade off. Most of these
                  criteria are observational: They depend only on the
                  joint distribution of predictor, protected
                  attribute, features, and outcome. While convenient
                  to work with, observational criteria have severe
                  inherent limitations that prevent them from
                  resolving matters of fairness conclusively.  Going
                  beyond observational criteria, we frame the problem
                  of discrimination based on protected attributes in
                  the language of causal reasoning. This viewpoint
                  shifts attention from ``What is the right fairness
                  criterion?'' to ``What do we want to assume about
                  our model of the causal data generating process?''
                  Through the lens of causality, we make several
                  contributions. First, we crisply articulate why and
                  when observational criteria fail, thus formalizing
                  what was before a matter of opinion. Second, our
                  approach exposes previously ignored subtleties and
                  why they are fundamental to the problem. Finally, we
                  put forward natural causal non-discrimination
                  criteria and develop algorithms that satisfy them.}
}

@article{KimGha06a,
  author =	 {Kim, Hyun-Chul and Ghahramani, Zoubin},
  cat =		 {gp},
  journal =	 {IEEE Trans. Pattern Anal. Mach. Intell.},
  number =	 12,
  pages =	 {1948-1959},
  title =	 {Bayesian {G}aussian Process Classification with the
                  {EM}-{EP} Algorithm},
  url =		 {.},
  volume =	 28,
  year =	 2006,
  abstract =	 {Gaussian process classifiers (GPCs) are Bayesian
                  probabilistic kernel classifiers. In GPCs, the
                  probability of belonging to a certain class at an
                  input location is monotonically related to the value
                  of some latent function at that location. Starting
                  from a Gaussian process prior over this latent
                  function, data are used to infer both the posterior
                  over the latent function and the values of
                  hyperparameters to determine various aspects of the
                  function. Recently, the expectation propagation (EP)
                  approach has been proposed to infer the posterior
                  over the latent function. Based on this work, we
                  present an approximate EM algorithm, the EM-EP
                  algorithm, to learn both the latent function and the
                  hyperparameters. This algorithm is found to converge
                  in practice and provides an efficient Bayesian
                  framework for learning hyperparameters of the
                  kernel. A multiclass extension of the EM-EP
                  algorithm for GPCs is also derived. In the
                  experimental results, the EM-EP algorithms are as
                  good or better than other methods for GPCs or
                  Support Vector Machines (SVMs) with
                  cross-validation.}
}

@inproceedings{KimGha08,
  cat =		 {gp},
  volume =	 5342,
  month =	 {December},
  author =	 {H.~Kim and Zoubin Ghahramani},
  series =	 lncs,
  booktitle =	 {Structural, Syntactic and Statistical Pattern
                  Recognition},
  editor =	 {L. Niels da Vitoria},
  title =	 {Outlier robust {Gaussian} process classification},
  address =	 {Berlin, Germany},
  publisher =	 {Springer Berlin / Heidelberg},
  year =	 2008,
  journal =	 lncs,
  pages =	 {896--905},
  url =		 {.},
  abstract =	 {Gaussian process classifiers (GPCs) are a fully
                  statistical model for kernel classification. We
                  present a form of GPC which is robust to labeling
                  errors in the data set. This model allows label
                  noise not only near the class boundaries, but also
                  far from the class boundaries which can result from
                  mistakes in labelling or gross errors in measuring
                  the input features. We derive an outlier robust
                  algorithm for training this model which alternates
                  iterations based on the EP approximation and
                  hyperparameter updates until convergence. We show
                  the usefulness of the proposed algorithm with model
                  selection method through simulation results.}
}

@inproceedings{KimGha08a,
  author =	 {Kim, Hyun-Chul and Ghahramani, Zoubin},
  booktitle =	 {SSPR/SPR},
  cat =		 {gp},
  editor =	 {da Vitoria Lobo, Niels and Kasparis, Takis and Roli,
                  Fabio and Kwok, James Tin-Yau and Georgiopoulos,
                  Michael and Anagnostopoulos, Georgios C. and Loog,
                  Marco},
  isbn =	 {978-3-540-89688-3},
  pages =	 {896-905},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {Outlier Robust {G}aussian Process Classification},
  url =		 {.},
  volume =	 5342,
  year =	 2008,
  abstract =	 {Gaussian process classifiers (GPCs) are a fully
                  statistical model for kernel classification. We
                  present a form of GPC which is robust to labeling
                  errors in the data set. This model allows label
                  noise not only near the class boundaries, but also
                  far from the class boundaries which can result from
                  mistakes in labelling or gross errors in measuring
                  the input features. We derive an outlier robust
                  algorithm for training this model which alternates
                  iterations based on the EP approximation and
                  hyperparameter updates until convergence. We show
                  the usefulness of the proposed algorithm with model
                  selection method through simulation results.  }
}

@inproceedings{KimGha12,
  author =	 {Hyun-Chul Kim and Zoubin Ghahramani},
  year =	 2012,
  title =	 {{B}ayesian Classifier Combination},
  booktitle =	 aistats15,
  abstract =	 {Bayesian model averaging linearly mixes the
                  probabilistic predictions of multiple models, each
                  weighted by its posterior probability. This is the
                  coherent Bayesian way of combining multiple models
                  only under certain restrictive assumptions, which we
                  outline. We explore a general framework for Bayesian
                  model combination (which differs from model
                  averaging) in the context of classification. This
                  framework explicitly models the relationship between
                  each model’s output and the unknown true label. The
                  framework does not require that the models be
                  probabilistic (they can even be human assessors),
                  that they share prior information or receive the
                  same training data, or that they be independent in
                  their errors. Finally, the Bayesian combiner does
                  not need to believe any of the models is in fact
                  correct. We test several variants of this classifier
                  combination procedure starting from a classic
                  statistical model proposed by Dawid and Skene (1979)
                  and using MCMC to add more complex but important
                  features to the model. Comparisons on sev- eral data
                  sets to simpler methods like majority voting show
                  that the Bayesian methods not only perform well but
                  result in interpretable diagnostics on the data
                  points and the models.},
  url =		 {.}
}

@article{KimKimGha06a,
  author =	 {Kim, Hyun-Chul and Kim, Daijin and Ghahramani,
                  Zoubin and Bang, Sung Yang},
  cat =		 {gp},
  journal =	 {Pattern Recognition Letters},
  number =	 6,
  pages =	 {618-626},
  title =	 {Appearance-based gender classification with
                  {G}aussian processes},
  url =		 {.},
  volume =	 27,
  year =	 2006,
  abstract =	 {This paper concerns the gender classification task
                  of discriminating between images of faces of men and
                  women from face images. In appearance-based
                  approaches, the initial images are preprocessed
                  (e.g. normalized) and input into
                  classifiers. Recently, support vector machines
                  (SVMs) which are popular kernel classifiers have
                  been applied to gender classification and have shown
                  excellent performance. SVMs have difficulty in
                  determining the hyperparameters in kernels (using
                  cross-validation). We propose to use Gaussian
                  process classifiers (GPCs) which are Bayesian kernel
                  classifiers. The main advantage of GPCs over SVMs is
                  that they determine the hyperparameters of the
                  kernel based on Bayesian model selection
                  criterion. The experimental results show that our
                  methods outperformed SVMs with cross-validation in
                  most of data sets. Moreover, the kernel
                  hyperparameters found by GPCs using Bayesian methods
                  can be used to improve SVM performance.}
}

@inproceedings{KimKimGha06b,
  author =	 {Kim, Hyun-Chul and Kim, Daijin and Ghahramani,
                  Zoubin and Bang, Sung Yang},
  booktitle =	 {IJCNN},
  editor =	 {Cohen, William W. and Moore, Andrew},
  isbn =	 {1-59593-383-2},
  pages =	 {3371-3376},
  publisher =	 acm,
  series =	 {ACM International Conference Proceeding Series},
  title =	 {Gender Classification with {B}ayesian Kernel
                  Methods},
  url =		 {.},
  volume =	 148,
  year =	 2006,
  abstract =	 {We consider the gender classification task of
                  discriminating between images of faces of men and
                  women from face images. In appearance-based
                  approaches, the initial images are preprocessed
                  (e.g. normalized) and input into
                  classifiers. Recently, SVMs which are popular kernel
                  classifiers have been applied to gender
                  classification and have shown excellent
                  performance. We propose to use one of Bayesian
                  kernel methods which is Gaussian process classifiers
                  (GPCs) for gender classification. The main advantage
                  of Bayesian kernel methods such as GPCs over SVMs is
                  that they determine the hyperparameters of the
                  kernel based on Bayesian model selection
                  criterion. Our results show that GPCs outperformed
                  SVMs with cross validation.  }
}

@article{KirGriSav12a,
  author =	 {Kirk, Paul D. W. and Griffin, Jim E. and Savage,
                  Richard S. and Ghahramani, Zoubin and Wild, David
                  L.},
  cat =		 {np, bioinf},
  journal =	 {Bioinformatics},
  number =	 24,
  pages =	 {3290-3297},
  title =	 {Bayesian Correlated clustering to integrate multiple
                  datasets},
  url =		 {.},
  volume =	 28,
  year =	 2012,
  abstract =	 {MOTIVATION: The integration of multiple datasets
                  remains a key challenge in systems biology and
                  genomic medicine. Modern high-throughput
                  technologies generate a broad array of different
                  data types, providing distinct-but often
                  complementary-information. We present a Bayesian
                  method for the unsupervised integrative modelling of
                  multiple datasets, which we refer to as MDI
                  (Multiple Dataset Integration). MDI can integrate
                  information from a wide range of different datasets
                  and data types simultaneously (including the ability
                  to model time series data explicitly using Gaussian
                  processes). Each dataset is modelled using a
                  Dirichlet-multinomial allocation (DMA) mixture
                  model, with dependencies between these models
                  captured through parameters that describe the
                  agreement among the datasets.  RESULTS: Using a set
                  of six artificially constructed time series
                  datasets, we show that MDI is able to integrate a
                  significant number of datasets simultaneously, and
                  that it successfully captures the underlying
                  structural similarity between the datasets. We also
                  analyse a variety of real Saccharomyces cerevisiae
                  datasets. In the two-dataset case, we show that
                  MDI's performance is comparable with the present
                  state-of-the-art. We then move beyond the
                  capabilities of current approaches and integrate
                  gene expression, chromatin immunoprecipitation-chip
                  and protein-protein interaction data, to identify a
                  set of protein complexes for which genes are
                  co-regulated during the cell cycle. Comparisons to
                  other unsupervised data integration techniques-as
                  well as to non-integrative approaches-demonstrate
                  that MDI is competitive, while also providing
                  information that would be difficult or impossible to
                  extract using other methods.}
}

@article{KirGriSavGhaetal12,
  cat =		 {bioinf np clust},
  author =	 {P. Kirk and J. E. Griffin and R. S. Savage and
                  Z. Ghahramani and D. L. Wild},
  year =	 2012,
  title =	 {{B}ayesian correlated clustering to integrate
                  multiple datasets},
  journal =	 {Bioinformatics},
  abstract =	 {Motivation: The integration of multiple datasets
                  remains a key challenge in systems biology and
                  genomic medicine. Modern high-throughput
                  technologies generate a broad array of different
                  data types, providing distinct – but often
                  complementary – information. We present a Bayesian
                  method for the unsupervised integrative modelling of
                  multiple datasets, which we refer to as MDI
                  (Multiple Dataset Integration). MDI can integrate
                  information from a wide range of different datasets
                  and data types simultaneously (including the ability
                  to model time series data explicitly using Gaussian
                  processes). Each dataset is modelled using a
                  Dirichlet-multinomial allocation (DMA) mixture
                  model, with dependencies between these models
                  captured via parameters that describe the agreement
                  among the datasets.  <br> Results: Using a set of 6
                  artificially constructed time series datasets, we
                  show that MDI is able to integrate a significant
                  number of datasets simultaneously, and that it
                  successfully captures the underlying structural
                  similarity between the datasets. We also analyse a
                  variety of real S. cerevisiae datasets. In the
                  2-dataset case, we show that MDI’s performance is
                  comparable to the present state of the art. We then
                  move beyond the capabilities of current approaches
                  and integrate gene expression, ChIP-chip and
                  protein-protein interaction data, to identify a set
                  of protein complexes for which genes are
                  co-regulated during the cell cycle. Comparisons to
                  other unsupervised data integration techniques – as
                  well as to non-integrative approaches – demonstrate
                  that MDI is very competitive, while also providing
                  information that would be difficult or impossible to
                  extract using other methods.},
  annote =	 {This paper is available from the <a
                  href="http://bioinformatics.oxfordjournals.org/content/early/2012/10/08/bioinformatics.bts595.abstract">Bioinformatics
                  site</a> and a Matlab implementation of MDI is
                  available from<a
                  href="https://sites.google.com/site/mdipackage/">this
                  site</a>.},
  url =		 {.}
}

@inproceedings{KnoGaeGha11,
  cat =		 {np approx},
  author =	 {David A.~Knowles and Jurgen Van Gael and Zoubin
                  Ghahramani},
  title =	 {Message Passing Algorithms for the {D}irichlet
                  Diffusion Tree},
  booktitle =	 icml28,
  year =	 2011,
  abstract =	 {We demonstrate efficient approximate inference for
                  the Dirichlet Diffusion Tree (Neal, 2003), a
                  Bayesian nonparametric prior over tree
                  structures. Although DDTs provide a powerful and
                  elegant approach for modeling hierarchies they
                  haven't seen much use to date. One problem is the
                  computational cost of MCMC inference. We provide the
                  first deterministic approximate inference methods
                  for DDT models and show excellent performance
                  compared to the MCMC alternative.  We present
                  message passing algorithms to approximate the
                  Bayesian model evidence for a specific tree. This is
                  used to drive sequential tree building and greedy
                  search to find optimal tree structures,
                  corresponding to hierarchical clusterings of the
                  data. We demonstrate appropriate observation models
                  for continuous and binary data. The empirical
                  performance of our method is very close to the
                  computationally expensive MCMC alternative on a
                  density estimation problem, and significantly
                  outperforms kernel density estimators.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/knowles2011-icml.pdf">web
                  site</a>}
}

@inproceedings{KnoGha07,
  cat =		 {np bioinf},
  url =		 {.},
  author =	 {David Knowles and Zoubin Ghahramani},
  title =	 {Infinite Sparse Factor Analysis and Infinite
                  Independent Components Analysis},
  booktitle =	 {7th International Conference on Independent
                  Component Analysis and Signal Separation},
  year =	 2007,
  month =	 {September},
  address =	 {London, UK},
  publisher =	 {Springer},
  pages =	 {381--388},
  doi =		 {10.1007/978-3-540-74494-8_48},
  abstract =	 {A nonparametric Bayesian extension of Independent
                  Components Analysis (ICA) is proposed where observed
                  data Y is modelled as a linear superposition, G, of
                  a potentially infinite number of hidden sources,
                  X. Whether a given source is active for a specific
                  data point is specified by an infinite binary
                  matrix, Z. The resulting sparse representation
                  allows increased data reduction compared to standard
                  ICA.  We define a prior on Z using the Indian Buffet
                  Process (IBP). We describe four variants of the
                  model, with Gaussian or Laplacian priors on X and
                  the one or two-parameter IBPs. We demonstrate
                  Bayesian inference under these models using a Markov
                  chain Monte Carlo (MCMC) algorithm on synthetic and
                  gene expression data and compare to standard ICA
                  algorithms.}
}

@inproceedings{KnoGha11a,
  cat =		 {np clust},
  author =	 {David A.~Knowles and Zoubin Ghahramani},
  title =	 {Pitman-{Y}or Diffusion Trees},
  booktitle =	 uai27,
  year =	 2011,
  abstract =	 {We introduce the Pitman Yor Diffusion Tree (PYDT)
                  for hierarchical clustering, a generalization of the
                  Dirichlet Diffusion Tree (Neal, 2001) which removes
                  the restriction to binary branching structure. The
                  generative process is described and shown to result
                  in an exchangeable distribution over data points. We
                  prove some theoretical properties of the model and
                  then present two inference methods: a collapsed MCMC
                  sampler which allows us to model uncertainty over
                  tree structures, and a computationally efficient
                  greedy Bayesian EM search algorithm. Both algorithms
                  use message passing on the tree structure. The
                  utility of the model and algorithms is demonstrated
                  on synthetic and real world data, both continuous
                  and binary.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/knowles2011uai.pdf">web
                  site</a>}
}

@article{KnoGha11b,
  cat =		 {np bioinf},
  author =	 {David A. Knowles and Zoubin Ghahramani},
  year =	 2011,
  title =	 {Nonparametric {B}ayesian Sparse Factor Models with
                  application to Gene Expression modelling.},
  journal =	 {Annals of Applied Statistics},
  volume =	 5,
  number =	 {2B},
  pages =	 {1534--1552},
  url =		 {.},
  abstract =	 {A nonparametric Bayesian extension of Factor
                  Analysis (FA) is proposed where observed data Y is
                  modeled as a linear superposition, G, of a
                  potentially infinite number of hidden factors,
                  X. The Indian Buffet Process (IBP) is used as a
                  prior on G to incorporate sparsity and to allow the
                  number of latent features to be inferred. The
                  model's utility for modeling gene expression data is
                  investigated using randomly generated data sets
                  based on a known sparse connectivity matrix for
                  E. Coli, and on three biological data sets of
                  increasing complexity.}
}

@inproceedings{KnoHol09,
  cat =		 {bioinf},
  author =	 {David A. Knowles and Susan Holmes},
  title =	 {Statistical tools for ultra-deep pyrosequencing of
                  fast evolving viruses},
  booktitle =	 {NIPS Workshop: Computational Biology},
  year =	 2009,
  abstract =	 {We aim to detect minor variant Hepatitis B viruses
                  (HBV) in 38 pyrosequencing samples from infected
                  individuals. Errors involved in the amplification
                  and ultra deep pyrosequencing (UDPS) of these
                  samples are characterised using HBV plasmid
                  controls. Homopolymeric regions and quality scores
                  are found to be significant covariates in
                  determining insertion and deletion (indel) error
                  rates, but not mismatch rates which depend on the
                  nucleotide transition matrix. This knowledge is used
                  to derive two methods for classifying genuine
                  mutations: a hypothesis testing framework and a
                  mixture model. Using an approximate "ground truth"
                  from a limiting dilution Sanger sequencing run,
                  these methods are shown to outperform the naive
                  percentage threshold approach. The possibility of
                  early stage PCR errors becoming significant is
                  investigated by simulation, which underlines the
                  importance of the initial copy number.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/nips454.pdf">web
                  site</a>}
}

@inproceedings{KnoMin11,
  cat =		 {np clust},
  author =	 {David A.~Knowles and Thomas P.~Minka},
  title =	 {Non-conjugate Variational Message Passing for
                  Multinomial and Binary Regression},
  booktitle =	 nips25,
  year =	 2011,
  abstract =	 {Variational Message Passing (VMP) is an algorithmic
                  implementation of the Variational Bayes (VB) method
                  which applies only in the special case of conjugate
                  exponential family models. We propose an extension
                  to VMP, which we refer to as Non-conjugate
                  Variational Message Passing (NCVMP) which aims to
                  alleviate this restriction while maintaining
                  modularity, allowing choice in how expectations are
                  calculated, and integrating into an existing
                  message-passing framework: Infer.NET. We demonstrate
                  NCVMP on logistic binary and multinomial
                  regression. In the multinomial case we introduce a
                  novel variational bound for the softmax factor which
                  is tighter than other commonly used bounds whilst
                  maintaining computational tractability.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/nips2011.pdf">web
                  site</a> <a
                  href="http://mlg.eng.cam.ac.uk/dave/nips2011supp.pdf">supplementary</a>}
}

@inproceedings{KnoParGlaWin10,
  cat =		 {bioinf},
  author =	 {David A. Knowles and Leopold Parts and Daniel Glass
                  and John M. Winn},
  title =	 {Modeling skin and ageing phenotypes using latent
                  variable models in Infer.NET},
  booktitle =	 {NIPS Workshop: Predictive Models in Personalized
                  Medicine Workshop},
  year =	 2010,
  abstract =	 {We demonstrate and compare three unsupervised
                  Bayesian latent variable models implemented in
                  Infer.NET for biomedical data modeling of 42 skin
                  and ageing phenotypes measured on the 12,000 female
                  twins in the Twins UK study. We address various data
                  modeling problems include high missingness,
                  heterogeneous data, and repeat observations. We
                  compare the proposed models in terms of their
                  performance at predicting disease labels and
                  symptoms from available explanatory variables,
                  concluding that factor analysis type models have the
                  strongest statistical performance in this
                  setting. We show that such models can be combined
                  with regression components for improved
                  interpretability.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/knowles2010_predictive_medicine.pdf">web
                  site</a>}
}

@inproceedings{KnoParGlaWin11,
  cat =		 {bioinf},
  author =	 {David A. Knowles and Leopold Parts and Daniel Glass
                  and John M. Winn},
  title =	 {Inferring a measure of physiological age from
                  multiple ageing related phenotypes},
  booktitle =	 {NIPS Workshop: From Statistical Genetics to
                  Predictive Models in Personalized Medicine},
  year =	 2011,
  abstract =	 {What is ageing? One definition is simultaneous
                  degradation of multiple organ systems. Can an
                  individual be said to be "old" or "young" for their
                  (chronological) age in a scientifically meaningful
                  way? We investigate these questions using ageing
                  related phenotypes measured on the 12,000 female
                  twins in the Twins UK study. We propose a simple
                  linear model of ageing, which allows a latent
                  adjustment to be made to an individual's
                  chronological age to give her "physiological age",
                  shared across the observed phenotypes. We note
                  problems with the analysis resulting from the
                  linearity assumption and show how to alleviate these
                  issues using a non-linear extension. We find more
                  gene expression probes are significantly associated
                  with our measurement of physiological age than to
                  chronological age. },
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/dave/physiological_age.pdf">web
                  site</a>}
}

@inproceedings{KocBanLiketal03,
  cat =		 {gp},
  booktitle =	 {IFAC Internaltional Conference on Intelligent
                  Control Systems and Signal Processing},
  title =	 {A case based comparison of identification with
                  neural network and {G}aussian process models},
  author =	 {Ju{\v s} Kocijan and Bla{\v z} Banko and Bojan Likar
                  and Agathe Girard and Roderick Murray-Smith and Carl
                  Edward Rasmussen},
  year =	 2003,
  volume =	 1,
  pages =	 {137--142},
  url =		 {.},
  abstract =	 {In this paper an alternative approach to black-box
                  identification of non-linear dynamic systems is
                  compared with the more established approach of using
                  artificial neural networks. The Gaussian process
                  prior approach is a representative of non-parametric
                  modelling approaches. It was compared on a pH
                  process modelling case study. The purpose of
                  modelling was to use the model for control
                  design. The comparison revealed that even though
                  Gaussian process models can be effectively used for
                  modelling dynamic systems caution has to be
                  axercised when signals are selected.}
}

@inproceedings{KocMurRasGir04,
  cat =		 {gp rl},
  author =	 {Ju{\v s} Kocijan and Roderick Murray-Smith and Carl
                  Edward Rasmussen and Agathe Girard},
  title =	 {Gaussian process model based predictive control},
  year =	 2004,
  pages =	 {2214--2219},
  journal =	 {Proceedings of the ACC 2004},
  abstract =	 {Gaussian process models provide a probabilistic
                  non-parametric modelling approach for black-box
                  identi cation of non-linear dynamic systems. The
                  Gaussian processes can highlight areas of the input
                  space where prediction quality is poor, due to the
                  lack of data or its complexity, by indicating the
                  higher variance around the predicted mean. Gaussian
                  process models contain noticeably less coef cients
                  to be optimised. This paper illustrates possible
                  application of Gaussian process models within
                  model-based predictive control. The extra
                  information provided within Gaussian process model
                  is used in predictive control, where optimisation of
                  control signal takes the variance information into
                  account. The predictive control principle is
                  demonstrated on control of pH process benchmark.},
  booktitle =	 {American Control Conference},
  location =	 {Boston, MA},
  url =		 {.}
}

@inproceedings{KocMurRasLik03,
  cat =		 {gp rl},
  title =	 {Predictive control with {G}aussian process models},
  author =	 {Ju{\v s} Kocijan and Roderick Murray-Smith and Carl
                  Edward Rasmussen and Bojan Likar},
  editor =	 {B.~Zajc and M.~Tkal},
  pages =	 {352--356},
  url =		 {.},
  year =	 2003,
  booktitle =	 {IEEE Region 8 Eurocon 2003: Computer as a Tool},
  abstract =	 {This paper describes model-based predictive control
                  based on Gaussian processes. Gaussian process models
                  provide a probabilistic non-parametric modelling
                  approach for black-box identification of non-linear
                  dynamic systems. It offers more insight in variance
                  of obtained model response, as well as fewer
                  parameters to determine than other models. The
                  Gaussian processes can highlight areas of the input
                  space where prediction quality is poor, due to the
                  lack of data or its complexity, by indicating the
                  higher variance around the predicted mean. This
                  property is used in predictive control, where
                  optimisation of control signal takes the variance
                  information into account. The predictive control
                  principle is demonstrated on a simulated example of
                  nonlinear system.}
}

@article{KokHolSch17,
  cat =		 {sigproc review},
  author =	 {Manon Kok and Jeroen D. Hol and Thomas B. Sch\"on},
  title =	 {Using Inertial Sensors for Position and Orientation
                  Estimation},
  journal =	 {Foundations and Trends in Signal Processing},
  volume =	 11,
  number =	 {1--2},
  pages =	 {1--153},
  year =	 2017,
  url =
                  {https://www.nowpublishers.com/article/Details/SIG-094},
  abstract =	 {In recent years, MEMS inertial sensors (3D
                  accelerometers and 3D gyroscopes) have become widely
                  available due to their small size and low
                  cost. Inertial sensor measurements are obtained at
                  high sampling rates and can be integrated to obtain
                  position and orientation information. These
                  estimates are accurate on a short time scale, but
                  suffer from integration drift over longer time
                  scales. To overcome this issue, inertial sensors are
                  typically combined with additional sensors and
                  models. In this tutorial we focus on the signal
                  processing aspects of position and orientation
                  estimation using inertial sensors. We discuss
                  different modeling choices and a selected number of
                  important algorithms. The algorithms include
                  optimization-based smoothing and filtering as well
                  as computationally cheaper extended Kalman filter
                  and complementary filter implementations. The
                  quality of their estimates is illustrated using both
                  experimental and simulated data.},
  annote =	 {<a
                  href="https://arxiv.org/abs/1704.06053">arXiv</a>},
}

@inproceedings{KokSol18,
  cat =		 {sigproc gp},
  author =	 {Manon Kok and Arno Solin},
  title =	 {Scalable Magnetic Field SLAM in 3D Using Gaussian
                  Process Maps},
  booktitle =	 {Proceedings of the 21th International Conference on
                  Information Fusion (accepted for publication)},
  year =	 2018,
  month =	 {July},
  address =	 {Cambridge, UK},
  url =		 {https://arxiv.org/abs/1804.01926},
  abstract =	 {We present a method for scalable and fully 3D
                  magnetic field simultaneous localisation and mapping
                  (SLAM) using local anomalies in the magnetic field
                  as a source of position information. These anomalies
                  are due to the presence of ferromagnetic material in
                  the structure of buildings and in objects such as
                  furniture. We represent the magnetic field map using
                  a Gaussian process model and take well-known
                  physical properties of the magnetic field into
                  account. We build local magnetic field maps using
                  three-dimensional hexagonal block tiling. To make
                  our approach computationally tractable we use
                  reduced-rank Gaussian process regression in
                  combination with a Rao--Blackwellised particle
                  filter. We show that it is possible to obtain
                  accurate position and orientation estimates using
                  measurements from a smartphone, and that our
                  approach provides a scalable magnetic SLAM algorithm
                  in terms of both computational complexity and map
                  storage.}
}

@inproceedings{KorCheWel14,
  author =	 {Anoop Korattikara and Yutian Chen and Max Welling},
  cat =		 {mcmc approx},
  title =	 {Austerity in {MCMC} Land: Cutting the
                  {M}etropolis-{H}astings Budget},
  booktitle =	 icml31,
  pages =	 {181-189},
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =
                  {http://jmlr.org/proceedings/papers/v32/korattikara14.pdf},
  annote =	 {<a
                  href="http://jmlr.org/proceedings/papers/v32/korattikara14-supp.pdf">supplementary</a>},
  abstract =	 {Can we make Bayesian posterior MCMC sampling more
                  efficient when faced with very large datasets? We
                  argue that computing the likelihood for N datapoints
                  in the Metropolis-Hastings (MH) test to reach a
                  single binary decision is computationally
                  inefficient. We introduce an approximate MH rule
                  based on a sequential hypothesis test that allows us
                  to accept or reject samples with high confidence
                  using only a fraction of the data required for the
                  exact MH rule. While this method introduces an
                  asymptotic bias, we show that this bias can be
                  controlled and is more than offset by a decrease in
                  variance due to our ability to draw more samples per
                  unit of time.  }
}

@article{KraStrRadetal13,
  cat =		 {active},
  title =	 {Experimental Adaptive {B}ayesian Tomography},
  author =	 {Konstantin Kravtsov and Stanislav Straupe and Igor
                  Radchenko and Neil Houlsby and Ferenc Husz{\'a}r and
                  Sergey Kulik},
  journal =	 {Physical Review A},
  volume =	 87,
  number =	 6,
  pages =	 062122,
  year =	 2013,
  publisher =	 {APS},
  url =		 {http://link.aps.org/doi/10.1103/PhysRevA.87.062122},
  abstract =	 {We report an experimental realization of an adaptive
                  quantum state tomography protocol. Our method takes
                  advantage of a Bayesian approach to statistical
                  inference and is naturally tailored for adaptive
                  strategies. For pure states we observe close to
                  $N^{-1}$ scaling of infidelity with overall number
                  of registered events, while best non-adaptive
                  protocols allow for $N^{-1/2}$ scaling
                  only. Experiments are performed for polarization
                  qubits, but the approach is readily adapted to any
                  dimension.}
}

@inproceedings{KueRubSchWel19,
  cat =		 {active causal},
  title =	 {Optimal experimental design via Bayesian
                  optimization: active causal structure learning for
                  Gaussian process networks},
  author =	 {von K{\"u}gelgen, J. and Rubenstein, P. K. and
                  Sch{\"o}lkopf, B. and Weller, A.},
  booktitle =	 {NeurIPS 2019 Workshop Do the right thing: machine
                  learning and causal inference for improved decision
                  making},
  month =	 dec,
  year =	 2019,
  url =		 {https://arxiv.org/abs/1910.03962},
  abstract =	 {We study the problem of causal discovery through
                  targeted interventions. Starting from few
                  observational measurements, we follow a Bayesian
                  active learning approach to perform those
                  experiments which, in expectation with respect to
                  the current model, are maximally informative about
                  the underlying causal structure. Unlike previous
                  work, we consider the setting of continuous random
                  variables with non-linear functional relationships,
                  modelled with Gaussian process priors. To address
                  the arising problem of choosing from an uncountable
                  set of possible interventions, we propose to use
                  Bayesian optimisation to efficiently maximise a
                  Monte Carlo estimate of the expected information
                  gain.},
}

@article{KugGreSch20,
  cat =		 {causal},
  title =	 {Simpson's paradox in Covid-19 case fatality rates: a
                  mediation analysis of age-related causal effects},
  author =	 {von K{\"u}gelgen*, J. and Gresele*, L. and
                  Sch{\"o}lkopf, B.},
  journal =	 {IEEE Transactions on Artificial Intelligence},
  volume =	 2,
  number =	 1,
  pages =	 {18--27},
  publisher =	 {IEEE Computer Society},
  year =	 2021,
  note =	 {*equal contribution},
  doi =		 {10.1109/TAI.2021.3073088},
  url =
                  {https://www.computer.org/csdl/journal/ai/5555/01/09404149/1sNm718j25W},
  abstract =	 {We point out an instantiation of Simpson's paradox
                  in COVID-19 case fatality rates (cfrs): comparing a
                  large-scale study from China (February 17) with
                  early reports from Italy (March 9), we find that
                  cfrs are lower in Italy for every age group, but
                  higher overall. This phenomenon is explained by a
                  stark difference in case demographic between the two
                  countries. Using this as a motivating example, we
                  introduce basic concepts from mediation analysis and
                  show how these can be used to quantify different
                  direct and indirect effects when assuming a
                  coarse-grained causal graph involving country, age,
                  and case fatality. We curate an age-stratified cfr
                  dataset with >750 k cases and conduct a case study,
                  investigating total, direct, and indirect
                  (age-mediated) causal effects between different
                  countries and at different points in time. This
                  allows us to separate age-related effects from
                  others unrelated to age and facilitates a more
                  transparent comparison of cfrs across countries at
                  different stages of the COVID-19 pandemic. Using
                  longitudinal data from Italy, we discover a sign
                  reversal of the direct causal effect in mid-March,
                  which temporally aligns with the reported collapse
                  of the healthcare system in parts of the
                  country. Moreover, we find that direct and indirect
                  effects across 132 pairs of countries are only
                  weakly correlated, suggesting that a country's
                  policy and case demographic may be largely
                  unrelated. We point out limitations and extensions
                  for future work, and finally, discuss the role of
                  causal reasoning in the broader context of using AI
                  to combat the COVID-19 pandemic.},
}

@inproceedings{KugMeyLoo19,
  cat =		 {causal},
  title =	 {Semi-Generative Modelling: Covariate-Shift
                  Adaptation with Cause and Effect Features},
  author =	 {von K{\"u}gelgen, J. and Mey, A. and Loog, M.},
  booktitle =	 aistats22,
  volume =	 89,
  pages =	 {1361--1369},
  editors =	 {Kamalika Chaudhuri and Masashi Sugiyama},
  publisher =	 {PMLR},
  year =	 2019,
  url =		 {https://proceedings.mlr.press/v89/kugelgen19a.html},
  abstract =	 {Current methods for covariate-shift adaptation use
                  unlabelled data to compute importance weights or
                  domain-invariant features, while the final model is
                  trained on labelled data only. Here, we consider a
                  particular case of covariate shift which allows us
                  also to learn from unlabelled data, that is,
                  combining adaptation with semi-supervised
                  learning. Using ideas from causality, we argue that
                  this requires learning with both causes, $X_C$, and
                  effects, $X_E$, of a target variable, $Y$, and show
                  how this setting leads to what we call a
                  semi-generative model, $P(Y,X_E|X_C,\theta)$. Our
                  approach is robust to domain shifts in the
                  distribution of causal features and leverages
                  unlabelled data by learning a direct map from causes
                  to effects. Experiments on synthetic data
                  demonstrate significant improvements in
                  classification over purely-supervised and
                  importance-weighting baselines.},
}

@inproceedings{KugMeyLooSch20,
  cat =		 {ssl causal},
  title =	 {Semi-supervised learning, causality, and the
                  conditional cluster assumption},
  author =	 {von K\"{u}gelgen, J. and Mey, A. and Loog, M. and
                  Sch\"{o}lkopf, B.},
  booktitle =	 {Proceedings of the 36th International Conference on
                  Uncertainty in Artificial Intelligence (UAI) },
  volume =	 124,
  pages =	 {1--10},
  series =	 {Proceedings of Machine Learning Research},
  editors =	 {Jonas Peters and David Sontag},
  publisher =	 {PMLR},
  year =	 2020,
  note =	 {*also at NeurIPS 2019 Workshop Do the right thing:
                  machine learning and causal inference for improved
                  decision making},
  url =		 {http://proceedings.mlr.press/v124/kugelgen20a.html},
  abstract =	 {While the success of semi-supervised learning (SSL)
                  is still not fully understood, Schölkopf et
                  al. (2012) have established a link to the principle
                  of independent causal mechanisms. They conclude that
                  SSL should be impossible when predicting a target
                  variable from its causes, but possible when
                  predicting it from its effects. Since both these
                  cases are restrictive, we extend their work by
                  considering classification using cause and effect
                  features at the same time, such as predicting a
                  disease from both risk factors and symptoms. While
                  standard SSL exploits information contained in the
                  marginal distribution of all inputs (to improve the
                  estimate of the conditional distribution of the
                  target given in-puts), we argue that in our more
                  general setting we should use information in the
                  conditional distribution of effect features given
                  causal features. We explore how this insight
                  generalises the previous understanding, and how it
                  relates to and can be exploited algorithmically for
                  SSL.},
}

@inproceedings{KugSch22,
  cat =		 {causal review},
  title =	 {From statistical to causal learning},
  author =	 {Sch{\"o}lkopf*, B. and von K{\"u}gelgen*, J.},
  booktitle =	 {Proceedings of the International Congress of
                  Mathematicians (ICM)},
  publisher =	 {EMS Press},
  year =	 2022,
  note =	 {*equal contribution},
  url =		 {https://arxiv.org/abs/2204.00607},
  abstract =	 {We describe basic ideas underlying research to build
                  and understand artificially intelligent systems:
                  from symbolic approaches via statistical learning to
                  interventional models relying on concepts of
                  causality. Some of the hard open problems of machine
                  learning and AI are intrinsically related to
                  causality, and progress may require advances in our
                  understanding of how to model and infer causality
                  from data.  },
}

@inproceedings{KugUstGehBetSch20,
  cat =		 {mvision causal},
  title =	 {Towards causal generative scene models via
                  competition of experts},
  author =	 {von K{\"u}gelgen*, J. and Ustyuzhaninov*, I. and
                  Gehler, P. and Bethge, M. and Sch{\"o}lkopf, B.},
  booktitle =	 {ICLR 2020 Workshop "Causal Learning for Decision
                  Making"},
  year =	 2020,
  note =	 {*equal contribution},
  url =		 {https://arxiv.org/abs/2004.12906},
  abstract =	 {Learning how to model complex scenes in a modular
                  way with recombinable components is a pre-requisite
                  for higher-order reasoning and acting in the
                  physical world. However, current generative models
                  lack the ability to capture the inherently
                  compositional and layered nature of visual
                  scenes. While recent work has made progress towards
                  unsupervised learning of object-based scene
                  representations, most models still maintain a global
                  representation space (i.e., objects are not
                  explicitly separated), and cannot generate scenes
                  with novel object arrangement and depth
                  ordering. Here, we present an alternative approach
                  which uses an inductive bias encouraging modularity
                  by training an ensemble of generative models
                  (experts). During training, experts compete for
                  explaining parts of a scene, and thus specialise on
                  different object classes, with objects being
                  identified as parts that re-occur across multiple
                  scenes. Our model allows for controllable sampling
                  of individual objects and recombination of experts
                  in physically plausible ways. In contrast to other
                  methods, depth layering and occlusion are handled
                  correctly, moving this approach closer to a causal
                  generative scene model. Experiments on simple toy
                  data qualitatively demonstrate the conceptual
                  advantages of the proposed approach.},
}

@inproceedings{Kugelenetal21,
  cat =		 {deep causal},
  title =	 {Self-supervised learning with data augmentations
                  provably isolates content from style},
  author =	 {von K{\"u}gelgen*, J. and Sharma*, Y. and Gresele*,
                  L. and Brendel, W. and Sch{\"o}lkopf, B. and
                  Besserve, M. and Locatello, F.},
  booktitle =	 nips34,
  pages =	 {16451--16467},
  editors =	 {M. Ranzato and A. Beygelzimer and Y. Dauphin and
                  P.S. Liang and J. Wortman Vaughan},
  publisher =	 {Curran Associates, Inc.},
  year =	 2021,
  note =	 {*equal contribution},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf},
  abstract =	 {Self-supervised representation learning has shown
                  remarkable success in a number of domains. A common
                  practice is to perform data augmentation via
                  hand-crafted transformations intended to leave the
                  semantics of the data invariant. We seek to
                  understand the empirical success of this approach
                  from a theoretical perspective.  We formulate the
                  augmentation process as a latent variable model by
                  postulating a partition of the latent representation
                  into a content component, which is assumed invariant
                  to augmentation, and a style component, which is
                  allowed to change.  Unlike prior work on
                  disentanglement and independent component analysis,
                  we allow for both nontrivial statistical and causal
                  dependencies in the latent space.  We study the
                  identifiability of the latent representation based
                  on pairs of views of the observations and prove
                  sufficient conditions that allow us to identify the
                  invariant content partition up to an invertible
                  mapping in both generative and discriminative
                  settings. We find numerical simulations with
                  dependent latent variables are consistent with our
                  theory. Lastly, we introduce Causal3DIdent, a
                  dataset of high-dimensional, visually complex images
                  with rich causal dependencies, which we use to study
                  the effect of data augmentations performed in
                  practice.},
}

@inproceedings{Kugelgen20,
  cat =		 {causal fairness interpretability},
  title =	 {On the Fairness of Causal Algorithmic Recourse},
  author =	 {von K{\"u}gelgen, J. and Karimi, A.-H. and Bhatt,
                  U. and Valera, I. and Weller, A. and Sch{\"o}lkopf,
                  B.},
  booktitle =	 {Proceedings of the 36th AAAI Conference on
                  Artificial Intelligence (AAAI)},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2010.06529},
  abstract =	 {Algorithmic fairness is typically studied from the
                  perspective of predictions. Instead, here we
                  investigate fairness from the perspective of
                  recourse actions suggested to individuals to remedy
                  an unfavourable classification. We propose two new
                  fairness criteria at the group and individual level,
                  which -- unlike prior work on equalising the average
                  group-wise distance from the decision boundary --
                  explicitly account for causal relationships between
                  features, thereby capturing downstream effects of
                  recourse actions performed in the physical world. We
                  explore how our criteria relate to others, such as
                  counterfactual fairness, and show that fairness of
                  recourse is complementary to fairness of
                  prediction. We study theoretically and empirically
                  how to enforce fair causal recourse by altering the
                  classifier and perform a case study on the Adult
                  dataset. Finally, we discuss whether fairness
                  violations in the data generating process revealed
                  by our criteria may be better addressed by societal
                  interventions as opposed to constraints on the
                  classifier.},
}

@techreport{KusPfiCsaRas05,
  cat =		 {gp},
  author =	 {Malte Ku{\ss} and Tobias Pfingsten and Lehel
                  Csat{\`o} and Carl Edward Rasmussen},
  title =	 {Approximate Inference for Robust {G}aussian Process
                  Regression},
  year =	 2005,
  institution =	 {Max Planck Institute for Biological Cybernetics},
  number =	 136,
  address =	 {T{\"u}bingen, Germany},
  abstract =	 {Gaussian process (GP) priors have been successfully
                  used in non-parametric Bayesian regression and
                  classification models. Inference can be performed
                  analytically only for the regression model with
                  Gaussian noise. For all other likelihood models
                  inference is intractable and various approximation
                  techniques have been proposed. In recent years
                  expectation-propagation (EP) has been developed as a
                  general method for approximate inference. This
                  article provides a general summary of how
                  expectation-propagation can be used for approximate
                  inference in Gaussian process models. Furthermore we
                  present a case study describing its implementation
                  for a new robust variant of Gaussian process
                  regression. To gain further insights into the
                  quality of the EP approximation we present
                  experiments in which we compare to results obtained
                  by Markov chain Monte Carlo (MCMC) sampling.},
  url =		 {.}
}

@article{KusRas05,
  cat =		 {gp approx},
  author =	 {Malte {Ku\ss} and Carl Edward Rasmussen},
  title =	 {Assessing Approximate Inference for Binary
                  {G}aussian Process Classification},
  journal =	 jmlr,
  year =	 2005,
  volume =	 6,
  pages =	 {1679--1704},
  url =
                  {http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf},
  abstract =	 {Gaussian process priors can be used to define
                  flexible, probabilistic classification
                  models. Unfortunately exact Bayesian inference is
                  analytically intractable and various approximation
                  techniques have been proposed. In this work we
                  review and compare Laplace's method and Expectation
                  Propagation for approximate Bayesian inference in
                  the binary Gaussian process classification model. We
                  present a comprehensive comparison of the
                  approximations, their predictive performance and
                  marginal likelihood estimates to results obtained by
                  MCMC sampling. We explain theoretically and
                  corroborate empirically the advantages of
                  Expectation Propagation compared to Laplace's
                  method.}
}

@inproceedings{KusRas06,
  cat =		 {gp rl},
  author =	 {Malte Ku{\ss} and Carl Edward Rasmussen},
  title =	 {Assessing Approximations for {G}aussian Process
                  Classification},
  year =	 2006,
  publisher =	 mit,
  pages =	 {699--706},
  month =	 {April},
  booktitle =	 nips18,
  editor =	 {Y.~Weiss and B.~Sch{\"o}lkopf and J.~Platt},
  address =	 {Cambridge, MA, USA},
  abstract =	 {Gaussian processes are attractive models for
                  probabilistic classification but unfortunately exact
                  inference is analytically intractable. We compare
                  Laplace's method and Expectation Propagation (EP)
                  focusing on marginal likelihood estimates and
                  predictive performance. We explain theoretically and
                  corroborate empirically that EP is superior to
                  Laplace. We also compare to a sophisticated MCMC
                  scheme and show that EP is surprisingly accurate.},
  location =	 {Whistler, BC, Canada},
  URL =		 {.}
}

@inproceedings{LacHusGha11,
  cat =		 {approx gp},
  url =		 {.},
  author =	 {Simon Lacoste-Julien and Ferenc Husz\'{a}r and
                  Zoubin Ghahramani},
  title =	 {Approximate Inference for the Loss-Calibrated
                  {B}ayesian},
  booktitle =	 aistats14,
  year =	 2011,
  editor =	 {Geoff Gordon and David Dunson},
  volume =	 15,
  pages =	 {416--424},
  address =	 {Fort Lauderdale, FL, USA},
  month =	 {April},
  publisher =	 jmlr,
  abstract =	 {We consider the problem of approximate inference in
                  the context of Bayesian decision theory. Traditional
                  approaches focus on approximating general properties
                  of the posterior, ignoring the decision task -- and
                  associated losses -- for which the posterior could
                  be used. We argue that this can be suboptimal and
                  propose instead to \emph{loss-calibrate} the
                  approximate inference methods with respect to the
                  decision task at hand. We present a general
                  framework rooted in Bayesian decision theory to
                  analyze approximate inference from the perspective
                  of losses, opening up several research
                  directions. As a first loss-calibrated approximate
                  inference attempt, we propose an EM-like algorithm
                  on the Bayesian posterior risk and show how it can
                  improve a standard approach to Gaussian process
                  classification when losses are asymmetric.}
}

@inproceedings{LacPalDav13a,
  author =	 {Lacoste-Julien, Simon and Palla, Konstantina and
                  Davies, Alex and Kasneci, Gjergji and Graepel, Thore
                  and Ghahramani, Zoubin},
  booktitle =	 {KDD},
  cat =		 {ir},
  isbn =	 {978-1-4503-2174-7},
  pages =	 {572-580},
  publisher =	 acm,
  title =	 {SIGMa: simple greedy matching for aligning large
                  knowledge bases},
  url =		 {.},
  year =	 2013,
  abstract =	 {The Internet has enabled the creation of a growing
                  number of large-scale knowledge bases in a variety
                  of domains containing complementary
                  information. Tools for automatically aligning these
                  knowledge bases would make it possible to unify many
                  sources of structured knowledge and answer complex
                  queries. However, the efficient alignment of
                  large-scale knowledge bases still poses a
                  considerable challenge. Here, we present Simple
                  Greedy Matching (SiGMa), a simple algorithm for
                  aligning knowledge bases with millions of entities
                  and facts. SiGMa is an iterative propagation
                  algorithm which leverages both the structural
                  information from the relationship graph as well as
                  flexible similarity measures between entity
                  properties in a greedy local search, thus making it
                  scalable. Despite its greedy nature, our experiments
                  indicate that SiGMa can efficiently match some of
                  the world's largest knowledge bases with high
                  precision. We provide additional experiments on
                  benchmark datasets which demonstrate that SiGMa can
                  outperform state-of-the-art approaches both in
                  accuracy and efficiency.}
}

@inproceedings{LalBruBuretal22,
  cat =		 {gp approx mcmc},
  title =	 {Sparse {G}aussian Process Hyperparameters: Optimize
                  or Integrate?},
  author =	 {Vidhi Lalchand and Wessel P. Bruinsma and David R.
                  Burt and Carl E. Rasmussen},
  booktitle =	 {nips36},
  year =	 2022,
  url =		 {https://openreview.net/pdf?id=oDoj_LKI3JZ},
  abstract =	 {The kernel function and its hyperparameters are the
                  central model selection choice in a Gaussian process
                  [Rasmussen and Williams, 2006]. Typically, the
                  hyperparameters of the kernel are chosen by
                  maximising the marginal likelihood, an approach
                  known as Type-II maximum likelihood (ML-II).
                  However, ML-II does not account for hyperparameter
                  uncertainty, and it is well-known that this can lead
                  to severely biased estimates and an underestimation
                  of predictive uncertainty. While there are several
                  works which employ a fully Bayesian characterisation
                  of GPs, relatively few propose such approaches for
                  the sparse GPs paradigm. In this work we propose an
                  algorithm for sparse Gaussian process regression
                  which leverages MCMC to sample from the
                  hyperparameter posterior within the variational
                  inducing point framework of [Titsias, 2009]. This
                  work is closely related to Hensman et al. [2015b],
                  but side-steps the need to sample the inducing
                  points, thereby significantly improving sampling
                  efficiency in the Gaussian likelihood case. We
                  compare this scheme against natural baselines in
                  literature along with stochastic variational GPs
                  (SVGPs) along with an extensive computational
                  analysis.}
}

@inproceedings{LalRas20,
  title =	 {Approximate inference for Fully {B}ayesian
                  {G}aussian process Regression},
  cat =		 {gp np},
  author =	 {Lalchand, Vidhi and Rasmussen, Carl Edward},
  booktitle =	 aabi2,
  pages =	 {1--12},
  year =	 2020,
  organization = {PMLR},
  url =		 {https://proceedings.mlr.press/v118/lalchand20a},
  abstract =	 {Learning in Gaussian Process models occurs through
                  the adaptation of hyperparameters of the mean and
                  the covariance function. The classical approach
                  entails maximizing the marginal likelihood yielding
                  fixed point estimates (an approach called Type II
                  maximum likelihood or ML-II). An alternative
                  learning procedure is to infer the posterior over
                  hyper-parameters in a hierarchical specication of
                  GPs we call Fully Bayesian Gaussian Process
                  Regression (GPR). This work considers two
                  approximation schemes for the intractable
                  hyperparameter posterior: 1) Hamiltonian Monte Carlo
                  (HMC) yielding a sampling based approximation and 2)
                  Variational Inference (VI) where the posterior over
                  hyperparameters is approximated by a factorized
                  Gaussian (mean-field) or a full-rank Gaussian
                  accounting for correlations between
                  hyperparameters. We analyse the predictive
                  performance for fully Bayesian GPR on a range of
                  benchmark data sets.}
}

@inproceedings{LalRavLaw22,
  title =	 {Generalised {GPLVM} with {S}tochastic {V}ariational
                  {I}nference },
  cat =		 {gp clust},
  author =	 {Lalchand, Vidhi and Ravuri, Aditya and Lawrence,
                  Neil D.},
  booktitle =	 aistats25,
  pages =	 {7841--7864},
  year =	 2022,
  volume =	 151,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {28--30 Mar},
  publisher =	 {PMLR},
  pdf =
                  {https://proceedings.mlr.press/v151/lalchand22a/lalchand22a.pdf},
  url =		 {https://proceedings.mlr.press/v151/lalchand22a.html},
  abstract =	 {Gaussian process latent variable models (GPLVM) are
                  a flexible and non-linear approach to dimensionality
                  reduction, extending classical Gaussian processes to
                  an unsupervised learning context. The Bayesian
                  incarnation of the GPLVM uses a variational
                  framework, where the posterior over latent variables
                  is approximated by a well-behaved variational
                  family, a factorised Gaussian yielding a tractable
                  lower bound.  However, the non-factorisability of
                  the lower bound prevents truly scalable
                  inference. In this work, we study the doubly
                  stochastic formulation of the Bayesian GPLVM model
                  amenable with minibatch training.  We show how this
                  framework is compatible with different latent
                  variable formulations and perform experiments to
                  compare a suite of models. Further, we demonstrate
                  how we can train in the presence of massively
                  missing data and obtain high-fidelity
                  reconstructions. We demonstrate the model’s
                  performance by benchmarking against the canonical
                  sparse GPLVM for high dimensional data examples.}
}

@inproceedings{LalTazCheEtal22,
  title =	 {Kernel Learning for Explainable Climate Science},
  cat =		 {gp np},
  author =	 {Lalchand, Vidhi and Tazi, Kenza and Cheema, Talay M
                  and Turner, Richard E and Hosking, Scott},
  url =		 {https://arxiv.org/abs/2209.04947},
  booktitle =	 {16th Bayesian Modelling Applications Workshop at
                  UAI, 2022 },
  year =	 2022,
  abstract =	 {The Upper Indus Basin, Himalayas provides water for
                  270 million people and countless ecosystems.
                  However, precipitation, a key component to
                  hydrological modelling, is poorly understood in this
                  area. A key challenge surrounding this uncertainty
                  comes from the complex spatial-temporal distribution
                  of precipitation across the basin. In this work we
                  propose Gaussian processes with structured
                  non-stationary kernels to model precipitation
                  patterns in the UIB. Previous attempts to quantify
                  or model precipitation in the Hindu Kush Karakoram
                  Himalayan region have often been qualitative or
                  include crude assumptions and simplifications which
                  cannot be resolved at lower resolutions.  This body
                  of research also provides little to no error
                  propagation. We account for the spatial variation in
                  precipitation with a non-stationary Gibbs kernel
                  parameterised with an input dependent
                  lengthscale. This allows the posterior function
                  samples to adapt to the varying precipitation
                  patterns inherent in the distinct underlying
                  topography of the Indus region. The input dependent
                  lengthscale is governed by a latent Gaussian process
                  with a stationary squared-exponential kernel to
                  allow the function level hyperparameters to vary
                  smoothly.  In ablation experiments we motivate each
                  component of the proposed kernel by demonstrating
                  its ability to model the spatial covariance,
                  temporal structure and joint spatio-temporal
                  reconstruction. We benchmark our model with a
                  stationary Gaussian process and a Deep Gaussian
                  processes.}
}

@inproceedings{LanKohShaetal22,
  title =	 {Goal Misgeneralization in Deep Reinforcement
                  Learning},
  author =	 {di Langosco, Lauro Langosco and Koch, Jack and
                  Sharkey, Lee D and Pfau, Jacob and Krueger, David},
  booktitle =	 {icml2022},
  year =	 2022,
  url =		 {http://arxiv.org/abs/2105.14111}
}

@article{LauKugKanBar22,
  cat =		 {network},
  title =	 {Complex interlinkages, key objectives and nexuses
                  amongst the Sustainable Development Goals and
                  climate change: a network analysis},
  author =	 {Laumann, F. and von K{\"u}gelgen, J. and Kanashiro
                  Uehara, T. H. and Barahona, M.},
  journal =	 {The Lancet Planetary Health},
  volume =	 6,
  number =	 5,
  pages =	 {E422--E430},
  year =	 2022,
  doi =		 {10.1016/S2542-5196(22)00070-5},
  url =
                  {https://www.thelancet.com/journals/lanplh/article/PIIS2542-5196(22)00070-5/fulltext},
  abstract =	 {Background: Global sustainability is an enmeshed
                  system of complex socioeconomic, climatological, and
                  ecological interactions. The numerous objectives of
                  the UN’s Sustainable Development Goals (SDGs) and
                  the Paris Agreement have various levels of
                  interdependence, making it difficult to ascertain
                  the influence of changes to particular indicators
                  across the whole system. In this analysis, we aimed
                  to detect and rank the complex interlinkages between
                  objectives of sustainability agendas.  Methods: We
                  developed a method to find interlinkages among the
                  17 SDGs and climate change, including non-linear and
                  non-monotonic dependences. We used time series of
                  indicators defined by the World Bank, consisting of
                  400 indicators that measure progress towards the 17
                  SDGs and an 18th variable (annual average
                  temperatures), representing progress in the response
                  to the climate crisis, from 2000 to 2019. This
                  method detects significant dependencies among the
                  time evolution of the objectives by using partial
                  distance correlations, a non-linear measure of
                  conditional dependence that also discounts spurious
                  correlations originating from lurking variables. We
                  then used a network representation to identify the
                  most important objectives (using network centrality)
                  and to obtain nexuses of objectives (defined as
                  highly interconnected clusters in the network).
                  Findings: Using temporal data from 181 countries
                  spanning 20 years, we analysed dependencies among
                  SDGs and climate for 35 country groupings based on
                  region, development, and income level. The observed
                  significant interlinkages, central objectives, and
                  nexuses identified varied greatly across country
                  groupings; however, SDG 17 (partnerships for the
                  goals) and climate change ranked as highly important
                  across many country groupings. Temperature rise was
                  strongly linked to urbanisation, air pollution, and
                  slum expansion (SDG 11), especially in country
                  groupings likely to be worst affected by climate
                  breakdown, such as Africa. In several country
                  groupings composed of developing nations, we
                  observed a consistent nexus of strongly
                  interconnected objectives formed by SDG 1 (poverty
                  reduction), SDG 4 (education), and SDG 8 (economic
                  growth), sometimes incorporating SDG 5 (gender
                  equality), and SDG 16 (peace and justice).
                  Interpretation: The differences across groupings
                  emphasise the need to define goals in accordance
                  with local circumstances and priorities. Our
                  analysis highlights global partnerships (SDG 17) as
                  a pivot in global sustainability efforts, which have
                  been strongly linked to economic growth (SDG
                  8). However, if economic growth and trade expansion
                  were repositioned as a means instead of an end goal
                  of development, our analysis showed that education
                  (SDG 4) and poverty reduction (SDG 1) become more
                  central, thus suggesting that these could be
                  prioritised in global partnerships. Urban
                  livelihoods (SDG 11) were also flagged as important
                  to avoid replicating unsustainable patterns of the
                  past.},
}

@article{LazQuiRasFig10,
  cat =		 {gp},
  author =	 {Miguel L\'azaro-Gredilla and Joaquin
                  Qui{\~n}onero-Candela and Carl Edward Rasmussen and
                  An\'{i}bal Figueiras-Vidal},
  title =	 {Sparse Spectrum {G}aussian Process Regression},
  journal =	 jmlr,
  url =
                  {http://jmlr.csail.mit.edu/papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf},
  volume =	 11,
  pages =	 {1865--1881},
  month =	 {June},
  year =	 2010,
  abstract =	 {We present a new sparse Gaussian Process (GP) model
                  for regression. The key novel idea is to sparsify
                  the \emph{spectral representation} of the GP. This
                  leads to a simple, practical algorithm for
                  regression tasks. We compare the achievable
                  trade-offs between predictive accuracy and
                  computational requirements, and show that these are
                  typically superior to existing state-of-the-art
                  sparse approximations. We discuss both the weight
                  space and function space representations, and note
                  that the new construction implies priors over
                  functions which are always stationary, and can
                  approximate any covariance function in this class.}
}

@article{LeoStoTurGos2014,
  cat =		 {neuro},
  author =	 {Victoria Leong and Michael A Stone and Richard E
                  Turner and Usha Goswami},
  title =	 {A role for amplitude modulation phase relationships
                  in speech rhythm perception},
  journal =	 {Journal of the Acoustical Society of America},
  year =	 2014,
  volume =	 136,
  pages =	 {366--381},
  publisher =	 {Acoustical Society of America},
  abstract =	 {Prosodic rhythm in speech [the alternation of
                  "Strong" (S) and "weak" (w) syllables] is cued,
                  among others, by slow rates of amplitude modulation
                  (AM) within the speech envelope. However, it is
                  unclear exactly which envelope modulation rates and
                  statistics are the most important for the rhythm
                  percept. Here, the hypothesis that the phase
                  relationship between "Stress" rate (~2 Hz) and
                  "Syllable" rate (~4 Hz) AMs provides a perceptual
                  cue for speech rhythm is tested. In a rhythm
                  judgment task, adult listeners identified AM
                  tone-vocoded nursery rhyme sentences that carried
                  either trochaic (S-w) or iambic patterning
                  (w-S). Manipulation of listeners' rhythm perception
                  was attempted by parametrically phase-shifting the
                  Stress AM and Syllable AM in the vocoder. It was
                  expected that a 1π radian phase-shift (half a cycle)
                  would reverse the perceived rhythm pattern (i.e.,
                  trochaic -> iambic) whereas a 2\pi radian shift
                  (full cycle) would retain the perceived rhythm
                  pattern (i.e., trochaic -> trochaic). The results
                  confirmed these predictions. Listeners judgments of
                  rhythm systematically followed Stress-Syllable AM
                  phase-shifts, but were unaffected by phase-shifts
                  between the Syllable AM and the Sub-beat AM (~14 Hz)
                  in a control condition. It is concluded that the
                  Stress-Syllable AM phase relationship is an
                  envelope-based modulation statistic that supports
                  speech rhythm perception.},
  url =
                  {http://scitation.aip.org/content/asa/journal/jasa/136/1/10.1121/1.4883366},
}

@article{LesChaKleetal10,
  author =	 {J. Leskovec and D. Chakrabarti and J. Kleinberg and
                  C. Faloutsos and Z. Ghahramani},
  year =	 2010,
  title =	 {Kronecker Graphs: An Approach to Modeling Networks},
  journal =	 jmlr,
  volume =	 {11(Feb)},
  pages =	 {985--1042},
  url =		 {.},
  abstract =	 {How can we generate realistic networks? In addition,
                  how can we do so with a mathematically tractable
                  model that allows for rigorous analysis of network
                  properties? Real networks exhibit a long list of
                  surprising properties: Heavy tails for the in- and
                  out-degree distribution, heavy tails for the
                  eigenvalues and eigenvectors, small diameters, and
                  densification and shrinking diameters over
                  time. Current network models and generators either
                  fail to match several of the above properties, are
                  complicated to analyze mathematically, or both. Here
                  we propose a generative model for networks that is
                  both mathematically tractable and can generate
                  networks that have all the above mentioned
                  structural properties. Our main idea here is to use
                  a non-standard matrix operation, the Kronecker
                  product, to generate graphs which we refer to as
                  "Kronecker graphs".<br>First, we show that Kronecker
                  graphs naturally obey common network properties. In
                  fact, we rigorously prove that they do so. We also
                  provide empirical evidence showing that Kronecker
                  graphs can effectively model the structure of real
                  networks.<br>We then present KRONFIT, a fast and
                  scalable algorithm for fitting the Kronecker graph
                  generation model to large real networks. A naive
                  approach to fitting would take super-exponential
                  time. In contrast, KRONFIT takes linear time, by
                  exploiting the structure of Kronecker matrix
                  multiplication and by using statistical simulation
                  techniques.  Experiments on a wide range of large
                  real and synthetic networks show that KRONFIT finds
                  accurate parameters that very well mimic the
                  properties of target networks. In fact, using just
                  four parameters we can accurately model several
                  aspects of global network structure. Once fitted,
                  the model parameters can be used to gain insights
                  about the network structure, and the resulting
                  synthetic graphs can be used for null-models,
                  anonymization, extrapolations, and graph
                  summarization.}
}

@inproceedings{LeyBhaWel22,
  cat =		 {interpretability},
  title =	 {Diverse and Amortised Counterfactual Explanations
                  for Uncertainty Estimates},
  author =	 {Dan Ley and Umang Bhatt and Adrian Weller},
  booktitle =	 {Proceedings of the 36th AAAI Conference on
                  Artificial Intelligence (AAAI)},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2112.02646},
  abstract =	 {To interpret uncertainty estimates from
                  differentiable probabilistic models, recent work has
                  proposed generating a single Counterfactual Latent
                  Uncertainty Explanation (CLUE) for a given data
                  point where the model is uncertain. We broaden the
                  exploration to examine δ-CLUE, the set of potential
                  CLUEs within a δ ball of the original input in
                  latent space. We study the diversity of such sets
                  and find that many CLUEs are redundant; as such, we
                  propose DIVerse CLUE (∇-CLUE), a set of CLUEs which
                  each propose a distinct explanation as to how one
                  can decrease the uncertainty associated with an
                  input. We then further propose GLobal AMortised CLUE
                  (GLAM-CLUE), a distinct, novel method which learns
                  amortised mappings that apply to specific groups of
                  uncertain inputs, taking them and efficiently
                  transforming them in a single function call into
                  inputs for which a model will be certain. Our
                  experiments show that δ-CLUE, ∇-CLUE, and GLAM-CLUE
                  all address shortcomings of CLUE and provide
                  beneficial explanations of uncertainty estimates to
                  practitioners.},
}

@inproceedings{LiGal17,
  cat =		 {approx deep},
  title =	 {{D}ropout {I}nference in {B}ayesian {N}eural
                  {N}etworks with {A}lpha-divergences},
  author =	 {Yingzhen Li and Yarin Gal},
  booktitle =	 icml34,
  year =	 2017,
  month =	 {Aug},
  address =	 {Sydney AUSTRALIA},
  url =		 {http://proceedings.mlr.press/v70/li17a.html},
  abstract =	 {To obtain uncertainty estimates with real-world
                  Bayesian deep learning models, practical inference
                  approximations are needed. Dropout variational
                  inference (VI) for example has been used for machine
                  vision and medical applications, but VI can severely
                  underestimates model uncertainty.  Alpha-divergences
                  are alternative divergences to VI’s KL objective,
                  which are able to avoid VI’s uncertainty
                  underestimation. But these are hard to use in
                  practice: existing techniques can only use Gaussian
                  approximating distributions, and require existing
                  models to be changed radically, thus are of limited
                  use for practitioners.  We propose a
                  re-parametrisation of the alpha-divergence
                  objectives, deriving a simple inference technique
                  which, together with dropout, can be easily
                  implemented with existing models by simply changing
                  the loss of the model.  We demonstrate improved
                  uncertainty estimates and accuracy compared to VI in
                  dropout networks. We study our model’s epistemic
                  uncertainty far away from the data using adversarial
                  images, showing that these can be distinguished from
                  non-adversarial images by examining our model’s
                  uncertainty. }
}

@inproceedings{LiHerTur15,
  cat =		 {approx deep},
  title =	 {{S}tochastic {E}xpectation {P}ropagation},
  author =	 {Yingzhen Li and Jos\'e Miguel Hern\'andez-Lobato and
                  Richard E.~Turner},
  booktitle =	 nips28,
  year =	 2015,
  month =	 {Dec},
  address =	 {Montréal CANADA},
  url =		 {http://arxiv.org/abs/1506.04132},
  abstract =	 {Expectation propagation (EP) is a deterministic
                  approximation algorithm that is often used to
                  perform approximate Bayesian parameter learning. EP
                  approximates the full intractable posterior
                  distribution through a set of local-approximations
                  that are iteratively refined for each datapoint. EP
                  can offer analytic and computational advantages over
                  other approximations, such as Variational Inference
                  (VI), and is the method of choice for a number of
                  models. The local nature of EP appears to make it an
                  ideal candidate for performing Bayesian learning on
                  large models in large-scale datasets
                  settings. However, EP has a crucial limitation in
                  this context: the number approximating factors needs
                  to increase with the number of data-points, N, which
                  often entails a prohibitively large memory
                  overhead. This paper presents an extension to EP,
                  called stochastic expectation propagation (SEP),
                  that maintains a global posterior approximation
                  (like VI) but updates it in a local way (like EP
                  ). Experiments on a number of canonical learning
                  problems using synthetic and real-world datasets
                  indicate that SEP performs almost as well as full
                  EP, but reduces the memory consumption by a factor
                  of N. SEP is therefore ideally suited to performing
                  approximate Bayesian learning in the large model,
                  large dataset setting.}
}

@inproceedings{LiMan18,
  cat =		 {time gm deep},
  title =	 {{D}isentangled {S}equential {A}utoencoder},
  author =	 {Yingzhen Li and Stephan Mandt},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm Sweden},
  url =		 {https://arxiv.org/abs/1803.02991},
  abstract =	 {We present a VAE architecture for encoding and
                  generating high dimensional sequential data, such as
                  video or audio.  Our deep generative model learns a
                  latent representation of the data which is split
                  into a static and dynamic part, allowing us to
                  approximately disentangle latent time-dependent
                  features (dynamics) from features which are
                  preserved over time (content). This architecture
                  gives us partial control over generating content and
                  dynamics by conditioning on either one of these sets
                  of features. In our experiments on artificially
                  generated cartoon video clips and voice recordings,
                  we show that we can convert the content of a given
                  sequence into another one by such content
                  swapping. For audio, this allows us to convert a
                  male speaker into a female speaker and vice versa,
                  while for video we can separately manipulate shapes
                  and dynamics.  Furthermore, we give empirical
                  evidence for the hypothesis that stochastic RNNs as
                  latent state models are more efficient at
                  compressing and generating long sequences than
                  deterministic ones, which may be relevant for
                  applications in video compression.}
}

@inproceedings{LiTur16,
  cat =		 {approx deep},
  title =	 {{R}\'enyi {D}ivergence {V}ariational {I}nference},
  author =	 {Yingzhen Li and Richard E.~Turner},
  booktitle =	 nips29,
  year =	 2016,
  month =	 {Dec},
  address =	 {Barcelona SPAIN},
  url =
                  {https://papers.nips.cc/paper/6208-renyi-divergence-variational-inference},
  abstract =	 {This paper introduces the variational R{\'e}nyi
                  bound (VR) that extends traditional variational
                  inference to R{\'e}nyi's alpha-divergences. This new
                  family of variational methods unifies a number of
                  existing approaches, and enables a smooth
                  interpolation from the evidence lower-bound to the
                  log (marginal) likelihood that is controlled by the
                  value of alpha that parametrises the divergence. The
                  reparameterization trick, Monte Carlo approximation
                  and stochastic optimisation methods are deployed to
                  obtain a tractable and unified framework for
                  optimisation. We further consider negative alpha
                  values and propose a novel variational inference
                  method as a new special case in the proposed
                  framework. Experiments on Bayesian neural networks
                  and variational auto-encoders demonstrate the wide
                  applicability of the VR bound.}
}

@inproceedings{LiTur18,
  cat =		 {approx deep},
  title =	 {{G}radient {E}stimators for {I}mplicit {M}odels},
  author =	 {Yingzhen Li and Richard E.~Turner},
  booktitle =	 {Sixth International Conference on Learning
                  Representations},
  year =	 2018,
  month =	 {May},
  address =	 {Vancouver CANADA},
  url =		 {https://openreview.net/forum?id=SJi9WOeRb},
  abstract =	 {Implicit models, which allow for the generation of
                  samples but not for point-wise evaluation of
                  probabilities, are omnipresent in real-world
                  problems tackled by machine learning and a hot topic
                  of current research. Some examples include data
                  simulators that are widely used in engineering and
                  scientific research, generative adversarial networks
                  (GANs) for image synthesis, and hot-off-the-press
                  approximate inference techniques relying on implicit
                  distributions. The majority of existing approaches
                  to learning implicit models rely on approximating
                  the intractable distribution or optimisation
                  objective for gradient- based optimisation, which is
                  liable to produce inaccurate updates and thus poor
                  models. This paper alleviates the need for such
                  approximations by proposing the Stein gradient
                  estimator, which directly estimates the score
                  function of the implicitly defined distribution. The
                  efficacy of the proposed estimator is empirically
                  demonstrated by examples that include meta-learning
                  for approximate inference and entropy regularised
                  GANs that provide improved sample diversities.}
}

@article{LikArnChoetal21,
  cat =		 {mvision},
  author =	 {Valerii Likhosherstov and Anurag Arnab and Krzysztof
                  Choromanski and Mario Lucic and Yi Tay and Adrian
                  Weller and Mostafa Dehghani},
  title =	 {PolyViT: Co-training Vision Transformers on Images,
                  Videos and Audio},
  journal =	 {CoRR},
  volume =	 {abs/2111.12993},
  year =	 2021,
  url =		 {https://arxiv.org/abs/2111.12993},
  eprinttype =	 {arXiv},
  eprint =	 {2111.12993},
  timestamp =	 {Wed, 01 Dec 2021 15:16:43 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-2111-12993.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org},
  abstract =	 {Can we train a single transformer model capable of
                  processing multiple modalities and datasets, whilst
                  sharing almost all of its learnable parameters? We
                  present PolyViT, a model trained on image, audio and
                  video which answers this question. By co-training
                  different tasks on a single modality, we are able to
                  improve the accuracy of each individual task and
                  achieve state-of-the-art results on 5 standard
                  video- and audio-classification
                  datasets. Co-training PolyViT on multiple modalities
                  and tasks leads to a model that is even more
                  parameter-efficient, and learns representations that
                  generalize across multiple domains. Moreover, we
                  show that co-training is simple and practical to
                  implement, as we do not need to tune hyperparameters
                  for each combination of datasets, but can simply
                  adapt those from standard, single-task training.}
}

@inproceedings{LikChoDavetal21,
  cat =		 {nlp},
  author =	 {Likhosherstov, Valerii and Choromanski, Krzysztof M
                  and Davis, Jared Quincy and Song, Xingyou and
                  Weller, Adrian},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {M. Ranzato and A. Beygelzimer and Y. Dauphin and
                  P.S. Liang and J. Wortman Vaughan},
  pages =	 {6707--6719},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Sub-Linear Memory: How to Make Performers SLiM},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf},
  volume =	 34,
  year =	 2021,
  abstract =	 {The Transformer architecture has revolutionized deep
                  learning on sequential data, becoming ubiquitous in
                  state-of-the-art solutions for a wide variety of
                  applications. Yet vanilla Transformers are
                  notoriously resource-expensive, requiring O(L2) in
                  serial time and memory as functions of input length
                  L. Recent works proposed various linear
                  self-attention mechanisms, scaling only as O(L) for
                  serial computation. We perform a thorough analysis
                  of recent Transformer mechanisms with linear
                  self-attention, Performers, in terms of overall
                  computational complexity. We observe a remarkable
                  computational flexibility: forward and backward
                  propagation can be performed with no approximations
                  using sublinear memory as a function of L (in
                  addition to negligible storage for the input
                  sequence), at a cost of greater time complexity in
                  the parallel setting. In the extreme case, a
                  Performer consumes only O(1) memory during training,
                  and still requires O(L) time. This discovered
                  time-memory tradeoff can be used for training or,
                  due to complete backward-compatibility, for
                  fine-tuning on a low-memory device, e.g. a
                  smartphone or an earlier-generation GPU, thus
                  contributing towards decentralized and democratized
                  deep learning.}
}

@misc{LikChoDubetal22,
  cat =		 {nlp},
  doi =		 {10.48550/ARXIV.2205.15317},
  url =		 {https://arxiv.org/abs/2205.15317},
  author =	 {Likhosherstov, Valerii and Choromanski, Krzysztof
                  and Dubey, Avinava and Liu, Frederick and Sarlos,
                  Tamas and Weller, Adrian},
  keywords =	 {Machine Learning (cs.LG), Artificial Intelligence
                  (cs.AI), FOS: Computer and information sciences,
                  FOS: Computer and information sciences},
  title =	 {Chefs' Random Tables: Non-Trigonometric Random
                  Features},
  publisher =	 {arXiv},
  year =	 2022,
  copyright =	 {arXiv.org perpetual, non-exclusive license},
  abstract =	 {We introduce chefs' random tables (CRTs), a new
                  class of non-trigonometric random features (RFs) to
                  approximate Gaussian and softmax kernels. CRTs are
                  an alternative to standard random kitchen sink (RKS)
                  methods, which inherently rely on the trigonometric
                  maps. We present variants of CRTs where RFs are
                  positive, a key requirement for applications in
                  recent low-rank Transformers. Further variance
                  reduction is possible by leveraging statistics which
                  are simple to compute. One instantiation of CRTs,
                  the optimal positive random features (OPRFs), is to
                  our knowledge the first RF method for unbiased
                  softmax kernel estimation with positive and bounded
                  RFs, resulting in exponentially small tails and much
                  lower variance than its counterparts. As we show,
                  orthogonal random features applied in OPRFs provide
                  additional variance reduction for any dimensionality
                  d (not only asymptotically for sufficiently large d,
                  as for RKS). We test CRTs on many tasks ranging from
                  non-parametric classification to training
                  Transformers for text, speech and image data,
                  obtaining new state-of-the-art results for low-rank
                  text Transformers, while providing linear space and
                  time complexity.}
}

@InProceedings{LikDavChoetal21,
  cat =		 {nlp},
  title =	 { CWY Parametrization: a Solution for Parallelized
                  Optimization of Orthogonal and Stiefel Matrices },
  author =	 {Likhosherstov, Valerii and Davis, Jared and
                  Choromanski, Krzysztof and Weller, Adrian},
  booktitle =	 {Proceedings of The 24th International Conference on
                  Artificial Intelligence and Statistics},
  pages =	 {55--63},
  year =	 2021,
  editor =	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume =	 130,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--15 Apr},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v130/likhosherstov21a/likhosherstov21a.pdf},
  url =
                  {https://proceedings.mlr.press/v130/likhosherstov21a.html},
  abstract =	 { We introduce an efficient approach for optimization
                  over orthogonal groups on highly parallel
                  computation units such as GPUs or TPUs. As in
                  earlier work, we parametrize an orthogonal matrix as
                  a product of Householder reflections. However, to
                  overcome low parallelization capabilities of
                  computing Householder reflections sequentially, we
                  propose employing an accumulation scheme called the
                  compact WY (or CWY) transform – a compact
                  parallelization-friendly matrix representation for
                  the series of Householder reflections. We further
                  develop a novel Truncated CWY (or T-CWY) approach
                  for Stiefel manifold parametrization which has a
                  competitive complexity and, again, yields benefits
                  when computed on GPUs and TPUs. We prove that our
                  CWY and T-CWY methods lead to convergence to a
                  stationary point of the training objective when
                  coupled with stochastic gradient descent. We apply
                  our methods to train recurrent neural network
                  architectures in the tasks of neural machine
                  translation and video prediction. }
}

@InProceedings{LikSonChoetal21,
  cat =		 {mcmc},
  title =	 {Debiasing a First-order Heuristic for Approximate
                  Bi-level Optimization},
  author =	 {Likhosherstov, Valerii and Song, Xingyou and
                  Choromanski, Krzysztof and Davis, Jared Q and
                  Weller, Adrian},
  booktitle =	 {Proceedings of the 38th International Conference on
                  Machine Learning},
  pages =	 {6621--6630},
  year =	 2021,
  editor =	 {Meila, Marina and Zhang, Tong},
  volume =	 139,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {18--24 Jul},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v139/likhosherstov21a/likhosherstov21a.pdf},
  url =
                  {https://proceedings.mlr.press/v139/likhosherstov21a.html},
  abstract =	 {Approximate bi-level optimization (ABLO) consists of
                  (outer-level) optimization problems, involving
                  numerical (inner-level) optimization loops. While
                  ABLO has many applications across deep learning, it
                  suffers from time and memory complexity proportional
                  to the length $r$ of its inner optimization loop. To
                  address this complexity, an earlier first-order
                  method (FOM) was proposed as a heuristic which omits
                  second derivative terms, yielding significant speed
                  gains and requiring only constant memory. Despite
                  FOM’s popularity, there is a lack of theoretical
                  understanding of its convergence properties. We
                  contribute by theoretically characterizing FOM’s
                  gradient bias under mild assumptions. We further
                  demonstrate a rich family of examples where
                  FOM-based SGD does not converge to a stationary
                  point of the ABLO objective. We address this concern
                  by proposing an unbiased FOM (UFOM) enjoying
                  constant memory complexity as a function of $r$. We
                  characterize the introduced time-variance tradeoff,
                  demonstrate convergence bounds, and find an optimal
                  UFOM for a given ABLO problem. Finally, we propose
                  an efficient adaptive UFOM scheme.}
}

@inproceedings{LimCalMac17,
  cat =		 {np rl},
  author =	 {Daniel Limon and Jan-Peter Calliess and Jan
                  Maciejowski},
  title =	 {Learning-based Nonlinear Model Predictive Control},
  booktitle =	 {IFAC 2017 World Congress},
  year =	 2017,
  address =	 {Toulouse, France},
  month =	 {July},
  abstract =	 {This paper presents stabilizing Model Predictive
                  Controllers (MPC) in which prediction models are
                  inferred from experimental data of the inputs and
                  outputs of the plant. Using a nonparametric machine
                  learning technique called LACKI, the estimated
                  (possibly nonlinear) model function together with an
                  estimation of Hoelder constant is provided. Based on
                  these, a number of predictive controllers with
                  stability guaranteed by design are
                  proposed. Firstly, the case when the prediction
                  model is estimated off- line is considered and
                  robust stability and recursive feasibility is
                  ensured by using tightened constraints in the
                  optimisation problem. This controller has been
                  extended to the more interesting and complex case:
                  the online learning of the model, where the new data
                  collected from feedback is added to enhance the
                  prediction model. A on-line learning MPC based on a
                  double sequence of predictions is
                  proposed. Stability of the online learning MPC is
                  proved. These controllers are illustrated by
                  simulation.},
  doi =		 {10.1016/j.ifacol.2017.08.1050}
}

@article{LipGhaBor10,
  cat =		 {bioinf},
  author =	 {C. Lippert and Z. Ghahramani and K. Borgwardt},
  title =	 {Gene function prediction from synthetic lethality
                  networks via ranking on demand},
  journal =	 {Bioinformatics},
  year =	 2010,
  volume =	 26,
  pages =	 {912--918},
  url =		 {.},
  abstract =	 {Motivation: Synthetic lethal interactions represent
                  pairs of genes whose individual mutations are not
                  lethal, while the double mutation of both genes does
                  incur lethality. Several studies have shown a
                  correlation between functional similarity of genes
                  and their distances in networks based on synthetic
                  lethal interactions. However, there is a lack of
                  algorithms for predicting gene function from
                  synthetic lethality interaction networks. <br>
                  Results: In this article, we present a novel
                  technique called kernelROD for gene function
                  prediction from synthetic lethal interaction
                  networks based on kernel machines. We apply our
                  novel algorithm to Gene Ontology functional
                  annotation prediction in yeast.  Our experiments
                  show that our method leads to improved gene function
                  prediction compared with state-of-the-art
                  competitors and that combining genetic and
                  congruence networks leads to a further improvement
                  in prediction accuracy.}
}

@inproceedings{LipSteGhaetal09,
  cat =		 {bioinf},
  volume =	 5,
  author =	 {C. Lippert and O. Stegle and Z. Ghahramani and K.
                  Borgwardt},
  note =	 {ISSN: 1938-7228},
  booktitle =	 aistats12,
  editor =	 {D. van Dyk and M. Welling},
  title =	 {A kernel method for unsupervised structured network
                  inference},
  publisher =	 jmlr,
  year =	 2009,
  month =	 {April},
  address =	 {Clearwater Beach, FL, USA},
  pages =	 {368--375},
  url =		 {.},
  abstract =	 {Network inference is the problem of inferring edges
                  between a set of real-world objects, for instance,
                  interactions between pairs of proteins in
                  bioinformatics. Current kernel-based approaches to
                  this problem share a set of common features: (i)
                  they are supervised and hence require labeled
                  training data; (ii) edges in the network are treated
                  as mutually independent and hence topological
                  properties are largely ignored; (iii) they lack a
                  statistical interpretation. We argue that these
                  common assumptions are often undesirable for network
                  inference, and propose (i) an unsupervised kernel
                  method (ii) that takes the global structure of the
                  network into account and (iii) is statistically
                  motivated. We show that our approach can explain
                  commonly used heuristics in statistical terms. In
                  experiments on social networks, dfferent variants of
                  our method demonstrate appealing predictive
                  performance.}
}

@article{Llo13,
  cat =		 {gp},
  title =	 {GEFCom2012 Hierarchical Load Forecasting: Gradient
                  Boosting Machines and Gaussian Processes},
  author =	 {James Robert Lloyd},
  year =	 2013,
  abstract =	 {This report discusses methods for forecasting hourly
                  loads of a US utility as part of the load
                  forecasting track of the Global Energy Forecasting
                  Competition 2012 hosted on Kaggle. The methods
                  described (gradient boosting machines and Gaussian
                  processes) are generic machine learning / regression
                  algorithms and few domain specific adjustments were
                  made. Despite this, the algorithms were able to
                  produce highly competitive predictions and hopefully
                  they can inspire more reﬁned techniques to compete
                  with state-of-the-art load forecasting
                  methodologies.},
  url =
                  {http://mlg.eng.cam.ac.uk/lloyd/papers/GEFCom2012.pdf},
  journal =	 {International Journal of Forecasting}
}

@phdthesis{Llo15,
  author =	 {James Rovert Lloyd},
  cat =		 {gp time np network},
  title =	 {Representation, learning, description and criticism
                  of probabilistic models with applications to
                  networks, functions and relational data},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2015,
  abstract =	 {This thesis makes contributions to a variety of
                  aspects of probabilistic inference. When performing
                  probabilistic inference, one must first represent
                  one’s beliefs with a probability
                  distribution. Specifying the details of a
                  probability distribution can be a difficult task in
                  many situations, but when expressing beliefs about
                  complex data structures it may not even be apparent
                  what form such a distribution should take. This
                  thesis starts by demonstrating how representation
                  theorems due to Aldous, Hoover and Kallenberg can be
                  used to specify appropriate models for data in the
                  form of networks. These theorems are then extended
                  in order to reveal appropriate probability
                  distributions for arbitrary relational data or
                  databases.  A simpler data structure to specify
                  probability distributions for is that of functions;
                  many probability distributions for functions have
                  been used for centuries. We demonstrate that many of
                  these distributions can be expressed in a common
                  language of Gaussian process kernels constructed
                  from a few base elements and operators. The
                  structure of this language allows for the effective
                  automatic construction of probabilistic models for
                  functions. Furthermore, the formal mathematical
                  language of kernels can be mapped neatly onto
                  natural language allowing for automatic descriptions
                  of the automatically constructed models.  By further
                  automating the construction of statistical models,
                  the need to be able to effectively check or
                  criticise these models becomes greater. This thesis
                  demonstrates how kernel two sample tests can be used
                  to demonstrate where a probabilistic model most
                  disagrees with data allowing for targeted
                  improvements to the model. In proposing a new method
                  of model criticism this thesis also briefly
                  discusses the philosophy of model criticism within
                  the context of probabilistic inference.},
  address =	 {Cambridge, UK},
  url =
                  {https://github.com/jamesrobertlloyd/phd-thesis/raw/master/thesis-final.pdf}
}

@inproceedings{LloDuvGroetal14,
  cat =		 {gp np time},
  author =	 {James Robert Lloyd and David Duvenaud and Roger
                  Grosse and Joshua B. Tenenbaum and Zoubin
                  Ghahramani},
  title =	 {Automatic Construction and Natural-Language
                  Description of Nonparametric Regression Models},
  booktitle =	 {Association for the Advancement of Artificial
                  Intelligence (AAAI)},
  abstract =	 {This paper presents the beginnings of an automatic
                  statistician, focusing on regression problems.  Our
                  system explores an open-ended space of statistical
                  models to discover a good explanation of a data set,
                  and then produces a detailed report with figures and
                  natural-language text. Our approach treats unknown
                  regression functions nonparametrically using
                  Gaussian processes, which has two important
                  consequences.  First, Gaussian processes can model
                  functions in terms of high-level properties
                  (e.g. smoothness, trends, periodicity,
                  changepoints). Taken together with the compositional
                  structure of our language of models this allows us
                  to automatically describe functions in simple
                  terms. Second, the use of flexible nonparametric
                  models and a rich language for composing them in an
                  open-ended manner also results in state-of-the-art
                  extrapolation performance evaluated over 13 real
                  time series data sets from various domains.},
  month =	 {July},
  year =	 2014,
  url =		 {http://arxiv.org/pdf/1402.4304.pdf}
}

@inproceedings{LloGha15,
  cat =		 {gp crit np},
  title =	 {Statistical Model Criticism using Kernel Two Sample
                  Tests},
  author =	 {James Robert Lloyd and Zoubin Ghahramani},
  abstract =	 {We propose an exploratory approach to statistical
                  model criticism using maximum mean discrepancy (MMD)
                  two sample tests. Typical approaches to model
                  criticism require a practitioner to select a
                  statistic by which to measure discrepancies between
                  data and a statistical model. MMD two sample tests
                  are instead constructed as an analytic maximisation
                  over a large space of possible statistics and
                  therefore automatically select the statistic which
                  most shows any discrepancy. We demonstrate on
                  synthetic data that the selected statistic, called
                  the witness function, can be used to identify where
                  a statistical model most misrepresents the data it
                  was trained on. We then apply the procedure to real
                  data where the models being assessed are restricted
                  Boltzmann machines, deep belief networks and
                  Gaussian process regression and demonstrate the ways
                  in which these models fail to capture the properties
                  of the data they are trained on.},
  url =
                  {http://jamesrobertlloyd.com/papers/kernel-model-checking.pdf},
  booktitle =	 nips29,
  year =	 2015,
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {1--9}
}

@inproceedings{LloOrbGhaRoy12,
  cat =		 {gp np network},
  title =	 {Random function priors for exchangeable arrays with
                  applications to graphs and relational data},
  author =	 {James Robert Lloyd and Peter Orbanz and Zoubin
                  Ghahramani and Daniel M. Roy},
  abstract =	 {A fundamental problem in the analysis of structured
                  relational data like graphs, networks, databases,
                  and matrices is to extract a summary of the common
                  structure underlying relations between individual
                  entities.  Relational data are typically encoded in
                  the form of arrays; invariance to the ordering of
                  rows and columns corresponds to exchangeable arrays.
                  Results in probability theory due to Aldous, Hoover
                  and Kallenberg show that exchangeable arrays can be
                  represented in terms of a random measurable function
                  which constitutes the natural model parameter in a
                  Bayesian model.  We obtain a flexible yet simple
                  Bayesian nonparametric model by placing a Gaussian
                  process prior on the parameter function.  Efficient
                  inference utilises elliptical slice sampling
                  combined with a random sparse approximation to the
                  Gaussian process.  We demonstrate applications of
                  the model to network data and clarify its relation
                  to models in the literature, several of which emerge
                  as special cases.  },
  url =
                  {http://books.nips.cc/papers/files/nips25/NIPS2012_0487.pdf},
  booktitle =	 nips26,
  year =	 2012,
  address =	 {Lake Tahoe, California, USA},
  month =	 {December},
  pages =	 {1--9}
}

@phdthesis{Lom17,
  author =	 {Maria Lomeli},
  cat =		 {np clust mcm},
  title =	 {General Bayesian inference schemes in infinite
                  mixture models},
  school =	 {University College London,Gatsby Unit},
  year =	 2017,
  address =	 {London, UK},
  url =		 {https://arxiv.org/abs/1702.08781},
  abstract =	 {Bayesian statistical models allow us to formalise
                  our knowledge about the world and reason about our
                  uncertainty, but there is a need for better
                  procedures to accurately encode its complexity. One
                  way to do so is through compositional models, which
                  are formed by combining blocks consisting of simpler
                  models. One can increase the complexity of the
                  compositional model by either stacking more blocks
                  or by using a not-so-simple model as a building
                  block. This thesis is an example of the latter. One
                  first aim is to expand the choice of Bayesian
                  nonparametric (BNP) blocks for constructing
                  tractable compositional models. So far, most of the
                  models that have a Bayesian nonparametric component
                  use a Dirichlet Process or a Pitman-Yor process
                  because of the availability of tractable and compact
                  representations. This thesis shows how to overcome
                  certain intractabilities in order to obtain
                  analogous compact representations for the class of
                  Poisson-Kingman priors which includes the Dirichlet
                  and Pitman-Yor processes.  A major impediment to the
                  widespread use of Bayesian nonparametric building
                  blocks is that inference is often costly,
                  intractable or difficult to carry out. This is an
                  active research area since dealing with the model's
                  infinite dimensional component forbids the direct
                  use of standard simulation-based methods. The main
                  contribution of this thesis is a variety of
                  inference schemes that tackle this problem: Markov
                  chain Monte Carlo and Sequential Monte Carlo
                  methods, which are exact inference schemes since
                  they target the true posterior. The contributions of
                  this thesis, in a larger context, provide general
                  purpose exact inference schemes in the flavour or
                  probabilistic programming: the user is able to
                  choose from a variety of models, focusing only on
                  the modelling part. Indeed, if the wide enough class
                  of Poisson-Kingman priors is used as one of our
                  blocks, this objective is achieved.}
}

@inproceedings{LomFavTeh15,
  cat =		 {np clust mcm},
  booktitle =	 nips28,
  title =	 {A hybrid sampler for {Poisson-K}ingman mixture
                  models},
  author =	 {Maria Lomeli and Stefano Favaro and Yee Whye Teh},
  year =	 2015,
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {1--9},
  url =
                  {https://papers.nips.cc/paper/5799-a-hybrid-sampler-for-poisson-kingman-mixture-models},
  abstract =	 {This paper concerns the introduction of a new Markov
                  Chain Monte Carlo scheme for posterior sampling in
                  Bayesian nonparametric mixture models with priors
                  that belong to the general Poisson-Kingman class. We
                  present a novel and compact way of representing the
                  infinite dimensional component of the model such
                  that while explicitly representing this infinite
                  component it has less memory and storage
                  requirements than previous MCMC schemes. We describe
                  comparative simulation results demonstrating the
                  efficacy of the proposed MCMC algorithm against
                  existing marginal and conditional MCMC samplers.}
}

@article{LomFavTeh17,
  cat =		 {np clust mcm},
  title =	 {A marginal sampler for {sigma-Stable
                  Poisson-Kingman} mixture models},
  author =	 {Maria Lomeli and Stefano Favaro and Yee Whye Teh},
  year =	 2017,
  journal =	 jcgs,
  volume =	 26,
  pages =	 {44--53},
  year =	 2017,
  url =
                  {http://www.tandfonline.com/doi/full/10.1080/10618600.2015.1110526},
  abstract =	 {We investigate the class of sigma-stable
                  Poisson-Kingman random probability measures (RPMs)
                  in the context of Bayesian nonparametric mixture
                  modeling. This is a large class of discrete RPMs,
                  which encompasses most of the popular discrete RPMs
                  used in Bayesian nonparametrics, such as the
                  Dirichlet process, Pitman-Yor process, the
                  normalized inverse Gaussian process, and the
                  normalized generalized Gamma process. We show how
                  certain sampling properties and marginal
                  characterizations of sigma-stable Poisson-Kingman
                  RPMs can be usefully exploited for devising a Markov
                  chain Monte Carlo (MCMC) algorithm for performing
                  posterior inference with a Bayesian nonparametric
                  mixture model. Specifically, we introduce a novel
                  and efficient MCMC sampling scheme in an augmented
                  space that has a small number of auxiliary variables
                  per iteration. We apply our sampling scheme to a
                  density estimation and clustering tasks with
                  unidimensional and multidimensional datasets, and
                  compare it against competing MCMC sampling
                  schemes. Supplementary materials for this article
                  are available online.}
}

@article{LomRowGreGha18,
  cat =		 {gp np mcm},
  title =	 {{A}ntithetic and {M}onte {C}arlo kernel estimators
                  for partial rankings},
  author =	 {Maria Lomeli and Mark Rowland and Arthur Gretton and
                  Zoubin Ghahramani },
  year =	 2018,
  journal =	 "arXiv preprint arXiv:1807.00400",
  url =		 {https://arxiv.org/abs/1807.00400},
  abstract =	 {In the modern age, rankings data is ubiquitous and
                  it is useful for a variety of applications such as
                  recommender systems, multi-object tracking and
                  preference learning. However, most rankings data
                  encountered in the real world is incomplete, which
                  prevents the direct application of existing
                  modelling tools for complete rankings. Our
                  contribution is a novel way to extend kernel methods
                  for complete rankings to partial rankings, via
                  consistent Monte Carlo estimators for Gram matrices:
                  matrices of kernel values between pairs of
                  observations.  We also present a novel variance
                  reduction scheme based on an antithetic variate
                  construction between permutations to obtain an
                  improved estimator for the Mallows kernel.  The
                  corresponding antithetic kernel estimator has lower
                  variance and we demonstrate empirically that it has
                  a better performance in a variety of Machine
                  Learning tasks.  Both kernel estimators are based on
                  extending kernel mean embeddings to the embedding of
                  a set of full rankings consistent with an observed
                  partial ranking. They form a computationally
                  tractable alternative to previous approaches for
                  partial rankings data. An overview of the existing
                  kernels and metrics for permutations is also
                  provided. }
}

@inproceedings{LopHenSco13,
  cat =		 {sigproc},
  booktitle =	 nips27,
  title =	 {The Randomized Dependence Coefficient},
  author =	 {David Lopez-Paz and Philipp Hennig and Bernhard
                  Scholk\"{o}pf},
  year =	 2013,
  address =	 {Lake Tahoe, California, USA},
  month =	 {December},
  pages =	 {1--9},
  url =		 {http://arxiv.org/pdf/1304.7717},
  abstract =	 {We introduce the {Randomized Dependence Coefficient
                  (RDC)}, a measure of non-linear dependence between
                  random variables of arbitrary dimension based on the
                  {Hirschfeld-Gebelein-R\'enyi Maximum Correlation
                  Coefficient}. {RDC} is defined in terms of
                  correlation of random non-linear copula projections;
                  it is invariant with respect to marginal
                  distribution transformations, has low computational
                  cost and is easy to implement: just five lines of
                  {R} code, included at the end of the paper.}
}

@inproceedings{LopHerGha13,
  cat =		 {gp np approx},
  title =	 {Gaussian Process Vine Copulas for Multivariate
                  Dependence},
  author =	 {David Lopez-Paz and Jos\'e Miguel Hern\'andez-Lobato
                  and Zoubin Ghahramani},
  booktitle =	 icml30,
  year =	 2013,
  month =	 {June},
  address =	 {Atlanta, Georgia, USA},
  url =
                  {http://jmlr.org/proceedings/papers/v28/lopez-paz13.pdf},
  abstract =	 {Copulas allow to learn marginal distributions
                  separately from the multivariate dependence
                  structure (copula) that links them together into a
                  density function. Vine factorizations ease the
                  learning of high-dimensional copulas by constructing
                  a hierarchy of conditional bivariate copulas.
                  However, to simplify inference, it is common to
                  assume that each of these conditional bivariate
                  copulas is independent from its conditioning
                  variables.  In this paper, we relax this assumption
                  by discovering the latent functions that specify the
                  shape of a conditional copula given its conditioning
                  variables We learn these functions by following a
                  Bayesian approach based on sparse Gaussian processes
                  with expectation propagation for scalable,
                  approximate inference. Experiments on real-world
                  datasets show that, when modeling all conditional
                  dependencies, we obtain better estimates of the
                  underlying copula of the data.}
}

@inproceedings{LopHerSco12,
  cat =		 {ssl},
  booktitle =	 nips26,
  title =	 {Semi-Supervised Domain Adaptation with
                  Non-Parametric Copulas},
  author =	 {David Lopez-Paz and Jos\'e Miguel Hern\'andez-Lobato
                  and Bernhard Scholk\"{o}pf},
  year =	 2012,
  address =	 {Lake Tahoe, California, USA},
  month =	 {December},
  pages =	 {1--9},
  url =
                  {http://books.nips.cc/papers/files/nips25/NIPS2012_0307.pdf},
  abstract =	 {A new framework based on the theory of copulas is
                  proposed to address semisupervised domain adaptation
                  problems. The presented method factorizes any
                  multivariate density into a product of marginal
                  distributions and bivariate copula
                  functions. Therefore, changes in each of these
                  factors can be detected and corrected to adapt a
                  density model accross different learning
                  domains. Importantly, we introduce a novel vine
                  copula model, which allows for this factorization in
                  a non-parametric manner. Experimental results on
                  regression problems with real-world data illustrate
                  the efficacy of the proposed approach when compared
                  to state-of-the-art techniques.}
}

@inproceedings{LopSraSmo14a,
  author =	 {Lopez-Paz, David and Sra, Suvrit and Smola, Alex
                  J. and Ghahramani, Zoubin and Sch{\"o}lkopf,
                  Bernhard},
  booktitle =	 {ICML},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {Randomized Nonlinear Component Analysis},
  url =		 {.},
  volume =	 29,
  year =	 2014,
  abstract =	 {Classical techniques such as Principal Component
                  Analysis (PCA) and Canonical Correlation Analysis
                  (CCA) are ubiquitous in statistics. However, these
                  techniques only reveal linear relationships in
                  data. Although nonlinear variants of PCA and CCA
                  have been proposed, they are computationally
                  prohibitive in the large scale.  In a separate
                  strand of recent research, randomized methods have
                  been proposed to construct features that help reveal
                  nonlinear patterns in data. For basic tasks such as
                  regression or classification, random features
                  exhibit little or no loss in performance, while
                  achieving dramatic savings in computational
                  requirements.  In this paper we leverage randomness
                  to design scalable new variants of nonlinear PCA and
                  CCA; our ideas also extend to key multivariate
                  analysis tools such as spectral clustering or
                  LDA. We demonstrate our algorithms through
                  experiments on real-world data, on which we compare
                  against the state-of-the-art. Code in R implementing
                  our methods is provided in the Appendix.}
}

@incollection{LucTurSahHen09a,
  cat =		 {approx neuro mvision},
  title =	 {Occlusive Components Analysis},
  author =	 {J\"{o}rg L\"{u}cke and Richard E. Turner and Maneesh
                  Sahani and Marc Henniges},
  booktitle =	 {nips22},
  publisher =	 {mit},
  editor =	 {Y Bengio and D Schuurmans and J Lafferty and C K I
                  Williams and A Culotta},
  pages =	 {1069--1077},
  year =	 2009,
  abstract =	 {We study unsupervised learning in a probabilistic
                  generative model for occlusion.  The model uses two
                  types of latent variables: one indicates which
                  objects are present in the image, and the other how
                  they are ordered in depth. This depth order then
                  determines how the positions and appearances of the
                  objects present, specified in the model parameters,
                  combine to form the image. We show that the object
                  parameters can be learnt from an unlabelled set of
                  images in which objects occlude one another. Exact
                  maximum-likelihood learning is intractable. However,
                  we show that tractable approximations to Expectation
                  Maximization (EM) can be found if the training
                  images each contain only a small number of objects
                  on average. In numerical experiments it is shown
                  that these approximations recover the correct set of
                  object parameters. Experiments on a novel version of
                  the bars test using colored bars, and experiments on
                  more realistic data, show that the algorithm
                  performs well in extracting the generating
                  causes. Experiments based on the standard bars
                  benchmark test for object learning show that the
                  algorithm performs well in comparison to other
                  recent component extraction approaches.  The model
                  and the learning algorithm thus connect research on
                  occlusion with the research field of multiple-causes
                  component extraction methods.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/lucke-et-al-2009.html},
}

@inproceedings{MacBusCunetal11,
  cat =		 {time neuro},
  booktitle =	 nips25,
  title =	 {Empirical models of spiking in neural populations},
  author =	 {J. H. Macke and L. Busing and J. P. Cunningham and
                  B. M. Yu and K. V. Shenoy and M. Sahani},
  year =	 2011,
  address =	 {Granada, Spain},
  month =	 {December},
  pages =	 {1--8},
  url =		 {.},
  abstract =	 {Neurons in the neocortex code and compute as part of
                  a locally interconnected population. Large-scale
                  multi-electrode recording makes it possible to
                  access these population processes empirically by
                  fitting statistical models to unaveraged data. What
                  statistical structure best describes the concurrent
                  spiking of cells within a local network? We argue
                  that in the cortex, where firing exhibits extensive
                  correlations in both time and space and where a
                  typical sample of neurons still reflects only a very
                  small fraction of the local population, the most
                  appropriate model captures shared variability by a
                  low-dimensional latent process evolving with smooth
                  dynamics, rather than by putative direct
                  coupling. We test this claim by comparing a latent
                  dynamical model with realistic spiking observations
                  to coupled generalised linear spike-response models
                  (GLMs) using cortical recordings. We find that the
                  latent dynamical approach outperforms the GLM in
                  terms of goodness-of- fit, and reproduces the
                  temporal correlations in the data more
                  accurately. We also compare models whose
                  observations models are either derived from a
                  Gaussian or point-process models, finding that the
                  non-Gaussian model provides slightly better
                  goodness-of-fit and more realistic population spike
                  counts.}
}

@inproceedings{Makansietal21,
  cat =		 {deep interpretability},
  title =	 {You Mostly Walk Alone: Analyzing Feature Attribution
                  in Trajectory Prediction},
  author =	 {Makansi, O. and von K{\"u}gelgen, J. and Locatello,
                  F. and Gehler, P. and Janzing, D. and Brox, T. and
                  Sch{\"o}lkopf, B.},
  booktitle =	 iclr10,
  year =	 2022,
  url =		 {https://openreview.net/forum?id=POxF-LEqnF},
  abstract =	 {Predicting the future trajectory of a moving agent
                  can be easy when the past trajectory continues
                  smoothly but is challenging when complex
                  interactions with other agents are involved. Recent
                  deep learning approaches for trajectory prediction
                  show promising performance and partially attribute
                  this to successful reasoning about agent-agent
                  interactions.  However, it remains unclear which
                  features such black-box models actually learn to use
                  for making predictions. This paper proposes a
                  procedure that quantifies the contributions of
                  different cues to model performance based on a
                  variant of Shapley values. Applying this procedure
                  to state-of-the-art trajectory prediction methods on
                  standard benchmark datasets shows that they are, in
                  fact, unable to reason about interactions. Instead,
                  the past trajectory of the target is the only
                  feature used for predicting its future. For a task
                  with richer social interaction patterns, on the
                  other hand, the tested models do pick up such
                  interactions to a certain extent, as quantified by
                  our feature attribution method. We discuss the
                  limits of the proposed method and its links to
                  causality.},
}

@inproceedings{MarPicIalYue21,
  cat =		 {rl approx},
  author =	 {Joseph Marino and Alexandre Piche and Alessandro
                  Davide Ialongo and Yisong Yue},
  title =	 {Iterative Amortized Policy Optimization},
  booktitle =	 nips34,
  year =	 2021,
  editor =	 {M. Ranzato and A. Beygelzimer and Y. Dauphin and
                  P.S. Liang and J. Wortman Vaughan},
  pages =	 {15667--15681},
  publisher =	 {Curran Associates, Inc.},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},
  volume =	 34,
  abstract =	 {Policy networks are a central feature of deep
                  reinforcement learning (RL) algorithms for
                  continuous control, enabling the estimation and
                  sampling of high-value actions. From the variational
                  inference perspective on RL, policy networks, when
                  used with entropy or KL regularization, are a form
                  of amortized optimization, optimizing network
                  parameters rather than the policy distributions
                  directly. However, direct amortized mappings can
                  yield suboptimal policy estimates and restricted
                  distributions, limiting performance and
                  exploration. Given this perspective, we consider the
                  more flexible class of iterative amortized
                  optimizers. We demonstrate that the resulting
                  technique, iterative amortized policy optimization,
                  yields performance improvements over direct
                  amortization on benchmark continuous control tasks.}
}

@inproceedings{MarReqBruetal22,
  cat =		 {deep},
  author =	 {Stratis Markou and James Requeima and Wessel
                  P. Bruinsma and Anna Vaughan and Richard E. Turner},
  title =	 {Practical Conditional Neural Processes via Tractable
                  Dependent Predictions},
  booktitle =	 iclr10,
  year =	 2022,
  url =		 {https://openreview.net/forum?id=3pugbNqOh5m},
  abstract =	 {Conditional Neural Processes (CNPs; Garnelo et al.,
                  2018) are meta-learning models which leverage the
                  flexibility of deep learning to produce
                  well-calibrated predictions and naturally handle
                  off-the-grid and missing data. CNPs scale to large
                  datasets and train with ease. Due to these features,
                  CNPs appear well-suited to tasks from environmental
                  sciences or healthcare. Unfortunately, CNPs do not
                  produce correlated predictions, making them
                  fundamentally inappropriate for many estimation and
                  decision making tasks. Predicting heat waves or
                  floods, for example, requires modelling dependencies
                  in temperature or precipitation over time and space.
                  Existing approaches which model output dependencies,
                  such as Neural Processes (NPs; Garnelo et al.,
                  2018b) or the FullConvGNP (Bruinsma et al., 2021),
                  are either complicated to train or prohibitively
                  expensive. What is needed is an approach which
                  provides dependent predictions, but is simple to
                  train and computationally tractable. In this work,
                  we present a new class of Neural Process models that
                  make correlated predictions and support exact
                  maximum likelihood training that is simple and
                  scalable. We extend the proposed models by using
                  invertible output transformations, to capture
                  non-Gaussian output distributions. Our models can be
                  used in downstream estimation tasks which require
                  dependent function samples. By accounting for output
                  dependencies, our models show improved predictive
                  performance on a range of experiments with synthetic
                  and real data.}
}

@inproceedings{MasZinBroetal21,
  cat =		 {mvision, deep},
  author =	 {Daniela Massiceti and Luisa Zintgraf and John
                  Bronskill and Lida Theodorou and Matthew Tobias
                  Harris and Edward Cutrell and Cecily Morrison and
                  Katja Hofmann and Simone Stumpf},
  title =	 {Orbit: A real-world few-shot dataset for teachable
                  object recognition},
  booktitle =	 {Proceedings of the IEEE/CVF International Conference
                  on Computer Vision},
  pages =	 {10818-10828},
  year =	 2021,
  url =
                  {https://openaccess.thecvf.com/content/ICCV2021/papers/Massiceti_ORBIT_A_Real-World_Few-Shot_Dataset_for_Teachable_Object_Recognition_ICCV_2021_paper.pdf},
  abstract =	 {Object recognition has made great advances in the
                  last decade, but predominately still relies on many
                  high-quality training examples per object
                  category. In contrast, learning new objects from
                  only a few examples could enable many impactful
                  applications from robotics to user
                  personalization. Most few-shot learning research,
                  however, has been driven by benchmark datasets that
                  lack the high variation that these applications will
                  face when deployed in the real-world. To close this
                  gap, we present the ORBIT dataset and benchmark,
                  grounded in the real-world application of teachable
                  object recognizers for people who are
                  blind/low-vision. The dataset contains 3,822 videos
                  of 486 objects recorded by people who are
                  blind/low-vision on their mobile phones. The
                  benchmark reflects a realistic, highly challenging
                  recognition problem, providing a rich playground to
                  drive research in robustness to few-shot,
                  high-variation conditions. We set the benchmark's
                  first state-of-the-art and show there is massive
                  scope for further innovation, holding the potential
                  to impact a broad range of real-world vision
                  applications including tools for the
                  blind/low-vision community.}
}

@article{MatGha14,
  cat =		 {ssl np},
  title =	 {{C}lassification using log {G}aussian {C}ox
                  processes},
  author =	 {Matthews, Alexander G. D. G. and Ghahramani, Zoubin
                  },
  year =	 2014,
  journal =	 "arXiv preprint arXiv:1405.4141",
  url =		 {http://arxiv.org/abs/1405.4141},
  abstract =	 {{M}c{C}ullagh and {Y}ang (2006) suggest a family of
                  classification algorithms based on Cox processes. We
                  further investigate the log Gaussian variant which
                  has a number of appealing properties. Conditioned on
                  the covariates, the distribution over labels is
                  given by a type of conditional Markov random
                  field. In the supervised case, computation of the
                  predictive probability of a single test point scales
                  linearly with the number of training points and the
                  multiclass generalization is straightforward. We
                  show new links between the supervised method and
                  classical nonparametric methods. We give a detailed
                  analysis of the pairwise graph representable Markov
                  random field, which we use to extend the model to
                  semi-supervised learning problems, and propose an
                  inference method based on graph min-cuts. We give
                  the first experimental analysis on supervised and
                  semi-supervised datasets and show good empirical
                  performance.}
}

@inproceedings{MatHenGha14,
  cat =		 {approx},
  title =	 {Comparing lower bounds on the entropy of mixture
                  distributions for use in variational inference},
  author =	 {Matthews, Alexander G. D. G and Hensman, James and
                  Ghahramani, Zoubin},
  address =	 {Montreal, Canada},
  booktitle =	 {{NIPS} workshop on {A}dvances in {V}ariational
                  {I}nference},
  url =
                  {https://sites.google.com/site/variationalworkshop/accepted-papers},
  year =	 2014,
  month =	 {December}
}

@inproceedings{MatHenTurGha15,
  cat =		 {gp np approx},
  title =	 {On {S}parse {V}ariational methods and the
                  {K}ullback-{L}eibler divergence between stochastic
                  processes},
  author =	 {Alexander G D G Matthews and James Hensman and
                  Richard E. Turner and Zoubin Ghahramani},
  booktitle =	 aistats19,
  year =	 2016,
  month =	 {May},
  address =	 {Cadiz, Spain},
  url =		 {http://arxiv.org/abs/1504.07027},
  abstract =	 {The variational framework for learning inducing
                  variables (Titsias, 2009a) has had a large impact on
                  the Gaussian process literature. The framework may
                  be interpreted as minimizing a rigorously defined
                  Kullback-Leibler divergence between the
                  approximating and posterior processes. To our
                  knowledge this connection has thus far gone
                  unremarked in the literature. In this paper we give
                  a substantial generalization of the literature on
                  this topic. We give a new proof of the result for
                  infinite index sets which allows inducing points
                  that are not data points and likelihoods that depend
                  on all function values. We then discuss augmented
                  index sets and show that, contrary to previous
                  works, marginal consistency of augmentation is not
                  enough to guarantee consistency of variational
                  inference with the original model. We then
                  characterize an extra condition where such a
                  guarantee is obtainable. Finally we show how our
                  framework sheds light on interdomain sparse
                  approximations and sparse approximations for Cox
                  processes.}
}

@article{MatHroRowetal18,
  cat =		 {network},
  title =	 {Gaussian process behaviour in wide deep neural
                  networks},
  author =	 {Matthews, Alexander G. D. G. and Hron, Jiri and
                  Rowland, Mark and Turner, Richard E. and Ghahramani,
                  Zoubin},
  journal =	 {ICLR},
  year =	 2018,
  url =		 {https://arxiv.org/abs/1804.11271},
  abstract =	 { Whilst deep neural networks have shown great
                  empirical success, there is still much work to be
                  done to understand their theoretical properties.  In
                  this paper, we study the relationship between
                  random, wide, fully connected, feedforward networks
                  with more than one hidden layer and Gaussian
                  processes with a recursive kernel definition. We
                  show that, under broad conditions, as we make the
                  architecture increasingly wide, the implied random
                  function converges in distribution to a Gaussian
                  process, formalising and extending existing results
                  by Neal (1996) to deep networks.  To evaluate
                  convergence rates empirically, we use maximum mean
                  discrepancy.  We then compare finite Bayesian deep
                  networks from the literature to Gaussian processes
                  in terms of the key predictive quantities of
                  interest, finding that in some cases the agreement
                  can be very close. We discuss the desirability of
                  Gaussian process behaviour and review non-Gaussian
                  alternative models from the literature.  }
}

@article{MatHroTuretal17,
  cat =		 {network gp},
  author =	 { Matthews, Alexander G. D. G. and Hron, Jiri and
                  Turner, Richard E. and Ghahramani, Zoubin},
  title =	 {Sample-then-optimise posterior sampling for
                  {B}ayesian linear models},
  journal =	 {AABI (NeurIPS workshop)},
  year =	 2017,
  url =
                  {http://approximateinference.org/2017/accepted/MatthewsEtAl2017.pdf},
  abstract =	 { In modern machine learning it is common to train
                  models which have an extremely high intrinsic
                  capacity. The results obtained are often i
                  nitialization dependent, are different for disparate
                  optimizers and in some cases have no explicit
                  regularization. This raises difficult questions
                  about generalization. A natural approach to
                  questions of generalization is a Bayesian one. There
                  is therefore a growing literature attempting to
                  understand how Bayesian posterior inference could
                  emerge from the complexity of modern practice, even
                  without having such a procedure as the stated goal.
                  In this work we consider a simple special case where
                  exact Bayesian posterior sampling emerges from
                  sampling (cf initialization) and then gradient
                  descent.  Specifically, for a Bayesian linear model,
                  if we parameterize it in terms of a deterministic
                  function of an isotropic normal prior, then the
                  action of sampling from the prior followed by first
                  order optimization of the squared loss will give a
                  posterior sample. Although the assumptions are
                  stronger than many real problems, it still exhibits
                  the challenging properties of redundant model
                  capacity and a lack of explicit regularizers, along
                  with initialization and optimizer dependence. It is
                  therefore an interesting controlled test case. Given
                  its simplicity, the method itself may turn out to be
                  of independent interest from our original goal.  }
}

@phdthesis{Mca16,
  author =	 {Rowan McAllister},
  cat =		 {time rl},
  title =	 {Bayesian Learning for Data-Efficient Control},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2016,
  address =	 {Cambridge, UK},
  url =		 {.},
  abstract =	 {Applications to learn control of unfamiliar
                  dynamical systems with increasing autonomy are
                  ubiquitous. From robotics, to finance, to industrial
                  processing, autonomous learning helps obviate a
                  heavy reliance on experts for system identification
                  and controller design. Often real world systems are
                  nonlinear, stochastic, and expensive to operate
                  (e.g. slow, energy intensive, prone to wear and
                  tear). Ideally therefore, nonlinear systems can be
                  identified with minimal system interaction. This
                  thesis considers data efficient autonomous learning
                  of control of nonlinear, stochastic systems. Data
                  efficient learning critically requires probabilistic
                  modelling of dynamics. Traditional control
                  approaches use deterministic models, which easily
                  overfit data, especially small datasets. We use
                  probabilistic Bayesian modelling to learn systems
                  from scratch, similar to the PILCO algorithm, which
                  achieved unprecedented data efficiency in learning
                  control of several benchmarks. We extend PILCO in
                  three principle ways. First, we learn control under
                  significant observation noise by simulating a
                  filtered control process using a tractably analytic
                  framework of Gaussian distributions. In addition, we
                  develop the `latent variable belief Markov decision
                  process' when filters must predict under real-time
                  constraints. Second, we improve PILCO's data
                  efficiency by directing exploration with predictive
                  loss uncertainty and Bayesian optimisation,
                  including a novel approximation to the Gittins
                  index. Third, we take a step towards data efficient
                  learning of high-dimensional control using Bayesian
                  neural networks (BNN). Experimentally we show
                  although filtering mitigates adverse effects of
                  observation noise, much greater performance is
                  achieved when optimising controllers with
                  evaluations faithful to reality: by simulating
                  closed-loop filtered control if executing
                  closed-loop filtered control. Thus, controllers are
                  optimised w.r.t. how they are used, outperforming
                  filters applied to systems optimised by unfiltered
                  simulations. We show directed exploration improves
                  data efficiency. Lastly, we show BNN dynamics models
                  are almost as data efficient as Gaussian process
                  models. Results show data efficient learning of
                  high-dimensional control is possible as BNNs scale
                  to high-dimensional state inputs.}
}

@inproceedings{McaGalKenEtal17,
  cat =		 {mvision, interpretability},
  booktitle =	 ijcai,
  title =	 {Concrete Problems for Autonomous Vehicle Safety:
                  Advantages of {B}ayesian Deep Learning,},
  author =	 {Rowan McAllister and Yarin Gal and Alex Kendall and
                  Mark van der Wilk and Amar Shah and Roberto Cipolla
                  and Adrian Weller},
  year =	 2017,
  month =	 {August},
  address =	 {Melbourne, Australia},
  url =		 {https://www.ijcai.org/proceedings/2017/0661.pdf},
  abstract =	 {Autonomous vehicle (AV) software is typically
                  composed of a pipeline of individual components,
                  linking sensor inputs to motor outputs. Erroneous
                  component outputs propagate downstream, hence safe
                  AV software must consider the ultimate effect of
                  each component's errors. Further, improving safety
                  alone is not sufficient. Passengers must also feel
                  safe to trust and use AV systems. To address such
                  concerns, we investigate three under-explored themes
                  for AV research: safety, interpretability, and
                  compliance. Safety can be improved by quantifying
                  the uncertainties of component outputs and
                  propagating them forward through the pipeline.
                  Interpretability is concerned with explaining what
                  the AV observes and why it makes the decisions it
                  does, building reassurance with the passenger.
                  Compliance refers to maintaining some control for
                  the passenger. We discuss open challenges for
                  research within these themes. We highlight the need
                  for concrete evaluation metrics, propose example
                  problems, and highlight possible solutions.}
}

@inproceedings{McaRas17,
  cat =		 {gp time rl},
  booktitle =	 nips31,
  title =	 {Data-Efficient Reinforcement Learning in Continuous
                  State-Action {G}aussian-{POMDP}s},
  author =	 {Rowan McAllister and Carl Edward Rasmussen},
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California},
  url =		 {.},
  abstract =	 {We present a data-efficient reinforcement learning
                  method for continuous state-action systems under
                  significant observation noise. Data-efficient
                  solutions under small noise exist, such as PILCO
                  which learns the cartpole swing-up task in 30s.
                  PILCO evaluates policies by planning
                  state-trajectories using a dynamics model. However,
                  PILCO applies policies to the observed state,
                  therefore planning in observation space. We extend
                  PILCO with filtering to instead plan in belief
                  space, consistent with partially observable Markov
                  decisions process (POMDP) planning. This enables
                  data-efficient learning under significant
                  observation noise, outperforming more naive methods
                  such as post-hoc application of a filter to policies
                  optimised by the original (unfiltered) PILCO
                  algorithm. We test our method on the cartpole
                  swing-up task, which involves nonlinear dynamics and
                  requires nonlinear control.}
}

@phdthesis{Mch14,
  author =	 {Andrew McHutchon},
  cat =		 {gp time rl},
  title =	 {Nonlinear Modelling and Control using {G}aussian
                  Processes},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2014,
  address =	 {Cambridge, UK},
  abstract =	 {In many scientific disciplines it is often required
                  to make predictions about how a system will behave
                  or to deduce the correct control values to elicit a
                  particular desired response. Efficiently solving
                  both of these tasks relies on the construction of a
                  model capturing the system's operation. In the most
                  interesting situations, the model needs to capture
                  strongly nonlinear effects and deal with the
                  presence of uncertainty and noise. Building models
                  for such systems purely based on a theoretical
                  understanding of underlying physical principles can
                  be infeasibly complex and require a large number of
                  simplifying assumptions. An alternative is to use a
                  data-driven approach, which builds a model directly
                  from observations. A powerful and principled
                  approach to doing this is to use a Gaussian Process
                  (GP).<br> In this thesis we start by discussing how
                  GPs can be applied to data sets which have noise
                  affecting their inputs. We present the "Noisy Input
                  GP", which uses a simple local-linearisation to
                  refer the input noise into heteroscedastic output
                  noise, and compare it to other methods both
                  theoretically and empirically. We show that this
                  technique leads to a effective model for nonlinear
                  functions with input and output noise. We then
                  consider the broad topic of GP state space models
                  for application to dynamical systems. We discuss a
                  very wide variety of approaches for using GPs in
                  state space models, including introducing a new
                  method based on moment-matching, which consistently
                  gave the best performance. We analyse the methods in
                  some detail including providing a systematic
                  comparison between approximate-analytic and particle
                  methods. To our knowledge such a comparison has not
                  been provided before in this area. Finally, we
                  investigate an automatic control learning framework,
                  which uses Gaussian Processes to model a system for
                  which we wish to design a controller. Controller
                  design for complex systems is a difficult task and
                  thus a framework which allows an automatic design
                  directly from data promises to be extremely
                  useful. We demonstrate that the previously published
                  framework cannot cope with the presence of
                  observation noise but that the introduction of a
                  state space model dramatically improves its
                  performance. This contribution, along with some
                  other suggested improvements opens the door for this
                  framework to be used in real-world applications.},
  url =		 {.}
}

@inproceedings{MchRas11,
  cat =		 {gp},
  booktitle =	 nips24,
  title =	 {Gaussian Process Training with Input Noise},
  author =	 {Andrew McHutchon and Carl Edward Rasmussen},
  year =	 2011,
  address =	 {Granada, Spain},
  editor =	 {J. Shawe-Taylor and R.S. Zemel and P.L. Bartlett and
                  F. Pereira and K.Q. Weinberger},
  publisher =	 {Curran Associates, Inc.},
  pages =	 {1341--1349},
  url =		 {.},
  abstract =	 {In standard Gaussian Process regression input
                  locations are assumed to be noise free. We present a
                  simple yet effective GP model for training on input
                  points corrupted by i.i.d. Gaussian noise. To make
                  computations tractable we use a local linear
                  expansion about each input point. This allows the
                  input noise to be recast as output noise
                  proportional to the squared gradient of the GP
                  posterior mean. The input noise hyperparameters are
                  trained alongside other hyperparameters by the usual
                  method of maximisation of the marginal likelihood,
                  and allow estimation of the noise levels on each
                  input dimension. Training uses an iterative scheme,
                  which alternates between optimising the
                  hyperparameters and calculating the posterior
                  gradient. Analytic predictive moments can then be
                  found for Gaussian distributed test points. We
                  compare our model to others over a range of
                  different regression problems and show that it
                  improves over current methods.}
}

@inproceedings{MeeGhaNeaetal07,
  cat =		 {np},
  month =	 {September},
  author =	 {E. Meeds and Z. Ghahramani and R. Neal and
                  S.T. Roweis},
  series =	 {Bradford Books},
  note =	 {Online contents gives pages 1002--1009, and 977--984
                  on pdf contents.},
  booktitle =	 nips19,
  editor =	 {B. Sch\"olkopf and J. Platt and T. Hofmann},
  title =	 {Modelling dyadic data with binary latent factors},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  year =	 2007,
  pages =	 {977--984},
  url =		 {.},
  abstract =	 {We introduce binary matrix factorization, a novel
                  model for unsupervised matrix decomposition. The
                  decomposition is learned by fitting a non-parametric
                  Bayesian probabilistic model with binary latent
                  variables to a matrix of dyadic data. Unlike
                  bi-clustering models, which assign each row or
                  column to a single cluster based on a categorical
                  hidden feature, our binary feature model reflects
                  the prior belief that items and attributes can be
                  associated with more than one latent cluster at a
                  time. We provide simple learning and inference rules
                  for this new model and show how to extend it to an
                  infinite model in which the number of features is
                  not a priori fixed but is allowed to grow with the
                  size of the data.}
}

@incollection{MenFreMiretal,
  cat =		 {binf},
  author =	 {Peter Menzel and Jes Frellsen and Mireya Plass and
                  Simon H. R<!>asmussen and Anders Krogh},
  booktitle =	 {Deep Sequencing Data Analysis},
  title =	 {On the Accuracy of Short Read Mapping},
  publisher =	 {Springer},
  year =	 2013,
  volume =	 1038,
  pages =	 {39--59},
  abstract =	 {The development of high-throughput sequencing
                  technologies has revolutionized the way we study
                  genomes and gene regulation. In a single experiment,
                  millions of reads are produced. To gain knowledge
                  from these experiments the first thing to be done is
                  finding the genomic origin of the reads, i.e.,
                  mapping the reads to a reference genome. In this new
                  situation, conventional alignment tools are
                  obsolete, as they cannot handle this huge amount of
                  data in a reasonable amount of time. Thus, new
                  mapping algorithms have been developed, which are
                  fast at the expense of a small decrease in
                  accuracy. In this chapter we discuss the current
                  problems in short read mapping and show that mapping
                  reads correctly is a nontrivial task. Through simple
                  experiments with both real and synthetic data, we
                  demonstrate that different mappers can give
                  different results depending on the type of data, and
                  that a considerable fraction of uniquely mapped
                  reads is potentially mapped to an incorrect
                  location. Furthermore, we provide simple statistical
                  results on the expected number of random matches in
                  a genome (E-value) and the probability of a random
                  match as a function of read length. Finally, we show
                  that quality scores contain valuable information for
                  mapping and why mapping quality should be evaluated
                  in a probabilistic manner. In the end, we discuss
                  the potential of improving the performance of
                  current methods by considering these quality scores
                  in a probabilistic mapping program.},
  doi =		 {10.1007/978-1-62703-514-9_3},
  annote =	 {Peter Menzel and Jes Frellsen contributed equally.}
}

@article{MesLonWelSon19,
  cat =		 {gm},
  author =	 {Ofer Meshi and Ben London and Adrian Weller and
                  David Sontag},
  year =	 2019,
  title =	 {Train and Test Tightness of {LP} Relaxations in
                  Structured Prediction},
  journal =	 jmlr,
  volume =	 {20(13)},
  pages =	 {1--34},
  url =		 {http://jmlr.org/papers/v20/17-535.html},
  abstract =	 {Structured prediction is used in areas including
                  computer vision and natural language processing to
                  predict structured outputs such as segmentations or
                  parse trees. In these settings, prediction is
                  performed by MAP inference or, equivalently, by
                  solving an integer linear program. Because of the
                  complex scoring functions required to obtain
                  accurate predictions, both learning and inference
                  typically require the use of approximate solvers. We
                  propose a theoretical explanation for the striking
                  observation that approximations based on linear
                  programming (LP) relaxations are often tight (exact)
                  on real-world instances. In particular, we show that
                  learning with LP relaxed inference encourages
                  integrality of training instances, and that this
                  training tightness generalizes to test data.}
}

@inproceedings{MesMahWelSon16,
  cat =		 {gm},
  title =	 {Train and Test Tightness of {LP} Relaxations in
                  Structured Prediction},
  booktitle =	 icml33,
  year =	 2016,
  month =	 {June},
  address =	 {New York, NY},
  url =		 {http://jmlr.org/proceedings/papers/v48/meshi16.html},
  author =	 {Ofer Meshi and Mehrdad Mahdavi and Adrian Weller and
                  David Sontag},
  abstract =	 {Structured prediction is used in areas such as
                  computer vision and natural language processing to
                  predict structured outputs such as segmentations or
                  parse trees. In these settings, prediction is
                  performed by MAP inference or, equivalently, by
                  solving an integer linear program. Because of the
                  complex scoring functions required to obtain
                  accurate predictions, both learning and inference
                  typically require the use of approximate solvers. We
                  propose a theoretical explanation to the striking
                  observation that approximations based on linear
                  programming (LP) relaxations are often tight on
                  real-world instances. In particular, we show that
                  learning with LP relaxed inference encourages
                  integrality of training instances, and that
                  tightness generalizes from train to test data.}
}

@phdthesis{Moh11,
  author =	 {Shakir Mohamed},
  title =	 {Generalised {B}ayesian Matrix Factorisation Models},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2011,
  address =	 {Cambridge, UK},
  abstract =	 {Factor analysis and related models for probabilistic
                  matrix factorisation are of central importance to
                  the unsupervised analysis of data, with a colourful
                  history more than a century long. Probabilistic
                  models for matrix factorisation allow us to explore
                  the underlying structure in data, and have relevance
                  in a vast number of application areas including
                  collaborative filtering, source separation, missing
                  data imputation, gene expression analysis,
                  information retrieval, computational finance and
                  computer vision, amongst others.<br> This thesis
                  develops generalisations of matrix factorisation
                  models that advance our understanding and enhance
                  the applicability of this important class of
                  models. The generalisation of models for matrix
                  factorisation focuses on three concerns: widening
                  the applicability of latent variable models to the
                  diverse types of data that are currently available;
                  considering alternative structural forms in the
                  underlying representations that are inferred; and
                  including higher order data structures into the
                  matrix factorisation framework. These three issues
                  reflect the reality of modern data analysis and we
                  develop new models that allow for a principled
                  exploration and use of data in these settings. We
                  place emphasis on Bayesian approaches to learning
                  and the advantages that come with the Bayesian
                  methodology. Our port of departure is a
                  generalisation of latent variable models to members
                  of the exponential family of distributions. This
                  generalisation allows for the analysis of data that
                  may be real-valued, binary, counts, non-negative or
                  a heterogeneous set of these data types. The model
                  unifies various existing models and constructs for
                  unsupervised settings, the complementary framework
                  to the generalised linear models in regression.<br>
                  Moving to structural considerations, we develop
                  Bayesian methods for learning sparse latent
                  representations. We define ideas of weakly and
                  strongly sparse vectors and investigate the classes
                  of prior distributions that give rise to these forms
                  of sparsity, namely the scale-mixture of Gaussians
                  and the spike-and-slab distribution. Based on these
                  sparsity favouring priors, we develop and compare
                  methods for sparse matrix factorisation and present
                  the first comparison of these sparse learning
                  approaches. As a second structural consideration, we
                  develop models with the ability to generate
                  correlated binary vectors. Moment-matching is used
                  to allow binary data with specified correlation to
                  be generated, based on dichotomisation of the
                  Gaussian distribution. We then develop a novel and
                  simple method for binary PCA based on Gaussian
                  dichotomisation. The third generalisation considers
                  the extension of matrix factorisation models to
                  multi-dimensional arrays of data that are
                  increasingly prevalent. We develop the first
                  Bayesian model for non-negative tensor factorisation
                  and explore the relationship between this model and
                  the previously described models for matrix
                  factorisation.},
  url =		 {.}
}

@inproceedings{MohHelGha08,
  title =	 {Bayesian Exponential Family {PCA}},
  author =	 {Shakir Mohamed and Katherine A. Heller and Zoubin
                  Ghahramani},
  booktitle =	 nips21,
  editor =	 {D. Koller and D. Schuurmans and Y. Bengio and
                  L. Bottou},
  pages =	 {1089--1096},
  year =	 2009,
  month =	 {December},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {Principal Components Analysis (PCA) has become
                  established as one of the key tools for
                  dimensionality reduction when dealing with real
                  valued data. Approaches such as exponential family
                  PCA and non-negative matrix factorisation have
                  successfully extended PCA to non-Gaussian data
                  types, but these techniques fail to take advantage
                  of Bayesian inference and can suffer from problems
                  of overfitting and poor generalisation. This paper
                  presents a fully probabilistic approach to PCA,
                  which is generalised to the exponential family,
                  based on Hybrid Monte Carlo sampling. We describe
                  the model which is based on a factorisation of the
                  observed data matrix, and show performance of the
                  model on both synthetic and real data.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/shakir/papers/NIPS08spotlight.pdf">spotlight</a>.}
}

@inproceedings{MohHelGha12,
  author =	 {Shakir Mohamed and Katherine A. Heller and Zoubin
                  Ghahramani},
  year =	 2012,
  title =	 {{B}ayesian and {L}<sub>1</sub> Approaches for Sparse
                  Unsupervised Learning},
  booktitle =	 icml29,
  abstract =	 {The use of L1 regularisation for sparse learning has
                  generated immense research interest, with many
                  successful applications in diverse areas such as
                  signal acquisition, image coding, genomics and
                  collaborative filtering. While existing work
                  highlights the many advantages of L1 methods, in
                  this paper we find that L1 regularisation often
                  dramatically under-performs in terms of predictive
                  performance when compared to other methods for
                  inferring sparsity. We focus on unsupervised latent
                  variable models, and develop L1 minimising factor
                  models, Bayesian variants of "L1", and Bayesian
                  models with a stronger L0-like sparsity induced
                  through spike-and-slab distributions. These
                  spike-and-slab Bayesian factor models encourage
                  sparsity while accounting for uncertainty in a
                  principled manner, and avoid unnecessary shrinkage
                  of non-zero values. We demonstrate on a number of
                  data sets that in practice spike-and-slab Bayesian
                  methods out-perform L1 minimisation, even on a com-
                  putational budget. We thus highlight the need to
                  re-assess the wide use of L1 methods in
                  sparsity-reliant applications, particularly when we
                  care about generalising to previously unseen data,
                  and provide an alternative that, over many varying
                  conditions, provides improved generalisation
                  performance.},
  url =		 {.}
}

@inproceedings{MosObePic22,
  cat =		 {gp active},
  author =	 {Henry B. Moss and Sebastian W. Ober and Victor
                  Picheny},
  title =	 {Information-theoretic Inducing Point Placement for
                  High-Throughput {B}ayesian Optimisation},
  booktitle =	 {ICML Workshop on Adaptive Experimental Design and
                  Active Learning in the Real World (RealML)},
  year =	 2022,
  url =		 {https://realworldml.github.io/files/cr/paper3.pdf},
  abstract =	 {Sparse Gaussian Processes are a key component of
                  high-throughput Bayesian optimisation (BO) loops —
                  an increasingly common setting where evaluation
                  budgets are large and highly parallelised. By using
                  representative subsets of the available data to
                  build approximate posteriors, sparse models
                  dramatically reduce the computational costs of
                  surrogate modelling by relying on a small set of
                  pseudo-observations, the so-called inducing points,
                  in lieu of the full data set. However, current
                  approaches to design inducing points are not
                  appropriate within BO loops as they seek to reduce
                  global uncertainty in the objective function. Thus,
                  the high-fidelity modelling of promising and
                  data-dense regions required for precise optimisation
                  is sacrificed and computational resources are
                  instead wasted on modelling areas of the space
                  already known to be sub-optimal. Inspired by
                  entropy-based BO methods, we propose a novel
                  inducing point design that uses a principled
                  information-theoretic criterion to select inducing
                  points. By choosing inducing points to maximally
                  reduce both global uncertainty and uncertainty in
                  the maximum value of the objective function, we
                  build surrogate models able to support
                  high-precision high-throughput BO.},
}

@article{MovChoKnoetal11,
  cat =		 {bioinf},
  author =	 {Mehregan Movassagh and Mun-Kit Choy and David
                  A. Knowles and Lina Cordeddu and Syed Haider and
                  Thomas Down and Lee Siggens and Ana Vujic and Ilenia
                  Simeoni and Chris Penkett and Martin Goddard and
                  Pietro Lio and Martin Bennett and Roger Foo},
  title =	 {Distinct epigenomic features in human
                  cardiomyopathy},
  url =
                  {http://circ.ahajournals.org/content/early/2011/10/24/CIRCULATIONAHA.111.040071.abstract},
  journal =	 {Circulation, American Heart Association},
  year =	 2011,
  abstract =	 {Background. The epigenome refers to marks on the
                  genome including DNA methylation and histone
                  modifications that regulate the expression of
                  underlying genes. A consistent profile of gene
                  expression changes in end- stage cardiomyopathy led
                  us to hypothesise that distinct global patterns of
                  the epigenome may also exist.  Methods and
                  Results. We constructed genome-wide maps of DNA
                  methylation and Histone-3 Lysine-36 tri-methylation
                  (H3K36me3)-enrichment for cardiomyopathic and normal
                  human hearts. 506Mb of sequence per library was
                  generated by high-throughput sequencing, covering 24
                  million out of the 28 million CG di-nucleotides in
                  the human genome. DNA methylation was significantly
                  different in promoter CpG-islands (CGI), intra-genic
                  CGI, gene bodies and H3K36me3-enriched regions of
                  the genome. Moreover DNA methylation differences
                  were present in promoters of upregulated genes but
                  not down-regulated genes. The profile of
                  H3K36me3-enrichment itself was also significantly
                  different in protein-coding regions of the genome.
                  Conclusions. Distinct epigenomic patterns exist in
                  important DNA elements of the human cardiac genome
                  in end-stage cardiomyopathy. If epigenomic patterns
                  track with disease progression, assays for the
                  epigenome may be more useful than quantification of
                  mRNA for assessing prognosis in heart failure. These
                  results open up an important new horizon of research
                  and further studies will be needed to determine how
                  epigenomics contribute to altered gene expression in
                  cardiomyopathy.}
}

@inproceedings{MurAllAntEtal21,
  cat =		 {approx deep},
  author =	 {Chelsea Murray and James Urquhart Allingham and
                  Javier Antor\'an and Jos\'e Miguel
                  Hern\'andez-Lobato},
  editor =	 {Melanie F. Pradier and Aaron Schein and Stephanie
                  L. Hyland and Francisco J. R. Ruiz and Jessica Zosa
                  Forde},
  title =	 {Addressing Bias in Active Learning with Depth
                  Uncertainty Networks...  or Not},
  booktitle =	 {I (Still) Can't Believe It's Not Better! Workshop at
                  NeurIPS 2021, Virtual Workshop, December 13, 2021},
  series =	 pmlr,
  volume =	 163,
  pages =	 {59--63},
  publisher =	 {{PMLR}},
  year =	 2021,
  url =		 {https://proceedings.mlr.press/v163/murray22a.html},
  abstract =	 {Farquhar et al. [2021] show that correcting for
                  active learning bias with underparameterised models
                  leads to improved downstream performance. For
                  overparameterised models such as NNs, however,
                  correction leads either to decreased or unchanged
                  performance. They suggest that this is due to an
                  "overfitting bias" which offsets the active learning
                  bias. We show that depth uncertainty networks
                  operate in a low overfitting regime, much like
                  underparameterised models. They should therefore see
                  an increase in performance with bias
                  correction. Surprisingly, they do not. We propose
                  that this negative result, as well as the results
                  Farquhar et al. [2021], can be explained via the
                  lens of the bias-variance decomposition of
                  generalisation error.},
}

@inproceedings{MurGha04a,
  author =	 {Murray, Iain and Ghahramani, Zoubin},
  booktitle =	 {UAI},
  cat =		 {mcmc, gm},
  editor =	 {Chickering, David Maxwell and Halpern, Joseph Y.},
  isbn =	 {0-9749039-0-6},
  pages =	 {392-399},
  publisher =	 {AUAI Press},
  title =	 {Bayesian Learning in Undirected Graphical Models:
                  Approximate {MCMC} Algorithms},
  url =		 {.},
  year =	 2004,
  abstract =	 {Bayesian learning in undirected graphical
                  models|computing posterior distributions over
                  parameters and predictive quantities is
                  exceptionally difficult. We conjecture that for
                  general undirected models, there are no tractable
                  MCMC (Markov Chain Monte Carlo) schemes giving the
                  correct equilibrium distribution over
                  parameters. While this intractability, due to the
                  partition function, is familiar to those performing
                  parameter optimisation, Bayesian learning of
                  posterior distributions over undirected model
                  parameters has been unexplored and poses novel
                  challenges. we propose several approximate MCMC
                  schemes and test on fully observed binary models
                  (Boltzmann machines) for a small coronary heart
                  disease data set and larger artificial
                  systems. While approximations must perform well on
                  the model, their interaction with the sampling
                  scheme is also important. Samplers based on
                  variational mean- field approximations generally
                  performed poorly, more advanced methods using loopy
                  propagation, brief sampling and stochastic dynamics
                  lead to acceptable parameter posteriors. Finally, we
                  demonstrate these techniques on a Markov random
                  field with hidden variables.  }
}

@inproceedings{MurGhaMac06a,
  author =	 {Murray, Iain and Ghahramani, Zoubin and MacKay,
                  David J. C.},
  booktitle =	 {UAI},
  cat =		 {mcmc},
  isbn =	 {0-9749039-2-2},
  publisher =	 {AUAI Press},
  title =	 {{MCMC} for Doubly-intractable Distributions},
  url =		 {.},
  year =	 2006,
  abstract =	 {Markov Chain Monte Carlo (MCMC) algorithms are
                  routinely used to draw samples from distributions
                  with intractable normalization constants. However,
                  standard MCMC algorithms do not apply to
                  doubly-intractable distributions in which there are
                  additional parameter-dependent normalization terms;
                  for example, the posterior over parameters of an
                  undirected graphical model. An ingenious
                  auxiliary-variable scheme (M{\o}ller et al., 2004)
                  offers a solution: exact sampling (Propp and Wilson,
                  1996) is used to sample from a Metropolis--Hastings
                  proposal for which the acceptance probability is
                  tractable. Unfortunately the acceptance probability
                  of these expensive updates can be low. This paper
                  provides a generalization of M{\o}ller et al. (2004)
                  and a new MCMC algorithm, which obtains better
                  acceptance probabilities for the same amount of
                  exact sampling, and removes the need to estimate
                  model parameters before sampling begins.  }
}

@inproceedings{MurMacGha05a,
  author =	 {Murray, Iain and MacKay, David J. C. and Ghahramani,
                  Zoubin and Skilling, John},
  booktitle =	 {NIPS},
  cat =		 {mcmc},
  title =	 {Nested sampling for {P}otts models},
  url =		 {.},
  year =	 2005,
  abstract =	 {Nested sampling is a new Monte Carlo method by
                  Skilling intended for general Bayesian
                  computation. Nested sampling provides a robust
                  alternative to annealing-based methods for computing
                  normalizing constants. It can also generate
                  estimates of other quantities such as posterior
                  expectations. The key technical requirement is an
                  ability to draw samples uniformly from the prior
                  subject to a constraint on the likelihood. We
                  provide a demonstration with the Potts model, an
                  undirected graphical model.  }
}

@inproceedings{MurSbaRasGir03,
  cat =		 {gp rl},
  author =	 {Roderick Murray-Smith and Daniel Sbarbaro and Carl
                  Edward Rasmussen and Agathe Girard},
  title =	 {Adaptive, Cautious, Predictive control with
                  {G}aussian Process Priors},
  year =	 2003,
  publisher =	 {Elsevier Science Ltd},
  pages =	 {1195-1200},
  month =	 {August},
  journal =	 {Proceedings of the 13th IFAC Symposium on System
                  Identification},
  editor =	 {P.~Van den Hof and B.~Wahlberg and S.~Weiland},
  url =		 {.},
  address =	 {Oxford, UK},
  abstract =	 {Nonparametric Gaussian Process models, a Bayesian
                  statistics approach, are used to implement a
                  nonlinear adaptive control law. Predictions,
                  including propagation of the state uncertainty are
                  made over a k-step horizon. The expected value of a
                  quadratic cost function is minimised, over this
                  prediction horizon, without ignoring the variance of
                  the model predictions. The general method and its
                  main features are illustrated on a simulation
                  example.},
  booktitle =	 {IFAC SYSID 2003},
  location =	 {Rotterdam, The Netherlands}
}

@inproceedings{NalLobSmy19,
  cat =		 {deep},
  title =	 {Dropout as a Structured Shrinkage Prior},
  author =	 {Eric Nalisnick and Jos\'e Miguel Hern\'andez-Lobato
                  and Padhraic Smyth},
  booktitle =	 icml36,
  year =	 2019,
  month =	 {June},
  address =	 {Long Beach},
  url =		 {http://proceedings.mlr.press/v97/nalisnick19a.html},
  abstract =	 {Dropout regularization of deep neural networks has
                  been a mysterious yet effective tool to prevent
                  overfitting. Explanations for its success range from
                  the prevention of co-adapted weights to it being a
                  form of cheap Bayesian inference. We propose a novel
                  framework for understanding multiplicative noise in
                  neural networks, considering continuous
                  distributions as well as Bernoulli noise
                  (i.e. dropout). We show that multiplicative noise
                  induces structured shrinkage priors on a network's
                  weights. We derive the equivalence through
                  reparametrization properties of scale mixtures and
                  without invoking any approximations. Given the
                  equivalence, we then show that dropout's Monte Carlo
                  training objective approximates marginal MAP
                  estimation. We leverage these insights to propose a
                  novel shrinkage framework for resnets, terming the
                  prior 'automatic depth determination' as it is the
                  natural analog of automatic relevance determination
                  for network depth. Lastly, we investigate two
                  inference strategies that improve upon the
                  aforementioned MAP approximation in regression
                  benchmarks.}
}

@inproceedings{NavFreTur16,
  cat =		 {gp approx},
  author =	 {Alexandre Khae Wu Navarro and Jes Frellsen and
                  Richard E. Turner},
  title =	 {The {M}ultivariate {G}eneralised von {M}ises
                  Distribution: {I}nference and applications},
  publisher =	 {AAAI Press},
  editors =	 {S. Singh and S. Markovitch},
  booktitle =	 aaai31,
  abstract =	 {Circular variables arise in a multitude of
                  data-modelling contexts ranging from robotics to the
                  social sciences, but they have been largely
                  overlooked by the machine learning community. This
                  paper partially redresses this imbalance by
                  extending some standard probabilistic modelling
                  tools to the circular domain. First we introduce a
                  new multivariate distribution over circular
                  variables, called the multivariate Generalised von
                  Mises (mGvM) distribution. This distribution can be
                  constructed by restricting and renormalising a
                  general multivariate Gaussian distribution to the
                  unit hyper-torus. Previously proposed multivariate
                  circular distributions are shown to be special cases
                  of this construction. Second, we introduce a new
                  probabilistic model for circular regression, that is
                  inspired by Gaussian Processes, and a method for
                  probabilistic principal component analysis with
                  circular hidden variables. These models can leverage
                  standard modelling tools (e.g. covariance functions
                  and methods for automatic relevance
                  determination). Third, we show that the posterior
                  distribution in these models is a mGvM distribution
                  which enables development of an efficient
                  variational free-energy scheme for performing
                  approximate inference and approximate
                  maximum-likelihood learning.},
  year =	 2017,
  month =	 {January},
  address =	 {San Francisco, CA, USA},
  url =		 {https://arxiv.org/abs/1602.05003},
}

@inproceedings{NguLiBuiTur18,
  cat =		 {approx deep},
  title =	 {{V}ariational {C}ontinual {L}earning},
  author =	 {Cuong V.~Nguyen and Yingzhen Li and Thang D.~Bui
                  Richard E.~Turner},
  booktitle =	 {Sixth International Conference on Learning
                  Representations},
  year =	 2018,
  month =	 {May},
  address =	 {Vancouver CANADA},
  url =		 {https://openreview.net/forum?id=BkQqq0gRb},
  abstract =	 {This paper develops variational continual learning
                  (VCL), a simple but general framework for continual
                  learning that fuses online variational inference
                  (VI) and recent advances in Monte Carlo VI for
                  neural networks. The framework can successfully
                  train both deep discriminative models and deep
                  generative models in complex continual learning
                  settings where existing tasks evolve over time and
                  entirely new tasks emerge. Experimental results show
                  that variational continual learning outperforms
                  state-of-the-art continual learning methods on a
                  variety of tasks, avoiding catastrophic forgetting
                  in a fully automatic way.}
}

@article{NicRas08,
  cat =		 {gp approx},
  author =	 {Hannes Nickisch and Carl Edward Rasmussen},
  title =	 {Approximations for Binary {G}aussian Process
                  Classification},
  year =	 2008,
  volume =	 9,
  pages =	 {2035--2078},
  month =	 {October},
  journal =	 jmlr,
  abstract =	 {We provide a comprehensive overview of many recent
                  algorithms for approximate inference in Gaussian
                  process models for probabilistic binary
                  classification. The relationships between several
                  approaches are elucidated theoretically, and the
                  properties of the different algorithms are
                  corroborated by experimental results. We examine
                  both 1) the quality of the predictive distributions
                  and 2) the suitability of the different marginal
                  likelihood approximations for model selection
                  (selecting hyperparameters) and compare to a gold
                  standard based on MCMC. Interestingly, some methods
                  produce good predictive distributions although their
                  marginal likelihood approximations are poor.  Strong
                  conclusions are drawn about the methods: The
                  Expectation Propagation algorithm is almost always
                  the method of choice unless the computational budget
                  is very tight. We also extend existing methods in
                  various ways, and provide unifying code implementing
                  all approaches.},
  url =
                  {http://www.jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf}
}

@inproceedings{NicRas10,
  cat =		 {gp},
  author =	 {Hannes Nickisch and Carl Edward Rasmussen},
  title =	 {Gaussian Mixture Modeling with {G}aussian Process
                  Latent Variable Models},
  publisher =	 {Springer},
  series =	 lncs,
  year =	 2010,
  month =	 {September},
  address =	 {Darmstadt, Germany},
  url =		 {.},
  booktitle =	 {Proceedings of the 32nd DAGM Symposium on Pattern
                  Recognition},
  abstract =	 {Density modeling is notoriously difficult for high
                  dimensional data. One approach to the problem is to
                  search for a lower dimensional manifold which
                  captures the main characteristics of the
                  data. Recently, the Gaussian Process Latent Variable
                  Model (GPLVM) has successfully been used to find low
                  dimensional manifolds in a variety of complex
                  data. The GPLVM consists of a set of points in a low
                  dimensional latent space, and a stochastic map to
                  the observed space. We show how it can be
                  interpreted as a density model in the observed
                  space. However, the GPLVM is not trained as a
                  density model and therefore yields bad density
                  estimates. We propose a new training strategy and
                  obtain improved generalisation performance and
                  better density estimates in comparative evaluations
                  on several benchmark data sets.},
  doi =		 {10.1007/978-3-642-15986-2_28}
}

@inproceedings{NiuDyGha12,
  cat =		 {np clust},
  author =	 {Donglin Niu and Jennifer G. Dy and Z. Ghahramani},
  year =	 2012,
  title =	 {A Nonparametric {B}ayesian Model for Multiple
                  Clustering with Overlapping Feature Views},
  booktitle =	 aistats15,
  abstract =	 {Most clustering algorithms produce a single
                  clustering solution. This is inadequate for many
                  data sets that are multi-faceted and can be grouped
                  and interpreted in many different ways. Moreover,
                  for high-dimensional data, different features may be
                  relevant or irrelevant to each clustering solution,
                  suggesting the need for feature selection in
                  clustering. Features relevant to one clustering
                  interpretation may be different from the ones
                  relevant for an alternative interpretation or view
                  of the data. In this paper, we introduce a
                  probabilistic nonparametric Bayesian model that can
                  discover multiple clustering solutions from data and
                  the feature subsets that are relevant for the
                  clusters in each view. In our model, the features in
                  different views may be shared and therefore the sets
                  of relevant features are allowed to overlap. We
                  model feature relevance to each view using an Indian
                  Buffet Process and the cluster membership in each
                  view using a Chinese Restaurant Process. We provide
                  an inference approach to learn the latent parameters
                  corresponding to this multiple partitioning
                  problem. Our model not only learns the features and
                  clusters in each view but also automatically learns
                  the number of clusters, number of views and number
                  of features in each view.},
  url =		 {.}
}

@article{NovXiaBahetal19,
  cat =		 {network},
  title =	 {{B}ayesian deep {CNN}s with many channels are
                  {G}aussian processes},
  author =	 {Novak, Roman and Xiao, Lechao and Bahri, Yasaman and
                  Lee, Jaehoon and Yang, Greg and Hron, Jiri and
                  Abolafia, Daniel A.  and Pennington, Jeffrey and
                  Sohl-Dickstein, Jascha},
  journal =	 {ICLR},
  year =	 2019,
  url =		 {https://arxiv.org/abs/1810.05148},
  abstract =	 { There is a previously identified equivalence
                  between wide fully connected neural networks (FCNs)
                  and Gaussian processes (GPs). This equivalence
                  enables, for instance, test set predictions that
                  would have resulted from a fully Bayesian,
                  infinitely wide trained FCN to be computed without
                  ever instantiating the FCN, but by instead
                  evaluating the corresponding GP.  In this work, we
                  derive an analogous equivalence for multi-layer
                  convolutional neural networks (CNNs) both with and
                  without pooling layers, and achieve state of the art
                  results on CIFAR10 for GPs without trainable
                  kernels. We also introduce a Monte Carlo method to
                  estimate the GP corresponding to a given neural
                  network architecture, even in cases where the
                  analytic form has too many terms to be
                  computationally feasible.  Surprisingly, in the
                  absence of pooling layers, the GPs corresponding to
                  CNNs with and without weight sharing are
                  identical. As a consequence, translation
                  equivariance, beneficial in finite channel CNNs
                  trained with stochastic gradient descent (SGD), is
                  guaranteed to play no role in the Bayesian treatment
                  of the infinite channel limit—a qualitative
                  difference between the two regimes that is not
                  present in the FCN case.  We confirm experimentally,
                  that while in some scenarios the performance of
                  SGD-trained finite CNNs approaches that of the
                  corresponding GPs as the channel count increases,
                  with careful tuning SGD-trained CNNs can
                  significantly outperform their corresponding GPs,
                  suggesting advantages from SGD training compared to
                  fully Bayesian parameter estimation.  }
}

@article{NovXiaHronetal20,
  cat =		 {network},
  author =	 {Novak, Roman and Xiao, Lechao and Hron, Jiri and
                  Lee, Jaehoon and Sohl-Dickstein, Jascha and
                  Schoenholz, Samuel},
  title =	 {Neural {T}angents: fast and easy infinite networks
                  in {P}ython},
  journal =	 {ICLR},
  year =	 2020,
  url =		 {https://arxiv.org/abs/1912.02803},
  abstract =	 { Neural Tangents is a library designed to enable
                  research into infinite-width neural networks. It
                  provides a high-level API for specifying complex and
                  hierarchical neural network architectures. These
                  networks can then be trained and evaluated either at
                  finite-width as usual or in their infinite-width
                  limit. Infinite-width networks can be trained
                  analytically using exact Bayesian inference or using
                  gradient descent via the Neural Tangent Kernel.
                  Additionally, Neural Tangents provides tools to
                  study gradient descent training dynamics of wide but
                  finite networks in either function space or weight
                  space. The entire library runs out-of-the-box on
                  CPU, GPU, or TPU.  All computations can be
                  automatically distributed over multiple accelerators
                  with near-linear scaling in the number of devices.
                  }
}

@inproceedings{ObeAit21,
  title =	 {Global inducing point variational posteriors for
                  {B}ayesian neural networks and deep {G}aussian
                  processes},
  author =	 {Sebastian W. Ober and Laurence Aitchison},
  booktitle =	 icml38,
  abstract =	 {We consider the optimal approximate posterior over
                  the top-layer weights in a Bayesian neural network
                  for regression, and show that it exhibits strong
                  dependencies on the lower-layer weights. We adapt
                  this result to develop a correlated approximate
                  posterior over the weights at all layers in a
                  Bayesian neural network. We extend this approach to
                  deep Gaussian processes, unifying inference in the
                  two model classes. Our approximate posterior uses
                  learned ``global'' inducing points, which are
                  defined only at the input layer and propagated
                  through the network to obtain inducing inputs at
                  subsequent layers. By contrast, standard ``local'',
                  inducing point methods from the deep Gaussian
                  process literature optimise a separate set of
                  inducing inputs at every layer, and thus do not
                  model correlations across layers. Our method gives
                  state-of-the-art performance for a variational
                  Bayesian method, without data augmentation or
                  tempering, on CIFAR-10 of 86.7%, which is comparable
                  to SGMCMC without tempering but with data
                  augmentation (88% in Wenzel et al. 2020).},
  year =	 2021,
  cat =		 {gp approx deep},
  url =
                  {http://proceedings.mlr.press/v139/ober21a/ober21a.pdf}
}

@inproceedings{ObeAit21b,
  cat =		 {deep approx gp},
  author =	 {Sebastian W. Ober and Laurence Aitchison},
  title =	 {A variational approximate posterior for the deep
                  {W}ishart process},
  booktitle =	 nips34,
  year =	 2021,
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf},
  abstract =	 {Recent work introduced deep kernel processes as an
                  entirely kernel-based alternative to NNs (Aitchison
                  et al. 2020). Deep kernel processes flexibly learn
                  good top-layer representations by alternately
                  sampling the kernel from a distribution over
                  positive semi-definite matrices and performing
                  nonlinear transformations. A particular deep kernel
                  process, the deep Wishart process (DWP), is of
                  particular interest because its prior can be made
                  equivalent to deep Gaussian process (DGP) priors for
                  kernels that can be expressed entirely in terms of
                  Gram matrices. However, inference in DWPs has not
                  yet been possible due to the lack of sufficiently
                  flexible distributions over positive semi-definite
                  matrices. Here, we give a novel approach to
                  obtaining flexible distributions over positive
                  semi-definite matrices by generalising the Bartlett
                  decomposition of the Wishart probability density. We
                  use this new distribution to develop an approximate
                  posterior for the DWP that includes dependency
                  across layers. We develop a doubly-stochastic
                  inducing-point inference scheme for the DWP and show
                  experimentally that inference in the DWP can improve
                  performance over doing inference in a DGP with the
                  equivalent prior.}
}

@inproceedings{ObeRas19,
  cat =		 {deep},
  author =	 {Sebastian W. Ober and Carl Edward Rasmussen},
  title =	 {Benchmarking the neural linear model for regression},
  booktitle =	 aabi2,
  year =	 2019,
  url =		 {https://openreview.net/pdf?id=S1xmc12EKS},
  abstract =	 {The neural linear model is a simple adaptive
                  Bayesian linear regression method that has recently
                  been used in a number of problems ranging from
                  Bayesian optimization to reinforcement
                  learning. Despite its apparent successes in these
                  settings, to the best of our knowledge there has
                  been no systematic exploration of its capabilities
                  on simple regression tasks. In this work we
                  characterize these on the UCI datasets, a popular
                  benchmark for Bayesian regression models, as well as
                  on the recently introduced UCI “gap” datasets, which
                  are better tests of out-of-distribution
                  uncertainty. We demonstrate that the neural linear
                  model is a simple method that shows generally good
                  performance on these tasks, but at the cost of
                  requiring good hyperparameter tuning.}
}

@inproceedings{ObeRasWil21,
  cat =		 {deep gp},
  author =	 {Sebastian W. Ober and Carl Edward Rasmussen and Mark
                  van der Wilk},
  title =	 {The promises and pitfalls of deep kernel learning},
  booktitle =	 uai37,
  year =	 2021,
  url =		 {https://arxiv.org/pdf/2102.12108.pdf},
  abstract =	 {Deep kernel learning (DKL) and related techniques
                  aim to combine the representational power of neural
                  networks with the reliable uncertainty estimates of
                  Gaussian processes. One crucial aspect of these
                  models is an expectation that, because they are
                  treated as Gaussian process models optimized using
                  the marginal likelihood, they are protected from
                  overfitting. However, we identify situations where
                  this is not the case. We explore this behavior,
                  explain its origins and consider how it applies to
                  real datasets. Through careful experimentation on
                  the UCI, CIFAR-10, and the UTKFace datasets, we find
                  that the overfitting from overparameterized maximum
                  marginal likelihood, in which the model is "somewhat
                  Bayesian", can in certain scenarios be worse than
                  that from not being Bayesian at all. We explain how
                  and when DKL can still be successful by
                  investigating optimization dynamics. We also find
                  that failures of DKL can be rectified by a fully
                  Bayesian treatment, which leads to the desired
                  performance improvements over standard neural
                  networks and Gaussian processes.}
}

@inproceedings{OldBroTur21,
  cat =		 {deep},
  title =	 {Attacking Few-Shot Classifiers with Adversarial
                  Support Poisoning},
  author =	 {Elre T. Oldewage and John Bronskill and Richard
                  E. Turner},
  booktitle =	 {A Blessing in Disguise: The Prospects and Perils of
                  Adversarial Machine Learning, Workshop at ICML 2021},
  url =		 {https://openreview.net/forum?id=SBjXIq5Zk8},
  year =	 2021,
  abstract =	 {This paper examines the robustness of deployed
                  few-shot meta-learning systems when they are fed an
                  imperceptibly perturbed few-shot dataset, showing
                  that the resulting predictions on test inputs can
                  become worse than chance. This is achieved by
                  developing a novel attack, Adversarial Support
                  Poisoning or ASP, which crafts a poisoned set of
                  examples. When even a small subset of malicious data
                  points is inserted into the support set of a
                  meta-learner, accuracy is significantly reduced. We
                  evaluate the new attack on a variety of few-shot
                  classification algorithms and scenarios, and propose
                  a form of adversarial training that significantly
                  improves robustness against both poisoning and
                  evasion attacks.}
}

@inproceedings{OldBroTur22,
  cat =		 {deep},
  title =	 {Adversarial Attacks are a Surprisingly Strong
                  Baseline for Poisoning Few-Shot Meta-Learners},
  author =	 {Elre T. Oldewage and John Bronskill and Richard
                  E. Turner},
  booktitle =	 {I Can't Believe It's Not Better, Workshop at Neurips
                  2022},
  year =	 2022,
  abstract =	 {This paper examines the robustness of deployed
                  few-shot meta-learning systems when they are fed an
                  imperceptibly perturbed few-shot dataset. We attack
                  amortized meta-learners, which allows us to craft
                  colluding sets of inputs that are tailored to fool
                  the system's learning algorithm when used as
                  training data. Jointly crafted adversarial inputs
                  might be expected to synergistically manipulate a
                  classifier, allowing for very strong data-poisoning
                  attacks that would be hard to detect. We show that
                  in a white box setting, these attacks are very
                  successful and can cause the target model's
                  predictions to become worse than chance. However, in
                  opposition to the well-known transferability of
                  adversarial examples in general, the colluding sets
                  do not transfer well to different classifiers. We
                  explore two hypotheses to explain this:
                  'overfitting' by the attack, and mismatch between
                  the model on which the attack is generated and that
                  to which the attack is transferred. Regardless of
                  the mitigation strategies suggested by these
                  hypotheses, the colluding inputs transfer no better
                  than adversarial inputs that are generated
                  independently in the usual way. }
}

@article{OliZilWeletal21,
  cat =		 {fairness},
  author =	 {Thomas, Oliver and Zilka, Miri and Weller, Adrian
                  and Quadrianto, Novi},
  title =	 {An Algorithmic Framework for Positive Action},
  journal =	 {Equity and Access in Algorithms, Mechanisms, and
                  Optimization},
  year =	 2021,
  url =		 {https://doi.org/10.1145/3465416.3483303},
  abstract =	 {Positive action is defined within
                  anti-discrimination legislation as voluntary, legal
                  action taken to address an imbalance of opportunity
                  affecting individuals belonging to under-represented
                  groups. Within this theme, we propose a novel
                  algorithmic fairness framework to advance equal
                  representation while respecting anti-discrimination
                  legislation and equal-treatment rights. We use a
                  counterfactual fairness approach to assign one of
                  three outcomes to each candidate: accept; reject; or
                  flagged as a positive action candidate.}
}

@inproceedings{Orb09,
  cat =		 {np},
  author =	 {Peter Orbanz},
  title =	 {Construction of Nonparametric {B}ayesian Models from
                  Parametric {B}ayes Equations},
  url =		 {.},
  publisher =	 mit,
  booktitle =	 nips22,
  year =	 2009,
  pages =	 {1392--1400},
  editor =	 {Y.~Bengio and D.~Schuurmans and J.~Lafferty and
                  C.~K.~I.~Williams and A.~Culotta},
  abstract =	 {We consider the general problem of constructing
                  nonparametric Bayesian models on
                  infinite-dimensional random objects, such as
                  functions, infinite graphs or infinite
                  permutations. The problem has generated much
                  interest in machine learning, where it is treated
                  heuristically, but has not been studied in full
                  generality in nonparametric Bayesian statistics,
                  which tends to focus on models over probability
                  distributions. Our approach applies a standard tool
                  of stochastic process theory, the construction of
                  stochastic processes from their finite-dimensional
                  marginal distributions. The main contribution of the
                  paper is a generalization of the classic Kolmogorov
                  extension theorem to conditional probabilities. This
                  extension allows a rigorous construction of
                  nonparametric Bayesian models from systems of
                  finitedimensional, parametric Bayes equations. Using
                  this approach, we show (i) how existence of a
                  conjugate posterior for the nonparametric model can
                  be guaranteed by choosing conjugate
                  finite-dimensional models in the construction, (ii)
                  how the mapping to the posterior parameters of the
                  nonparametric model can be explicitly determined,
                  and (iii) that the construction of conjugate models
                  in essence requires the finite-dimensional models to
                  be in the exponential family. As an application of
                  our constructive framework, we derive a model on
                  infinite permutations, the nonparametric Bayesian
                  analogue of a model recently proposed for the
                  analysis of rank data.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/porbanz/reports/NIPS2009_0901_extra.pdf">Supplements
                  (proofs)</a> and <a
                  href="http://mlg.eng.cam.ac.uk/porbanz/reports/porbanz_NIPS09TR.pdf">techreport
                  version</a>}
}

@article{Orb11,
  author =	 {Peter Orbanz},
  journal =	 {Electron.\ J.\ Stat.},
  pages =	 {1354--1373},
  title =	 {Projective limit random probabilities on {P}olish
                  spaces},
  volume =	 5,
  year =	 2011,
  url =		 {http://projecteuclid.org/euclid.ejs/1319028571},
  abstract =	 {A pivotal problem in Bayesian nonparametrics is the
                  construction of prior distributions on the space
                  M(V) of probability measures on a given domain V. In
                  principle, such distributions on the
                  infinite-dimensional space M(V) can be constructed
                  from their finite-dimensional marginals—the most
                  prominent example being the construction of the
                  Dirichlet process from finite-dimensional Dirichlet
                  distributions. This approach is both intuitive and
                  applicable to the construction of arbitrary
                  distributions on M(V), but also hamstrung by a
                  number of technical difficulties. We show how these
                  difficulties can be resolved if the domain V is a
                  Polish topological space, and give a representation
                  theorem directly applicable to the construction of
                  any probability distribution on M(V) whose first
                  moment measure is well-defined. The proof draws on a
                  projective limit theorem of Bochner, and on
                  properties of set functions on Polish spaces to
                  establish countable additivity of the resulting
                  random probabilities. }
}

@incollection{OrbTeh10,
  author =	 {Peter Orbanz and Yee-Whye Teh},
  title =	 {Bayesian Nonparametric Models},
  booktitle =	 {Encyclopedia of Machine Learning},
  publisher =	 {Springer},
  year =	 2010
}

@phdthesis{Ort11,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega},
  title =	 {A Unified Framework for Resource-Bounded Agents
                  Interacting with an Unknown Environment},
  school =	 {Department of Engineering, University of Cambridge},
  year =	 2011,
  abstract =	 {The aim of this thesis is to present a mathematical
                  framework for conceptualizing and constructing
                  adaptive autonomous systems under resource
                  constraints. The first part of this thesis contains
                  a concise presentation of the foundations of
                  classical agency: namely the formalizations of
                  decision making and learning. Decision making
                  includes: (a) subjective expected utility (SEU)
                  theory, the framework of decision making under
                  uncertainty; (b) the maximum SEU principle to choose
                  the optimal solution; and (c) its application to the
                  design of autonomous systems, culminating in the
                  Bellman optimality equations. Learning includes: (a)
                  Bayesian probability theory, the theory for
                  reasoning under uncertainty that extends logic; and
                  (b) Bayes-Optimal agents, the application of
                  Bayesian probability theory to the design of optimal
                  adaptive agents. Then, two major problems of the
                  maximum SEU principle are highlighted: (a) the
                  prohibitive computational costs and (b) the need for
                  the causal precedence of the choice of the
                  policy. The second part of this thesis tackles the
                  two aforementioned problems.  First, an
                  information-theoretic notion of resources in
                  autonomous systems is established. Second, a
                  framework for resource-bounded agency is
                  introduced. This includes: (a) a maximum bounded SEU
                  principle that is derived from a set of axioms of
                  utility; (b) an axiomatic model of probabilistic
                  causality, which is applied for the formalization of
                  autonomous systems having uncertainty over their
                  policy and environment; and (c) the Bayesian control
                  rule, which is derived from the maximum bounded SEU
                  principle and the model of causality, implementing a
                  stochastic adaptive control law that deals with the
                  case where autonomous agents are uncertain about
                  their policy and environment.},
  url =
                  {http://www.dcc.uchile.cl/~peortega/home/lib/exe/fetch.php?id=autonomous_agents&amp;cache=cache&amp;media=thesis.pdf}
}

@inproceedings{OrtBra10a,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun},
  title =	 {A conversion between utility and information},
  booktitle =	 {The third conference on artificial general
                  intelligence},
  publisher =	 {Atlantis Press},
  address =	 {Paris},
  pages =	 {115--120},
  year =	 2010,
  url =
                  {http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_35.pdf},
  abstract =	 {Rewards typically express desirabilities or
                  preferences over a set of alternatives. Here we
                  propose that rewards can be defined for any
                  probability distribution based on three desiderata,
                  namely that rewards should be real- valued, additive
                  and order-preserving, where the later implies that
                  more probable events should also be more
                  desirable. Our main result states that rewards are
                  then uniquely determined by the negative information
                  content. To analyze stochastic processes, we define
                  the utility of a realization as its reward rate.
                  Under this interpretation, we show that the expected
                  utility of a stochastic process is its negative
                  entropy rate. Furthermore, we apply our results to
                  analyze agent-environment interactions. We show that
                  the expected utility that will actually be achieved
                  by the agent is given by the negative cross-entropy
                  from the input-output (I/O) distribution of the
                  coupled interaction system and the agent's I/O
                  distribution. Thus, our results allow for an
                  information-theoretic interpretation of the notion
                  of utility and the characterization of
                  agent-environment interactions in terms of entropy
                  dynamics.}
}

@inproceedings{OrtBra10b,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun},
  title =	 {A {B}ayesian rule for adaptive control based on
                  causal interventions},
  booktitle =	 {The third conference on artificial general
                  intelligence},
  publisher =	 {Atlantis Press},
  address =	 {Paris},
  pages =	 {115--120},
  year =	 2010,
  url =
                  {http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_28.pdf},
  abstract =	 {Explaining adaptive behavior is a central problem in
                  artificial intelligence research. Here we formalize
                  adaptive agents as mixture distributions over
                  sequences of inputs and outputs (I/O). Each
                  distribution of the mixture constitutes a "possible
                  world", but the agent does not know which of the
                  possible worlds it is actually facing. The problem
                  is to adapt the I/O stream in a way that is
                  compatible with the true world. A natural measure of
                  adaptation can be obtained by the Kullback Leibler
                  (KL) divergence between the I/O distribution of the
                  true world and the I/O distribution expected by the
                  agent that is uncertain about possible worlds. In
                  the case of pure input streams, the Bayesian mixture
                  provides a well-known solution for this problem. We
                  show, however, that in the case of I/O streams this
                  solution breaks down, because outputs are issued by
                  the agent itself and require a different
                  probabilistic syntax as provided by intervention
                  calculus. Based on this calculus, we obtain a
                  Bayesian control rule that allows modeling adaptive
                  behavior with mixture distributions over I/O
                  streams. This rule might allow for a novel approach
                  to adaptive control based on a minimum
                  KL-principle.}
}

@article{OrtBra10c,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun},
  title =	 {A minimum relative entropy principle for learning
                  and acting},
  journal =	 {Journal of Artificial Intelligence Research},
  volume =	 38,
  pages =	 {475--511},
  year =	 2010,
  url =		 {http://www.jair.org/vol/vol38.html},
  abstract =	 {This paper proposes a method to construct an
                  adaptive agent that is univemacmacrsal with respect
                  to a given class of experts, where each expert is
                  designed specifically for a particular
                  environment. This adaptive control problem is
                  formalized as the problem of minimizing the relative
                  entropy of the adaptive agent from the expert that
                  is most suitable for the unknown environment. If the
                  agent is a passive observer, then the optimal
                  solution is the well-known Bayesian
                  predictor. However, if the agent is active, then its
                  past actions need to be treated as causal
                  interventions on the I/O stream rather than normal
                  probability conditions.  Here it is shown that the
                  solution to this new variational problem is given by
                  a stochastic controller called the Bayesian control
                  rule, which implements adaptive behavior as a
                  mixture of experts. Furthermore, it is shown that
                  under mild assumptions, the Bayesian control rule
                  converges to the control law of the most suitable
                  expert.},
  doi =		 {10.1613/jair.3062}
}

@techreport{OrtBra10d,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun},
  title =	 {An axiomatic formalization of bounded rationality
                  based on a utility-information equivalence},
  institution =	 {Dept. of Engineering, University of Cambridge},
  year =	 2010,
  url =		 {http://arxiv.org/abs/1007.0940},
  abstract =	 {Classic decision-theory is based on the maximum
                  expected utility (MEU) principle, but crucially
                  ignores the resource costs incurred when determining
                  optimal decisions. Here we propose an axiomatic
                  framework for bounded decision-making that considers
                  resource costs. Agents are formalized as probability
                  measures over input-output streams. We postulate
                  that any such probability measure can be assigned a
                  corresponding conjugate utility function based on
                  three axioms: utilities should be real-valued,
                  additive and monotonic mappings of probabilities. We
                  show that these axioms enforce a unique conversion
                  law between utility and probability (and thereby,
                  information).  Moreover, we show that this relation
                  can be characterized as a variational principle:
                  given a utility function, its conjugate probability
                  measure maximizes a free utility
                  functional. Transformations of probability measures
                  can then be formalized as a change in free utility
                  due to the addition of new constraints expressed by
                  a target utility function. Accordingly, one obtains
                  a criterion to choose a probability measure that
                  trades off the maximization of a target utility
                  function and the cost of the deviation from a
                  reference distribution. We show that optimal
                  control, adaptive estimation and adaptive control
                  problems can be solved this way in a
                  resource-efficient way. When resource costs are
                  ignored, the MEU principle is recovered. Our
                  formalization might thus provide a principled
                  approach to bounded rationality that establishes a
                  close link to information theory.}
}

@inproceedings{OrtBra11,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun},
  title =	 {Information, Utility and Bounded Rationality},
  booktitle =	 {The fourth conference on artificial general
                  intelligence},
  year =	 2011,
  volume =	 6830,
  pages =	 {269--274},
  series =	 {Lecture Notes on Artificial Intelligence},
  publisher =	 {Springer-Verlag},
  abstract =	 {Perfectly rational decision-makers maximize expected
                  utility, but crucially ignore the resource costs
                  incurred when determining optimal actions. Here we
                  employ an axiomatic framework for bounded rational
                  decision-making based on a thermodynamic
                  interpretation of resource costs as information
                  costs. This leads to a variational free utility
                  principle akin to thermodynamical free energy that
                  trades off utility and information costs. We show
                  that bounded optimal control solutions can be
                  derived from this variational principle, which leads
                  in general to stochastic policies. Furthermore, we
                  show that risk-sensitive and robust (minimax)
                  control schemes fall out naturally from this
                  framework if the environment is considered as a
                  bounded rational and perfectly rational opponent,
                  respectively. When resource costs are ignored, the
                  maximum expected utility principle is recovered.},
  url =
                  {http://www.dcc.uchile.cl/~peortega/home/lib/exe/fetch.php?id=autonomous_agents&amp;cache=cache&amp;media=infoutilityboundedrationality.pdf}
}

@inproceedings{OrtBraGod11,
  cat =		 {rl},
  author =	 {Pedro A.~Ortega and Daniel A.~Braun and Simon
                  Godsill},
  title =	 {Reinforcement Learning and the {B}ayesian Control
                  Rule},
  booktitle =	 {The fourth conference on artificial general
                  intelligence},
  year =	 2011,
  volume =	 6830,
  pages =	 {281--285},
  series =	 {Lecture Notes on Artificial Intelligence},
  publisher =	 {Springer-Verlag},
  abstract =	 {We present an actor-critic scheme for reinforcement
                  learning in complex domains. The main contribution
                  is to show that planning and I/O dynamics can be
                  separated such that an intractable planning problem
                  reduces to a simple multi-armed bandit problem,
                  where each lever stands for a potentially
                  arbitrarily complex policy. Furthermore, we use the
                  Bayesian control rule to construct an adaptive
                  bandit player that is universal with respect to a
                  given class of optimal bandit players, thus
                  indirectly constructing an adaptive agent that is
                  universal with respect to a given class of
                  policies.},
  url =
                  {http://www.dcc.uchile.cl/~peortega/home/lib/exe/fetch.php?id=autonomous_agents&amp;cache=cache&amp;media=actor-critic.pdf}
}

@inproceedings{OsaSwaJaietal19,
  cat =		 {deep approx},
  title =	 {Practical Deep Learning with {B}ayesian Principles},
  author =	 {Kazuki Osawa and Siddharth Swaroop and Anirudh Jain
                  and Runa Eschenhagen and Richard E. Turner and Rio
                  Yokota and Mohammad Emtiyaz Khan},
  booktitle =	 nips33,
  year =	 2019,
  url =
                  {https://papers.nips.cc/paper/8681-practical-deep-learning-with-bayesian-principles.html},
  abstract =	 {Bayesian methods promise to fix many shortcomings of
                  deep learning, but they are impractical and rarely
                  match the performance of standard methods, let alone
                  improve them. In this paper, we demonstrate
                  practical training of deep networks with
                  natural-gradient variational inference. By applying
                  techniques such as batch normalisation, data
                  augmentation, and distributed training, we achieve
                  similar performance in about the same number of
                  epochs as the Adam optimiser, even on large datasets
                  such as ImageNet. Importantly, the benefits of
                  Bayesian principles are preserved: predictive
                  probabilities are well-calibrated, uncertainties on
                  out-of-distribution data are improved, and
                  continual-learning performance is boosted. This work
                  enables practical deep learning while preserving
                  benefits of Bayesian principles. A PyTorch
                  implementation is available as a plug-and-play
                  optimiser. }
}

@inproceedings{OsbDuvGaretal12,
  cat =		 {gp},
  booktitle =	 nips25,
  title =	 {Active Learning of Model Evidence Using {B}ayesian
                  Quadrature},
  author =	 {Michael A. Osborne and David Duvenaud and Roman
                  Garnett and Carl Edward Rasmussen and Stephen
                  J. Roberts and Zoubin Ghahramani},
  year =	 2012,
  address =	 {Lake Tahoe, California, USA},
  month =	 {December},
  pages =	 {46-54},
  url =
                  {http://www.robots.ox.ac.uk/~mosb/papers/bbq_nips_final.pdf},
  abstract =	 {Numerical integration is a key component of many
                  problems in scientiﬁc computing, statistical
                  modelling, and machine learning. Bayesian Quadrature
                  is a model-based method for numerical integration
                  which, relative to standard Monte Carlo methods,
                  offers increased sample efficiency and a more robust
                  estimate of the uncertainty in the estimated
                  integral. We propose a novel Bayesian Quadrature
                  approach for numerical integration when the
                  integrand is non-negative, such as the case of
                  computing the marginal likelihood, predictive
                  distribution, or normalising constant of a
                  probabilistic model. Our approach approximately
                  marginalises the quadrature model’s hyperparameters
                  in closed form, and introduces an active learning
                  scheme to optimally select function evaluations, as
                  opposed to using Monte Carlo samples. We demonstrate
                  our method on both a number of synthetic benchmarks
                  and a real scientiﬁc problem from astronomy.}
}

@article{OswChaGooetal2022,
  cat =		 {fairness},
  author =	 {Oswald, Marion and Chambers, Luke and Goodman, Ellen
                  P and Ugwudike, Pam and Zilka, Miri},
  title =	 {The UK Algorithmic Transparency Standard: A
                  Qualitative Analysis of Police Perspectives},
  journal =	 {Available at SSRN},
  year =	 2022,
  url =
                  {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4155549},
  abstract =	 {1. The UK Government’s draft ‘Algorithmic
                  Transparency Standard’ is intended to provide a
                  standardised way for public bodies and government
                  departments to provide information about how
                  algorithmic tools are being used to support
                  decisions. The research discussed in this report was
                  conducted in parallel to the piloting of the
                  Standard by the Cabinet Office and the Centre for
                  Data Ethics and Innovation.  2. We conducted
                  semi-structured interviews with respondents from
                  across UK policing and commercial bodies involved in
                  policing technologies. Our aim was to explore the
                  implications for police forces of participation in
                  the Standard, to identify rewards, risks, challenges
                  for the police, and areas where the Standard could
                  be improved, and therefore to contribute to the
                  exploration of policy options for expansion of
                  participation in the Standard.  3. Algorithmic
                  transparency is both achievable for policing and
                  could bring significant rewards. A key reward of
                  police participation in the Standard is that it
                  provides the opportunity to demonstrate proficient
                  implementation of technology-driven policing, thus
                  enhancing earned trust. Research participants
                  highlighted the public good that could result from
                  the considered use of algorithms.  4. Participants
                  noted, however, a risk of misperception of the
                  dangers of policing technology, especially if use of
                  algorithmic tools was not appropriately compared to
                  the status quo and current methods.
                  5. Participation in the Standard provides an
                  opportunity to develop increased sharing among
                  police forces of best practices (and things to
                  avoid), and increased thoughtfulness among police
                  force personnel in building and implementing new
                  tools. Research participants were keen for
                  compliance with the Standard to become an integral
                  part of a holistic system to drive reflective
                  practice across policing around the development and
                  deployment of algorithmic technology. This could
                  enable police to learn from each other, facilitate
                  good policy choices and decrease wasted
                  costs. Otherwise, the Standard may come to be
                  regarded as an administrative burden rather than a
                  benefit for policing.  6. Several key areas for
                  amendment and improvement from the perspective of
                  policing were identified in the research. These
                  could improve the Standard for the benefit of all
                  participants. These include a need for clarification
                  of the scope of the Standard, and the stage of
                  project development at which the Standard should
                  apply. It is recommended that consideration be given
                  to a ‘Standard-Lite’ for projects at the pilot or
                  early stages of the development process in order to
                  gain public understanding of new tools and
                  applications. Furthermore, the Standard would
                  benefit from a more substantial glossary (to include
                  relevant policing terms) and additional guidance on
                  the level of detail required in each section and how
                  accuracy rates should be described, justified and
                  explained in order to ensure consistency.  7. The
                  research does not suggest any overriding reason why
                  the Standard should not be applied in
                  policing. Suitable exemptions for sensitive contexts
                  and tradecraft would be required, however, and
                  consideration given to ensuring that forces have the
                  resources to comply with the Standard and to respond
                  to the increased public interest that could
                  ensue. Limiting the scope initially to tools on a
                  defined list (to include the most high-risk tools,
                  such as those that produce individualised
                  risk/predictive scores) could assist in mitigating
                  concerns over sensitive policing capabilities and
                  resourcing. A non-public version of the Standard for
                  sensitive applications and tools could also be
                  considered, which would be available to bodies with
                  an independent oversight function.  8. To support
                  police compliance with the Standard, supplier
                  responsibilities – including appropriate disclosure
                  of algorithmic functionality, data inputs and
                  performance – should be covered in procurement
                  contracts and addressed up front as a mandatory
                  requirement of doing business with the police.
                  9. As well as contributing to the piloting of the
                  Standard, it is recommended that the findings of
                  this report are considered at NPCC level, by the
                  College of Policing and by the office of the Chief
                  Scientific Advisor for Policing, as new sector-led
                  guidance, best practice and policy are developed.}
}

@inproceedings{PalKnoGha12,
  cat =		 {np network},
  author =	 {Konstantina Palla and David A. Knowles and Zoubin
                  Ghahramani},
  title =	 {An Infinite Latent Attribute Model for Network Data},
  booktitle =	 icml29,
  year =	 2012,
  month =	 {June},
  address =	 {Edinburgh, Scotland},
  url =		 {http://icml.cc/2012/papers/785.pdf},
  abstract =	 {Latent variable models for network data extract a
                  summary of the relational structure underlying an
                  observed network. The simplest possible models
                  subdivide nodes of the network into clusters; the
                  probability of a link between any two nodes then
                  depends only on their cluster assignment. Currently
                  available models can be classified by whether
                  clusters are disjoint or are allowed to
                  overlap. These models can explain a "flat"
                  clustering structure.  Hierarchical Bayesian models
                  provide a natural approach to capture more complex
                  dependencies. We propose a model in which objects
                  are characterised by a latent feature vector. Each
                  feature is itself partitioned into disjoint groups
                  (subclusters), corresponding to a second layer of
                  hierarchy. In experimental comparisons, the model
                  achieves significantly improved predictive
                  performance on social and biological link prediction
                  tasks.  The results indicate that models with a
                  single layer hierarchy over-simplify real networks.}
}

@inproceedings{PalKnoGha14,
  author =	 {Konstantina Palla and David A. Knowles and Zoubin
                  Ghahramani},
  cat =		 {np},
  title =	 {A reversible infinite HMM using normalised random
                  measures },
  booktitle =	 icml31,
  year =	 2014,
  month =	 {June},
  address =	 {Beijing, China},
  url =		 {.},
  abstract =	 {We present a nonparametric prior over reversible
                  Markov chains.  We use completely random measures,
                  specifically gamma processes, to construct a
                  countably infinite graph with weighted edges.  By
                  enforcing symmetry to make the edges undirected we
                  define a prior over random walks on graphs that
                  results in a reversible Markov chain. The resulting
                  prior over infinite transition matrices is closely
                  related to the hierarchical Dirichlet process but
                  enforces reversibility. A reinforcement scheme has
                  recently been proposed with similar properties, but
                  the de Finetti measure is not well characterised.
                  We take the alternative approach of explicitly
                  constructing the mixing measure, which allows more
                  straightforward and efficient inference at the cost
                  of no longer having a closed form predictive
                  distribution. We use our process to construct a
                  reversible infinite HMM which we apply to two real
                  datasets, one from epigenomics and one ion channel
                  recording.}
}

@inproceedings{PallKnoGha12,
  cat =		 {np},
  title =	 {A nonparametric variable clustering model},
  author =	 {Konstantina Palla and David A. Knowles and Zoubin
                  Ghahramani},
  abstract =	 {Factor analysis models effectively summarise the
                  covariance structure of high dimensional data, but
                  the solutions are typically hard to interpret.  This
                  motivates attempting to find a disjoint partition,
                  i.e. a simple clustering, of observed variables into
                  highly correlated subsets. We introduce a Bayesian
                  non-parametric approach to this problem, and
                  demonstrate advantages over heuristic methods
                  proposed to date. Our Dirichlet process variable
                  clustering (DPVC) model can discover block-diagonal
                  covariance structures in data. We evaluate our
                  method on both synthetic and gene expression
                  analysis problems.  },
  url =
                  {http://books.nips.cc/papers/files/nips25/NIPS2012_1354.pdf},
  booktitle =	 nips26,
  year =	 2012,
  address =	 {Lake Tahoe, California, USA},
  month =	 {December},
  pages =	 {1--9}
}

@inproceedings{ParKilRojetal18,
  cat =		 {causality},
  title =	 {Learning Independent Causal Mechanisms},
  author =	 {Giambattista Parascandolo and Niki Kilbertus and
                  Mateo Rojas-Carulla and Bernhard Sch\"{o}lkopf},
  booktitle =	 icml35,
  year =	 2018,
  month =	 {July},
  address =	 {Stockholm Sweden},
  url =
                  {http://proceedings.mlr.press/v80/parascandolo18a.html},
  abstract =	 {Statistical learning relies upon data sampled from a
                  distribution, and we usually do not care what
                  actually generated it in the first place.  From the
                  point of view of causal modeling, the structure of
                  each distribution is induced by physical mechanisms
                  that give rise to dependences between
                  observables. Mechanisms, however, can be meaningful
                  autonomous modules of generative models that make
                  sense beyond a particular entailed data
                  distribution, lending themselves to transfer between
                  problems. We develop an algorithm to recover a set
                  of independent (inverse) mechanisms from a set of
                  transformed data points. The approach is
                  unsupervised and based on a set of experts that
                  compete for data generated by the mechanisms,
                  driving specialization. We analyze the proposed
                  method in a series of experiments on image
                  data. Each expert learns to map a subset of the
                  transformed data back to a reference
                  distribution. The learned mechanisms generalize to
                  novel domains. We discuss implications for transfer
                  learning and links to recent trends in generative
                  modeling.}
}

@inproceedings{ParRasPetDoy18,
  cat =		 {rl mcmc},
  title =	 {{PIPPS}: Flexible Model-Based Policy Search Robust
                  to the Curse of Chaos},
  author =	 {Paavo Parmas and Carl Edward Rasmussen and Jan
                  Peters and Kenji Doya},
  booktitle =	 icml35,
  year =	 2018,
  url =		 {.},
  abstract =	 {Previously, the exploding gradient problem has been
                  explained to be central in deep learning and
                  model-based reinforcement learning, because it
                  causes numerical issues and instability in
                  optimization. Our experiments in model-based
                  reinforcement learning imply that the problem is not
                  just a numerical issue, but it may be caused by a
                  fundamental chaos-like nature of long chains of
                  nonlinear computations. Not only do the magnitudes
                  of the gradients become large, the direction of the
                  gradients becomes essentially random. We show that
                  reparameterization gradients suffer from the
                  problem, while likelihood ratio gradients are
                  robust. Using our insights, we develop a model-based
                  policy search framework, Probabilistic Inference for
                  Particle-Based Policy Search (PIPPS), which is
                  easily extensible, and allows for almost arbitrary
                  models and policies, while simultaneously matching
                  the performance of previous data-efficient learning
                  algorithms. Finally, we invent the total propagation
                  algorithm, which efficiently computes a union over
                  all pathwise derivative depths during a single
                  backwards pass, automatically giving greater weight
                  to estimators with lower variance, sometimes
                  improving over reparameterization gradients by
                  10<sup>6</sup> times.}
}

@inproceedings{PehLanVeretal20,
  cat =		 {gm deep},
  title =	 {Einsum Networks: Fast and Scalable Learning of
                  Tractable Probabilistic Circuits},
  author =	 {Robert Peharz and Steven Lang and Antonio Vergari
                  and Karl Stelzner and Alejandro Molina and Martin
                  Trapp and Guy Van den Broeck and Kristian Kersting
                  and Zoubin Ghahramani},
  booktitle =	 icml37,
  url =		 {https://arxiv.org/abs/2004.06231},
  year =	 2020,
  month =	 {July},
  address =	 {Online},
  abstract =	 {Probabilistic circuits (PCs) are a promising avenue
                  for probabilistic modeling, as they permit a wide
                  range of exact and efficient inference routines.
                  Recent ``deep-learning-style'' implementations of
                  PCs strive for a better scalability, but are still
                  difficult to train on real-world data, due to their
                  sparsely connected computational graphs.  In this
                  paper, we propose Einsum Networks (EiNets), a novel
                  implementation design for PCs, improving prior art
                  in several regards.  At their core, EiNets combine a
                  large number of arithmetic operations in a single
                  monolithic einsum-operation, leading to speedups and
                  memory savings of up to two orders of magnitude, in
                  comparison to previous implementations.  As an
                  algorithmic contribution, we show that the
                  implementation of Expectation-Maximization (EM) can
                  be simplified for PCs, by leveraging automatic
                  differentiation.  Furthermore, we demonstrate that
                  EiNets scale well to datasets which were previously
                  out of reach, such as SVHN and CelebA, and that they
                  can be used as faithful generative image models.}
}

@incollection{PerGhaPon07,
  cat =		 {gm},
  month =	 {September},
  author =	 {F. P\'{e}rez-Cruz and Zoubin Ghahramani and
                  M. Pontil},
  note =	 {Chapter 12.},
  booktitle =	 {Predicting Structured Data},
  editor =	 {G.~H.~Bakir and T.~Hofmann and B.~Sch\"olkopf and
                  A.~J.~Smola and B.~Taskar and S.~V.~N.~Vishwanathan},
  address =	 {Cambridge, MA, USA},
  title =	 {Conditional graphical models},
  publisher =	 mit,
  year =	 2007,
  pages =	 {265--282},
  url =		 {.},
  abstract =	 {In this chapter we propose a modification of
                  CRF-like algorithms that allows for solving
                  large-scale structured classification problems. Our
                  approach consists in upper bounding the CRF
                  functional in order to decompose its training into
                  independent optimisation problems per
                  clique. Furthermore we show that each sub-problem
                  corresponds to solving a multiclass learning task in
                  each clique, which enlarges the applicability of
                  these tools for large-scale structural learning
                  problems.  Before presenting the Conditional
                  Graphical Model (CGM), as we refer to this
                  procedure, we review the family of CRF
                  algorithms. We concentrate on the best known
                  procedures and standard generalisations of CRFs. The
                  ob jective of this introduction is analysing from
                  the same viewpoint the proposed solutions in the
                  literature to tackle this problem, which allows
                  comparing their different features.  We complete the
                  chapter with a case study, in which we show the
                  possibility to work with large-scale problems using
                  CGM and that the obtained performance is comparable
                  to the result with CRF-like algorithms.}
}

@inproceedings{PerKugSch22,
  cat =		 {causal},
  title =	 {Causal Discovery in Heterogeneous Environments Under
                  the Sparse Mechanism Shift Hypothesis},
  author =	 {Perry, R. and von K{\"u}gelgen*, J. and
                  Sch{\"o}lkopf*, B.},
  booktitle =	 nips35,
  publisher =	 {Curran Associates, Inc.},
  year =	 2022,
  note =	 {*shared last author},
  url =		 {https://arxiv.org/abs/2206.02013},
  abstract =	 {Machine learning approaches commonly rely on the
                  assumption of independent and identically
                  distributed (i.i.d.) data. In reality, however, this
                  assumption is almost always violated due to
                  distribution shifts between environments. Although
                  valuable learning signals can be provided by
                  heterogeneous data from changing distributions, it
                  is also known that learning under arbitrary
                  (adversarial) changes is impossible. Causality
                  provides a useful framework for modeling
                  distribution shifts, since causal models encode both
                  observational and interventional distributions. In
                  this work, we explore the sparse mechanism shift
                  hypothesis, which posits that distribution shifts
                  occur due to a small number of changing causal
                  conditionals. Motivated by this idea, we apply it to
                  learning causal structure from heterogeneous
                  environments, where i.i.d. data only allows for
                  learning an equivalence class of graphs without
                  restrictive assumptions. We propose the Mechanism
                  Shift Score (MSS), a score-based approach amenable
                  to various empirical estimators, which provably
                  identifies the entire causal structure with high
                  probability if the sparse mechanism shift hypothesis
                  holds. Empirically, we verify behavior predicted by
                  the theory and compare multiple estimators and score
                  functions to identify the best approaches in
                  practice. Compared to other methods, we show how MSS
                  bridges a gap by both being nonparametric as well as
                  explicitly leveraging sparse changes.},
}

@inproceedings{PetYuCunetal11,
  cat =		 {time neuro},
  booktitle =	 nips25,
  title =	 {Dynamical Segmentation of single trials from
                  population neural data},
  author =	 {B. Petreska and B. M. Yu and J. P. Cunningham and
                  G. Santhanam and S. I. Ryu and K. V. Shenoy and
                  M. Sahani},
  year =	 2011,
  address =	 {Granada, Spain},
  month =	 {December},
  pages =	 {1--8},
  url =		 {.},
  abstract =	 {Simultaneous recordings of many neurons embedded
                  within a recurrently-connected cortical network may
                  provide concurrent views into the dynamical
                  processes of that network, and thus its
                  computational function. In principle, these dynamics
                  might be identified by purely unsupervised,
                  statistical means. Here, we show that a Hidden
                  Switching Linear Dynamical Systems (HSLDS) model -
                  in which multiple linear dynamical laws approximate
                  and nonlinear and potentially non-stationary
                  dynamical process - is able to distinguish dynamical
                  regimes within single-trial motor cortical activity
                  associated with the preparation and initiation of
                  hand movements. The regimes are identified without
                  reference to behavioural or experimental epochs, but
                  nonetheless transitions between them correlate
                  strongly with external events whose timing may vary
                  from trial to trial. The HSLDS model also performs
                  better than recent comparable models in predicting
                  the firing rate of an isolated neuron based on the
                  firing rates of others, suggesting that it captures
                  more of the "Shared variance" of the data. Thus, the
                  method is able to trace the dynamical processes
                  underlying the coordinated evolution of network
                  activity in a way that appears to reflect its
                  computational role.}
}

@article{PfiHerRas06,
  annote =	 {Winner of the 2006 Best Paper Award for the
                  journal.},
  author =	 {Tobias Pfingsten and Daniel Herrmann and Carl Edward
                  Rasmussen},
  journal =	 {IEEE Transactions on Semiconductor Manufacturing},
  number =	 4,
  pages =	 {475--486},
  title =	 {Model-based design analysis and yield optimization},
  url =		 {http://dx.doi.org/10.1109/TSM.2006.883589},
  doi =		 {10.1109/TSM.2006.883589},
  year =	 2006,
  volume =	 19,
  abstract =	 {Fluctuations are inherent to any fabrication
                  process. Integrated circuits and
                  microelectromechanical systems are particularly
                  affected by these variations, and due to
                  high-quality requirements the effect on the devices'
                  perform ance has to be understood quantitatively. In
                  recent years, it has become possible to model the
                  performance of such complex systems on the basis of
                  design specifications, and model-based sensitivity
                  analysis has made its way into industrial
                  engineering. We show how an efficient Bayesian
                  approach, using a Gaussian process prior, can
                  replace the commonly used brute-force Monte Carlo
                  scheme, making it possible to apply the analysis to
                  computationally costly models. We introduce a number
                  of global, statistically justified sensitivity
                  measures for design analysis and optimization. Two
                  models of integrated systems serve us as case
                  studies to introduce the analysis and to assess its
                  convergence properties. We show that the Bayesian
                  Monte Carlo scheme can save costly simulation runs
                  and can ensure a reliable accuracy of the analysis.}
}

@inproceedings{PhiSerHutetal22,
  cat =		 {deep np},
  title =	 {Spectral Diffusion Processes},
  author =	 {Phillips, Angus and Seror, Thomas and Hutchinson,
                  Michael and De Bortoli, Valentin and Doucet,Arnaud
                  and Mathieu, Emile},
  booktitle =	 {{NeurIPS} workshop on {S}core-Based {M}ethods},
  url =		 {https://arxiv.org/pdf/2209.14125},
  year =	 2022,
  abstract =	 {Score-based generative modelling (SGM) has proven to
                  be a very effective method for modelling densities
                  on finite-dimensional spaces. In this work we
                  propose to extend this methodology to learn
                  generative models over functional spaces. To do so,
                  we represent functional data in spectral space to
                  dissociate the stochastic part of the processes from
                  their space-time part. Using dimensionality
                  reduction techniques we then sample from their
                  stochastic component using finite dimensional
                  SGM. We demonstrate our method's effectiveness for
                  modelling various multimodal datasets.}
}

@inproceedings{PinAkrOsaetal18,
  cat =		 {rl},
  title =	 {Sample and Feedback Efficient Hierarchical
                  Reinforcement Learning from Human Preferences},
  author =	 {Robert Pinsler and Riad Akrour and Takayuki Osa and
                  Jan Peters and Gerhard Neumann},
  booktitle =	 icra,
  year =	 2018,
  month =	 {May},
  address =	 {Brisbane, Australia},
  url =
                  {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460907},
  abstract =	 {While reinforcement learning has led to promising
                  results in robotics, defining an informative reward
                  function is challenging. Prior work considered
                  including the human in the loop to jointly learn the
                  reward function and the optimal policy. Generating
                  samples from a physical robot and requesting human
                  feedback are both taxing efforts for which
                  efficiency is critical. We propose to learn reward
                  functions from both the robot and the human
                  perspectives to improve on both efficiency
                  metrics. Learning a reward function from the human
                  perspective increases feedback efficiency by
                  assuming that humans rank trajectories according to
                  a low-dimensional outcome space. Learning a reward
                  function from the robot perspective circumvents the
                  need for a dynamics model while retaining the sample
                  efficiency of model-based approaches. We provide an
                  algorithm that incorporates bi-perspective reward
                  learning into a general hierarchical reinforcement
                  learning framework and demonstrate the merits of our
                  approach on a toy task and a simulated robot
                  grasping task.}
}

@inproceedings{PinGorNaletal19,
  cat =		 {active},
  title =	 {Bayesian batch active learning as sparse subset
                  approximation},
  author =	 {Robert Pinsler and Jonathan Gordon and Eric
                  Nalisnick and Jose Miguel Hern\'{a}ndez-Lobato},
  booktitle =	 nips33,
  year =	 2019,
  url =
                  {https://papers.nips.cc/paper/8865-bayesian-batch-active-learning-as-sparse-subset-approximation},
  abstract =	 {Leveraging the wealth of unlabeled data produced in
                  recent years provides great potential for improving
                  supervised models.  When the cost of acquiring
                  labels is high, probabilistic active learning
                  methods can be used to greedily select the most
                  informative data points to be labeled. However, for
                  many large-scale problems standard greedy procedures
                  become computationally infeasible and suffer from
                  negligible model change. In this paper, we introduce
                  a novel Bayesian batch active learning approach that
                  mitigates these issues. Our approach is motivated by
                  approximating the complete data posterior of the
                  model parameters. While naive batch construction
                  methods result in correlated queries, our algorithm
                  produces diverse batches that enable efficient
                  active learning at scale. We derive interpretable
                  closed-form solutions akin to existing active
                  learning procedures for linear models, and
                  generalize to arbitrary models using random
                  projections. We demonstrate the benefits of our
                  approach on several large-scale regression and
                  classification tasks.}
}

@inproceedings{PinKarKupetal19,
  cat =		 {rl},
  title =	 {Factored Contextual Policy Search with {B}ayesian
                  Optimization},
  author =	 {Robert Pinsler and Peter Karkus and Andras Kupcsik
                  and David Hsu and Wee Sun Lee},
  booktitle =	 icra,
  year =	 2019,
  month =	 {May},
  address =	 {Montreal, Canada},
  url =		 {.},
  abstract =	 {Scarce data is a major challenge to scaling robot
                  learning to truly complex tasks, as we need to
                  generalize locally learned policies over different
                  task contexts. Contextual policy search offers
                  data-efficient learning and generalization by
                  explicitly conditioning the policy on a parametric
                  context space. In this paper, we further structure
                  the contextual policy representation. We propose to
                  factor contexts into two components: target contexts
                  that describe the task objectives, e.g. target
                  position for throwing a ball; and environment
                  contexts that characterize the environment,
                  e.g. initial position or mass of the ball. Our key
                  observation is that experience can be directly
                  generalized over target contexts. We show that this
                  can be easily exploited in contextual policy search
                  algorithms. In particular, we apply factorization to
                  a Bayesian optimization approach to contextual
                  policy search both in sampling-based and active
                  learning settings. Our simulation results show
                  faster learning and better generalization in various
                  robotic domains. See our supplementary video:
                  https://youtu.be/MNTbBAOufDY.}
}

@inproceedings{PocGhaSch12,
  author =	 {Barnabas Poczos and Zoubin Ghahramani and Jeff
                  Schneider},
  year =	 2012,
  title =	 {Copula-based Kernel Dependency Measures},
  booktitle =	 icml29,
  abstract =	 {The paper presents a new copula based method for
                  measuring dependence between random variables. Our
                  approach extends the Maximum Mean Discrepancy to the
                  copula of the joint distribution. We prove that this
                  approach has several advantageous
                  properties. Similarly to Shannon mutual information,
                  the proposed dependence measure is invariant to any
                  strictly increasing transformation of the marginal
                  variables. This is important in many applications,
                  for example in feature selection. The estimator is
                  consistent, robust to outliers, and uses rank
                  statistics only. We derive upper bounds on the
                  convergence rate and propose independence tests
                  too. We illustrate the theoretical contributions
                  through a series of experiments in feature selection
                  and low-dimensional embedding of distributions.},
  url =		 {.}
}

@inproceedings{PonGuDalLev18,
  cat =		 {deep rl},
  title =	 {Temporal Difference Models: Model-Free Deep RL for
                  Model-Based Control},
  author =	 {Vitchyr Pong and Shixiang Gu and Murtaza Dalal and
                  Sergey Levine},
  booktitle =	 iclr06,
  year =	 2018,
  month =	 {Apr},
  address =	 {Vancouver CANADA},
  url =
                  {https://openreview.net/forum?id=Skw0n-W0Z&noteId=Skw0n-W0Z},
  abstract =	 {Model-free reinforcement learning (RL) has been
                  proven to be a powerful, general tool for learning
                  complex behaviors. However, its sample efficiency is
                  often impractically large for solving challenging
                  real-world problems, even for off-policy algorithms
                  such as Q-learning. A limiting factor in classic
                  model-free RL is that the learning signal consists
                  only of scalar rewards, ignoring much of the rich
                  information contained in state transition
                  tuples. Model-based RL uses this information, by
                  training a predictive model, but often does not
                  achieve the same asymptotic performance as
                  model-free RL due to model bias. We introduce
                  temporal difference models (TDMs), a family of
                  goal-conditioned value functions that can be trained
                  with model-free learning and used for model-based
                  control. TDMs combine the benefits of model-free and
                  model-based RL: they leverage the rich information
                  in state transitions to learn very efficiently,
                  while still attaining asymptotic performance that
                  exceeds that of direct model-based RL methods. Our
                  experimental results show that, on a range of
                  continuous control tasks, TDMs provide a substantial
                  improvement in efficiency compared to
                  state-of-the-art model-based and model-free
                  methods.}
}

@inproceedings{QiMinPic04a,
  author =	 {Qi, Yuan (Alan) and Minka, Thomas P. and Picard,
                  Rosalind W. and Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {approx},
  editor =	 {Brodley, Carla E.},
  publisher =	 acm,
  series =	 {ACM International Conference Proceeding Series},
  title =	 {Predictive automatic relevance determination by
                  expectation propagation},
  url =		 {.},
  volume =	 69,
  year =	 2004,
  abstract =	 {In many real-world classification problems the input
                  contains a large number of potentially irrelevant
                  features. This paper proposes a new Bayesian
                  framework for determining the relevance of input
                  features. This approach extends one of the most
                  successful Bayesian methods for feature selection
                  and sparse learning, known as Automatic Relevance
                  Determination (ARD). ARD finds the relevance of
                  features by optimizing the model marginal
                  likelihood, also known as the evidence. We show that
                  this can lead to overfitting. To address this
                  problem, we propose Predictive ARD based on
                  estimating the predictive performance of the
                  classifier. While the actual leave-one-out
                  predictive performance is generally very costly to
                  compute, the expectation propagation (EP) algorithm
                  proposed by Minka provides an estimate of this
                  predictive performance as a side-effect of its
                  iterations. We exploit this in our algorithm to do
                  feature selection, and to select data points in a
                  sparse Bayesian kernel classifier. Moreover, we
                  provide two other improvements to previous
                  algorithms, by replacing Laplace's approximation
                  with the generally more accurate EP, and by
                  incorporating the fast optimization algorithm
                  proposed by Faul and Tipping. Our experiments show
                  that our method based on the EP estimate of
                  predictive performance is more accurate on test data
                  than relevance determination by optimizing the
                  evidence.}
}

@inproceedings{QuaCheLam12,
  cat =		 {network},
  author =	 {Novi Quadrianto and Chao Chen and Christoph Lampert},
  title =	 {The Most Persistent Soft-Clique in a Set of Sampled
                  Graphs},
  booktitle =	 icml29,
  year =	 2012,
  month =	 {June},
  address =	 {Edinburgh, Scotland},
  url =		 {.},
  abstract =	 {When searching for characteristic subpatterns in
                  potentially noisy graph data, it appears
                  self-evident that having multiple observations would
                  be better than having just one. However, it turns
                  out that the inconsistencies introduced when
                  different graph instances have different edge sets
                  pose a serious challenge.  In this work we address
                  this challenge for the problem of finding maximum
                  weighted cliques.  We introduce the concept of most
                  persistent soft-clique.  This is subset of vertices,
                  that 1) is almost fully or at least densely
                  connected, 2) occurs in all or almost all graph
                  instances, and 3) has the maximum weight.  We
                  present a measure of clique-ness, that essentially
                  counts the number of edge missing to make a subset
                  of vertices into a clique. With this measure, we
                  show that the problem of finding the most persistent
                  soft-clique problem can be cast either as: a) a
                  max-min two person game optimization problem, or b)
                  a min-min soft margin optimization problem.  Both
                  formulations lead to the same solution when using a
                  partial Lagrangian method to solve the optimization
                  problems.  By experiments on synthetic data and on
                  real social network data we show that the proposed
                  method is able to reliably find soft cliques in
                  graph data, even if that is distorted by random
                  noise or unreliable observations.}
}

@inproceedings{QuaShaKnoGha13,
  cat =		 {np},
  author =	 {Novi Quadrianto and Viktoriia Sharmanska and David
                  A. Knowles and Zoubin Ghahramani},
  title =	 {The Supervised IBP: Neighbourhood Preserving
                  Infinite Latent Feature Models},
  booktitle =	 {29th Conference on Uncertainty in Artificial
                  Intelligence},
  year =	 2013,
  month =	 {July},
  address =	 {Bellevue, USA},
  url =		 {.},
  abstract =	 {We propose a probabilistic model to infer supervised
                  latent variables in the Hamming space from observed
                  data.  Our model allows simultaneous inference of
                  the number of binary latent variables, and their
                  values.  The latent variables preserve neighbourhood
                  structure of the data in a sense that objects in the
                  same semantic concept have similar latent values,
                  and objects in different concepts have dissimilar
                  latent values. We formulate the supervised infinite
                  latent variable problem based on an intuitive
                  principle of pulling objects together if they are of
                  the same type, and pushing them apart if they are
                  not. We then combine this principle with a flexible
                  Indian Buffet Process prior on the latent variables.
                  We show that the inferred supervised latent
                  variables can be directly used to perform a nearest
                  neighbour search for the purpose of retrieval.  We
                  introduce a new application of dynamically extending
                  hash codes, and show how to effectively couple the
                  structure of the hash codes with continuously
                  growing structure of the neighbourhood preserving
                  infinite latent feature space.}
}

@inproceedings{QuiGirLarRas03,
  cat =		 {gp time},
  author =	 {Joaquin Qui{\~n}onero-Candela and Agathe Girard and
                  Jan Larsen and Carl Edward Rasmussen},
  title =	 {Propagation of Uncertainty in {B}ayesian Kernel
                  Models - Application to Multiple-Step Ahead
                  Forecasting},
  year =	 2003,
  month =	 {April},
  volume =	 2,
  pages =	 {701--704},
  journal =	 {IEEE International Conference on Acoustics, Speech
                  and Signal Processing},
  abstract =	 {The object of Bayesian modelling is the predictive
                  distribution, which in a forecasting scenario
                  enables improved estimates of forecasted values and
                  their uncertainties. In this paper we focus on
                  reliably estimating the predictive mean and variance
                  of forecasted values using Bayesian kernel based
                  models such as the Gaussian Process and the
                  Relevance Vector Machine. We derive novel analytic
                  expressions for the predictive mean and variance for
                  Gaussian kernel shapes under the assumption of a
                  Gaussian input distribution in the static case, and
                  of a recursive Gaussian predictive density in
                  iterative forecasting. The capability of the method
                  is demonstrated for forecasting of time-series and
                  compared to approximate methods.},
  url =		 {.},
  booktitle =	 {ICASSP 2003},
  location =	 {Hong Kong}
}

@inproceedings{QuiGirLarRas03b,
  author =	 {Joaquin Qui{\~n}onero-Candela and Agathe Girard and
                  Jan Larsen and Carl Edward Rasmussen},
  title =	 {Propagation of Uncertainty in {B}ayesian Kernel
                  Models - Application to Multiple-Step Ahead
                  Forecasting},
  year =	 2003,
  publisher =	 {IEEE Press},
  annote =	 {Electronic version of <a
                  href="/pub/#QuiGirLarRas03">Qui{\~n}onero-Candela,
                  Girard, Larsen and Rasmussen, 2003</a> which should
                  have been presented at ICASSP 03, but was cancelled
                  due to bird flu epidemic.},
  journal =	 {Proceedings of 2003 IEEE International Workshop on
                  Neural Networks for Signal Processing},
  editor =	 {C.~Molina and T.~Adali and J.~Larsen and M.~Van
                  Hulle and S.~C.~Douglas and J.~Rouat},
  address =	 {Piscataway, New Jersey},
  url =		 {/pub/#QuiGirLarRas03},
  abstract =	 {The object of Bayesian modelling is the predictive
                  distribution, which in a forecasting scenario
                  enables improved estimates of forecasted values and
                  their uncertainties. In this paper we focus on
                  reliably estimating the predictive mean and variance
                  of forecasted values using Bayesian kernel based
                  models such as the Gaussian Process and the
                  Relevance Vector Machine. We derive novel analytic
                  expressions for the predictive mean and variance for
                  Gaussian kernel shapes under the assumption of a
                  Gaussian input distribution in the static case, and
                  of a recursive Gaussian predictive density in
                  iterative forecasting. The capability of the method
                  is demonstrated for forecasting of time-series and
                  compared to approximate methods.},
  booktitle =	 {NNSP 2003},
  location =	 {Toulouse}
}

@techreport{QuiGirRas03,
  cat =		 {gp time},
  title =	 {Prediction at an uncertain input for {G}aussian
                  processes and {Relevance Vector Machines}
                  Application to multiple-step ahead time-series
                  prediction},
  author =	 {Joaquin Qui{\~n}onero-Candela and Agathe Girard and
                  Carl Edward Rasmussen},
  institution =	 {Instititute for Mathemetical Modelling, DTU},
  number =	 {IMM-2003-18},
  year =	 2003,
  url =		 {.},
  annote =	 {<a
                  href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=2800">techreport</a>}
}

@article{QuiRas05,
  cat =		 {gp approx},
  author =	 {Joaquin Qui{\~n}onero-Candela and Carl Edward
                  Rasmussen},
  title =	 {A Unifying View of Sparse Approximate {G}aussian
                  Process Regression},
  journal =	 jmlr,
  year =	 2005,
  pages =	 {1939--1959},
  volume =	 6,
  url =
                  {http://jmlr.csail.mit.edu/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf},
  abstract =	 {We provide a new unifying view, including all
                  existing proper probabilistic sparse approximations
                  for Gaussian process regression. Our approach relies
                  on expressing the effective prior which the methods
                  are using. This allows new insights to be gained,
                  and highlights the relationship between existing
                  methods. It also allows for a clear theoretically
                  justified ranking of the closeness of the known
                  approximations to the corresponding full
                  GPs. Finally we point directly to designs of new
                  better sparse approximations, combining the best of
                  the existing strategies, within attractive
                  computational constraints.}
}

@incollection{QuiRas05b,
  cat =		 {gp},
  author =	 {Joaquin Qui{\~n}onero-Candela and Carl Edward
                  Rasmussen},
  booktitle =	 {Switching and Learning in Feedback Systems},
  title =	 {Analysis of Some Methods for Reduced Rank {G}aussian
                  Process Regression},
  year =	 2005,
  publisher =	 {Springer},
  pages =	 {98--127},
  editor =	 {R.~Murray-Smith and R.~Shorten},
  address =	 {Berlin, Heidelberg},
  abstract =	 {While there is strong motivation for using Gaussian
                  Processes (GPs) due to their excellent performance
                  in regression and classification problems, their
                  computational complexity makes them impractical when
                  the size of the training set exceeds a few thousand
                  cases. This has motivated the recent proliferation
                  of a number of cost-effective approximations to GPs,
                  both for classification and for regression.  In this
                  paper we analyze one popular approximation to GPs
                  for regression: the reduced rank
                  approximation. While generally GPs are equivalent to
                  infinite linear models, we show that Reduced Rank
                  Gaussian Processes (RRGPs) are equivalent to finite
                  sparse linear models. We also introduce the concept
                  of degenerate GPs and show that they correspond to
                  inappropriate priors. We show how to modify the RRGP
                  to prevent it from being degenerate at test
                  time. Training RRGPs consists both in learning the
                  covariance function hyperparameters and the support
                  set. We propose a method for learning
                  hyperparameters for a given support set. We also
                  review the Sparse Greedy GP (SGGP) approximation
                  (Somla and Bartlett, 2001), which is a way of
                  learning the support set for given hyperparameters
                  based on approximating the posterior. We propose an
                  alternative method to the SGGP that has better
                  generalization capabilities. Finally we make
                  experiments to compare the different ways of
                  training a RRGP. We provide some Matlab code for
                  learning RRGPs.},
  url =		 {.}
}

@inproceedings{QuiRasSinetal06,
  author =	 {Joaquin Qui{\~n}onero-Candela and Carl Edward
                  Rasmussen and Fabian Sinz and Olivier Bousquet and
                  Bernhard Sch{\"o}lkopf},
  title =	 {Evaluating Predictive Uncertainty Challenge},
  year =	 2006,
  publisher =	 {Springer},
  pages =	 {1--27},
  month =	 04,
  volume =	 3944,
  series =	 lncs,
  editor =	 {J.~Qui{\~n}onero-Candela and I.~Dagan and B.~Magnini
                  and F.~d'Alch{\'e}{-}Buc},
  address =	 {Berlin, Germany},
  abstract =	 {This Chapter presents the PASCAL1 Evaluating
                  Predictive Uncertainty Challenge, introduces the
                  contributed Chapters by the participants who
                  obtained outstanding results, and provides a
                  discussion with some lessons to be learnt. The
                  Challenge was set up to evaluate the ability of
                  Machine Learning algorithms to provide good
                  "probabilistic predictions", rather than just the
                  usual "point predictions" with no measure of
                  uncertainty, in regression and classification
                  problems. Participants had to compete on a number of
                  regression and classification tasks, and were
                  evaluated by both traditional losses that only take
                  into account point predictions and losses we
                  proposed that evaluate the quality of the
                  probabilistic predictions.},
  booktitle =	 {Machine Learning Challenges. Evaluating predictive
                  uncertainty, visual object classification and
                  recognising tectual entailment. First PASCAL Machine
                  Learning Challenges Workshop},
  location =	 {Southampton, United Kingdom},
  url =		 {.},
  doi =		 {10.1007/11736790_1}
}

@incollection{QuiRasWil07,
  cat =		 {gp},
  booktitle =	 {Large-Scale Kernel Machines},
  author =	 {Joaquin Qui{\~n}onero-Candela and Carl Edward
                  Rasmussen and Christopher K.~I.~Williams},
  title =	 {Approximation Methods for {G}aussian Process
                  Regression},
  year =	 2007,
  series =	 {Neural Information Processing},
  publisher =	 mit,
  pages =	 {203--223},
  month =	 {September},
  editor =	 {L.~Bottou and O.~Chapelle and D.~DeCoste and
                  J.~Weston},
  address =	 {Cambridge, MA, USA},
  abstract =	 {A wealth of computationally efficient approximation
                  methods for Gaussian process regression have been
                  recently proposed. We give a unifying overview of
                  sparse approximations, following <a
                  href="/pub/#QuiRas05">Qui{\~n}onero-Candela and
                  Rasmussen (2005)</a>, and a brief review of
                  approximate matrix-vector multiplication methods.},
  URL =		 {.},
  annote =	 {<a
                  href="http://mitpress.mit.edu/9780262026253">book</a>}
}

@article{RabBlei14,
  author =	 {Rabinovich, Maxim and Blei, David M.},
  title =	 {{T}he {I}nverse {R}egression {T}opic Model},
  booktitle =	 icml31,
  year =	 2014,
  pages =	 {199-207},
  abstract =	 {Recently, multinomial inverse regression (MNIR) has
                  been proposed as a new model of annotated text based
                  on the influence of metadata and response variables
                  on the distribution of words in a document. While
                  effective, MNIR has no way to exploit structure in
                  the corpus to improve its predictions or facilitate
                  exploratory data analysis. On the other hand,
                  traditional probabilistic topic models (like latent
                  Dirichlet allocation) capture natural heterogeneity
                  in a collection but do not account for external
                  variables. In this paper, we introduce the inverse
                  regression topic model (IRTM), a mixed-membership
                  extension of MNIR that combines the strengths of
                  both methodologies. We present two inference
                  algorithms for the IRTM: an efficient batch
                  estimation algorithm and an online variant, which is
                  suitable for large corpora.  We apply these methods
                  to a corpus of 73K Congressional press releases and
                  another of 150K Yelp reviews, demonstrating that the
                  IRTM outperforms both MNIR and supervised topic
                  models on the prediction task.  Further, we give
                  examples showing that the IRTM enables systematic
                  discovery of in-topic lexical variation, which is
                  not possible with previous supervised topic models.},
  url =		 {.}
}

@article{RanAngGha04a,
  author =	 {Rangel, Claudia and Angus, John and Ghahramani,
                  Zoubin and Lioumi, Maria and Sotheran, Elizabeth and
                  Gaiba, Alessia and Wild, David L. and Falciani,
                  Francesco},
  cat =		 {bioinf},
  journal =	 {Bioinformatics},
  number =	 9,
  pages =	 {1361-1372},
  title =	 {Modeling T-cell activation using gene expression
                  profiling and state-space models},
  url =		 {.},
  volume =	 20,
  year =	 2004,
  abstract =	 {Motivation: We have used state-space models to
                  reverse engineer transcriptional networks from
                  highly replicated gene expression profiling time
                  series data obtained from a well-established model
                  of T-cell activation. State space models are a class
                  of dynamic Bayesian networks that assume that the
                  observed measurements depend on some hidden state
                  variables that evolve according to Markovian
                  dynamics. These hidden variables can capture effects
                  that cannot be measured in a gene expression
                  profiling experiment, e.g. genes that have not been
                  included in the microarray, levels of regulatory
                  proteins, the effects of messenger RNA and protein
                  degradation, etc.  Results: Bootstrap confidence
                  intervals are developed for parameters representing
                  `gene--gene' interactions over time. Our models
                  represent the dynamics of T-cell activation and
                  provide a methodology for the development of
                  rational and experimentally testable hypotheses.
                  Availability: Supplementary data and Matlab computer
                  source code will be made available on the web at the
                  URL given below.  Supplementary information: .}
}

@inproceedings{Ras00,
  cat =		 {np},
  author =	 {Carl Edward Rasmussen},
  title =	 {The Infinite {G}aussian Mixture Model},
  booktitle =	 nips12,
  year =	 2000,
  pages =	 {554--560},
  url =		 {.},
  editors =	 {Sara A. Solla, Todd K. Leen and Klaus-Robert
                  M{\"u}ller},
  publisher =	 mit,
  abstract =	 {In a Bayesian mixture model it is not necessary a
                  priori to limit the number of components to be
                  finite. In this paper an infinite Gaussian mixture
                  model is presented which neatly sidesteps the
                  difficult problem of finding the "right" number of
                  mixture components. Inference in the model is done
                  using an efficient parameter-free Markov Chain that
                  relies entirely on Gibbs sampling.}
}

@incollection{Ras03,
  cat =		 {gp mcmc},
  author =	 {Carl Edward Rasmussen},
  title =	 {{G}aussian Processes to Speed up {H}ybrid {M}onte
                  {C}arlo for Expensive {B}ayesian Integrals},
  booktitle =	 {Bayesian Statistics 7},
  pages =	 {651--659},
  url =		 {.},
  publisher =	 oup,
  year =	 2003,
  editors =	 {J.~M.~Bernardo and M.~J.~Bayarri and J.~O.~Berger
                  and A.~P.~Dawid and D.~Heckerman and A.~F.~M.~Smith
                  and M.~West},
  abstract =	 {Hybrid Monte Carlo (HMC) is often the method of
                  choice for computing Bayesian integrals that are not
                  analytically tractable. However the success of this
                  method may require a very large number of
                  evaluations of the (un-normalized) posterior and its
                  partial derivatives. In situations where the
                  posterior is computationally costly to evaluate,
                  this may lead to an unacceptable computational load
                  for HMC.  I propose to use a Gaussian Process model
                  of the (log of the) posterior for most of the
                  computations required by HMC. Within this scheme
                  only occasional evaluation of the actual posterior
                  is required to guarantee that the samples generated
                  have exactly the desired distribution, even if the
                  GP model is somewhat inaccurate. The method is
                  demonstrated on a 10 dimensional problem, where 200
                  evaluations suffice for the generation of 100
                  roughly independent points from the posterior. Thus,
                  the proposed scheme allows Bayesian treatment of
                  models with posteriors that are computationally
                  demanding, such as models involving computer
                  simulation.}
}

@incollection{Ras04,
  cat =		 {gp review},
  booktitle =	 {Advanced Lectures on Machine Learning: ML Summer
                  Schools 2003, Canberra, Australia, February 2 - 14,
                  2003, T{\"u}bingen, Germany, August 4 - 16, 2003,
                  Revised Lectures},
  author =	 {Carl Edward Rasmussen},
  title =	 {Gaussian Processes in Machine Learning},
  year =	 2004,
  volume =	 3176,
  series =	 lncs,
  publisher =	 {Springer-Verlag},
  pages =	 {63--71},
  annote =	 {Copyright by Springer, <a
                  href="http://www.springerlink.com/content/lrh41y849xdh">springerlink</a>},
  editor =	 {Olivier Bousquet and Ulrike von Luxburg and Gunnar
                  R{\"a}tsch},
  address =	 {Heidelberg},
  url =		 {.},
  abstract =	 {We give a basic introduction to Gaussian Process
                  regression models. We focus on understanding the
                  role of the stochastic process and how it is used to
                  define a distribution over functions. We present the
                  simple equations for incorporating training data and
                  examine how to learn the hyperparameters using the
                  marginal likelihood. We explain the practical
                  advantages of Gaussian Process and end with
                  conclusions and a look at the current trends in GP
                  work.}
}

@inproceedings{Ras96,
  cat =		 {mcmc},
  author =	 {Carl Edward Rasmussen},
  title =	 {A practical {M}onte {C}arlo implementation of
                  {B}ayesian learning},
  booktitle =	 nips8,
  editors =	 {D.~S.~Touretzky and M.~C.~Mozer and M.~E.~Hasselmo},
  pages =	 {598--604},
  url =		 {.},
  publisher =	 mit,
  address =	 {Cambridge, MA., USA},
  year =	 1996,
  abstract =	 {A practical method for Bayesian training of
                  feed-forward neural networks using sophisticated
                  Monte Carlo methods is presented and evaluated. In
                  reasonably small amounts of computer time this
                  approach outperforms other state-of-the-art methods
                  on 5 datalimited tasks from real world domains.}
}

@phdthesis{Ras96b,
  cat =		 {gp},
  author =	 {Carl Edward Rasmussen},
  title =	 {Evaluation of {G}aussian Processes and other Methods
                  for non-linear Regression},
  year =	 1996,
  url =		 {.},
  school =	 {University of Toronto, Department of Computer
                  Science},
  address =	 {Toronto, CANADA},
  abstract =	 {This thesis develops two Bayesian learning methods
                  relying on Gaussian processes and a rigorous
                  statistical approach for evaluating such methods.
                  In these experimental designs the sources of
                  uncertainty in the estimated generalisation
                  performances due to both variation in training and
                  test sets are accounted for. The framework allows
                  for estimation of generalisation performance as well
                  as statistical tests of significance for pairwise
                  comparisons. Two experimental designs are
                  recommended and supported by the DELVE software
                  environment.<br> Two new non-parametric Bayesian
                  learning methods relying on Gaussian process priors
                  over functions are developed.  These priors are
                  controlled by hyperparameters which set the
                  characteristic length scale for each input
                  dimension. In the simplest method, these parameters
                  are fit from the data using optimization.  In the
                  second, fully Bayesian method, a Markov chain Monte
                  Carlo technique is used to integrate over the
                  hyperparameters. One advantage of these Gaussian
                  process methods is that the priors and
                  hyperparameters of the trained models are easy to
                  interpret.<br> The Gaussian process methods are
                  benchmarked against several other methods, on
                  regression tasks using both real data and data
                  generated from realistic simulations. The
                  experiments show that small datasets are unsuitable
                  for benchmarking purposes because the uncertainties
                  in performance measurements are large. A second set
                  of experiments provide strong evidence that the
                  bagging procedure is advantageous for the
                  Multivariate Adaptive Regression Splines (MARS)
                  method.<br> The simulated datasets have controlled
                  characteristics which make them useful for
                  understanding the relationship between properties of
                  the dataset and the performance of different
                  methods. The dependency of the performance on
                  available computation time is also investigated. It
                  is shown that a Bayesian approach to learning in
                  multi-layer perceptron neural networks achieves
                  better performance than the commonly used early
                  stopping procedure, even for reasonably short
                  amounts of computation time. The Gaussian process
                  methods are shown to consistently outperform the
                  more conventional methods.}
}

@proceedings{RasBueGieSch04,
  title =	 {Pattern Recognition: 26th {DAGM} Symposium},
  editor =	 {Carl Edward Rasmussen and Heinrich H.~B{\"u}lthoff
                  and Martin A.~Giese and Bernhard Sch{\"o}lkopf},
  year =	 2004,
  publisher =	 {Springer},
  pages =	 581,
  month =	 {August},
  series =	 lncs,
  volume =	 3175,
  address =	 {Berlin, Germany},
  location =	 {T{\"u}bingen, Germany},
  url =		 {http://dx.doi.org/10.1007/b99676},
  doi =		 {10.1007/b99676}
}

@article{RasCruGhaWil09,
  cat =		 {clust bioinf},
  author =	 {Carl Edward Rasmussen and Bernhard J.~de la Cruz and
                  Zoubin Ghahramani and David L.~Wild},
  title =	 {Modeling and Visualizing Uncertainty in Gene
                  Expression Clusters Using {D}irichlet Process
                  Mixtures},
  journal =	 tcbb,
  volume =	 6,
  number =	 4,
  issn =	 {1545-5963},
  pages =	 {615--628},
  year =	 2009,
  url =		 {http://dx.doi.org/10.1109/TCBB.2007.70269},
  abstract =	 {Although the use of clustering methods has rapidly
                  become one of the standard computational approaches
                  in the literature of microarray gene expression
                  data, little attention has been paid to uncertainty
                  in the results obtained. Dirichlet process mixture
                  (DPM) models provide a nonparametric Bayesian
                  alternative to the bootstrap approach to modeling
                  uncertainty in gene expression clustering. Most
                  previously published applications of Bayesian
                  model-based clustering methods have been to short
                  time series data. In this paper, we present a case
                  study of the application of nonparametric Bayesian
                  clustering methods to the clustering of
                  high-dimensional nontime series gene expression data
                  using full Gaussian covariances. We use the
                  probability that two genes belong to the same
                  cluster in a DPM model as a measure of the
                  similarity of these gene expression
                  profiles. Conversely, this probability can be used
                  to define a dissimilarity measure, which, for the
                  purposes of visualization, can be input to one of
                  the standard linkage algorithms used for
                  hierarchical clustering. Biologically plausible
                  results are obtained from the Rosetta compendium of
                  expression profiles which extend previously
                  published cluster analyses of this data.},
  doi =		 {10.1109/TCBB.2007.70269}
}

@inproceedings{RasDei08,
  cat =		 {rl},
  title =	 {Probabilistic Inference for Fast Learning in
                  Control},
  pages =	 {229--242},
  booktitle =	 {Recent Advances in Reinforcement Learning},
  publisher =	 {Springer-Verlag},
  year =	 2008,
  editor =	 {S. Girgin and M. Loth and R. Munos and P. Preux and
                  D. Ryabko},
  author =	 {Carl Edward Rasmussen and Marc Peter Deisenroth},
  volume =	 5323,
  series =	 lncs,
  type =	 {Lecture Notes in Artificial Intelligence},
  month =	 {November},
  address =	 {Villeneuve d'Ascq, France},
  url =		 {.},
  abstract =	 {We provide a novel framework for very fast
                  model-based reinforcement learning in continuous
                  state and action spaces. The framework requires
                  probabilistic models that explicitly characterize
                  their levels of confidence. Within this framework,
                  we use flexible, non-parametric models to describe
                  the world based on previously collected experience.
                  We demonstrate learning on the cart-pole problem in
                  a setting where we provide very limited prior
                  knowledge about the task. Learning progresses
                  rapidly, and a good policy is found after only a
                  hand-full of iterations.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/carl/ewrl08/">videos
                  and more</a>. <a
                  href="http://mlg.eng.cam.ac.uk/marc/talks/2008-07-03-EWRL-lille.pdf">slides</a>.}
}

@inproceedings{RasGha01,
  cat =		 {np review},
  author =	 {Carl Edward Rasmussen and Zoubin Ghahramani},
  title =	 {{O}ccam's Razor},
  booktitle =	 nips13,
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  editors =	 {T. Leen, T. G. Diettrich and V. Tresp},
  pages =	 {294--300},
  year =	 2001,
  month =	 {December},
  url =		 {.},
  abstract =	 {The Bayesian paradigm apparently only sometimes
                  gives rise to Occam's Razor; at other times very
                  large models perform well. We give simple examples
                  of both kinds of behaviour. The two views are
                  reconciled when measuring complexity of functions,
                  rather than of the machinery used to implement
                  them. We analyze the complexity of functions for
                  some linear in the parameter models that are
                  equivalent to Gaussian Processes, and always find
                  Occam's Razor at work.}
}

@inproceedings{RasGha02,
  cat =		 {gp np},
  author =	 {Carl Edward Rasmussen and Zoubin Ghahramani},
  title =	 {Infinite Mixtures of {G}aussian Process Experts},
  booktitle =	 nips14,
  year =	 2002,
  month =	 {December},
  editors =	 {Dietterich, T. G., Becker, S. and Ghahramani, Z.},
  pages =	 {881--888},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {We present an extension to the Mixture of Experts
                  (ME) model, where the individual experts are
                  Gaussian Process (GP) regression models. Using an
                  input-dependent adaptation of the Dirichlet Process,
                  we implement a gating network for an infinite number
                  of Experts. Inference in this model may be done
                  efficiently using a Markov Chain relying on Gibbs
                  sampling. The model allows the effective covariance
                  function to vary with the inputs, and may handle
                  large datasets --- thus potentially overcoming two
                  of the biggest hurdles with GP models. Simulations
                  show the viability of this approach.}
}

@inproceedings{RasGha03,
  cat =		 {gp mcmc},
  author =	 {Carl Edward Rasmussen and Zoubin Ghahramani},
  title =	 {Bayesian {Monte Carlo}},
  booktitle =	 nips15,
  pages =	 {489--496},
  year =	 2003,
  month =	 {December},
  address =	 {Cambridge, MA, USA},
  editor =	 {S.~Becker and S.~Thrun and K.~Obermayer},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {We investigate Bayesian alternatives to classical
                  Monte Carlo methods for evaluating
                  integrals. Bayesian Monte Carlo (BMC) allows the
                  incorporation of prior knowledge, such as smoothness
                  of the integrand, into the estimation. In a simple
                  problem we show that this outperforms any classical
                  importance sampling method. We also attempt more
                  challenging multidimensional integrals involved in
                  computing marginal likelihoods of statistical models
                  (a.k.a. partition functions and model evidences). We
                  find that Bayesian Monte Carlo outperformed Annealed
                  Importance Sampling, although for very high
                  dimensional problems or problems with massive
                  multimodality BMC may be less adequate. One
                  advantage of the Bayesian approach to Monte Carlo is
                  that samples can be drawn from any
                  distribution. This allows for the possibility of
                  active design of sample points so as to maximise
                  information gain.}
}

@inproceedings{RasKus04,
  cat =		 {rl},
  author =	 {Carl Edward Rasmussen and Malte Ku\ss},
  booktitle =	 nips16,
  editor =	 {S. Thrun and L.K. Saul and B. Sch\"olkopf},
  title =	 {Gaussian processes in reinforcement learning},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  year =	 2004,
  month =	 {December},
  pages =	 {751--759},
  url =		 {.},
  abstract =	 {We exploit some useful properties of Gaussian
                  process (GP) regression models for reinforcement
                  learning in continuous state spaces and discrete
                  time. We demonstrate how the GP model allows
                  evaluation of the value function in closed form. The
                  resulting policy iteration algorithm is demonstrated
                  on a simple problem with a two dimensional state
                  space. Further, we speculate that the intrinsic
                  ability of GP models to characterise distributions
                  of functions would allow the method to capture
                  entire distributions over future values instead of
                  merely their expectation, which has traditionally
                  been the focus of much of reinforcement learning.}
}

@manual{RasNeaHinetal96,
  author =	 {Carl Edward Rasmussen and Radford M.~Neal and
                  Geoffrey E.~Hinton and Drew van Camp and Mike Revow
                  and Zoubin Ghahramani and Rafal Kustra and Robert
                  Tibshirani},
  title =	 {The {DELVE} manual},
  pages =	 {1--108},
  year =	 1996,
  abstract =	 {DELVE -- Data for Evaluating Learning in Valid
                  Experiments -- is a collection of datasets from many
                  sources, an environment within which this data can
                  be used to assess the performance of methods for
                  learning relationships from data, and a repository
                  for the results of such experiments.},
  url =		 {.},
  annote =	 {The <a href="http://www.cs.toronto.edu/~delve">delve
                  website</a>.},
  organizaton =	 {University of Toronto}
}

@article{RasNic10,
  cat =		 {gp},
  author =	 {Carl Edward Rasmussen and Hannes Nickisch},
  title =	 {{Gaussian Processes for Machine Learning (GPML)
                  Toolbox}},
  journal =	 jmlr,
  volume =	 11,
  pages =	 {3011--3015},
  year =	 2010,
  month =	 {December},
  url =
                  {http://www.jmlr.org/papers/volume11/rasmussen10a/rasmussen10a.pdf},
  abstract =	 {The GPML toolbox provides a wide range of
                  functionality for Gaussian process (GP) inference
                  and prediction. GPs are specified by mean and
                  covariance functions; we offer a library of simple
                  mean and covariance functions and mechanisms to
                  compose more complex ones. Several likelihood
                  functions are supported including Gaussian and
                  heavy-tailed for regression as well as others
                  suitable for classification. Finally, a range of
                  inference methods is provided, including exact and
                  variational inference, Expectation Propagation, and
                  Laplace's method dealing with non-Gaussian
                  likelihoods and FITC for dealing with large
                  regression tasks.},
  annote =	 {Toolbox avaiable from <a
                  href="http://GaussianProcess.org/gpml/code">here</a>. Implements
                  algorithms from <a href="/pub/#RasWil06">Rasmussen
                  and Williams, 2006</a>.}
}

@inproceedings{RasQui05,
  cat =		 {gp},
  author =	 {Carl Edward Rasmussen and Joaquin
                  Qui{\~n}onero-Candela},
  title =	 {Healing the {R}elevance {V}ector {M}achine through
                  Augmentation},
  year =	 2005,
  pages =	 {689--696},
  booktitle =	 icml22,
  editor =	 {L.~De Raedt and S.~Wrobel},
  abstract =	 {The Relevance Vector Machine (RVM) is a sparse
                  approximate Bayesian kernel method. It provides full
                  predictive distributions for test cases. However,
                  the predictive uncertainties have the unintuitive
                  property, that \emph{they get smaller the further
                  you move away from the training cases}. We give a
                  thorough analysis. Inspired by the analogy to
                  non-degenerate Gaussian Processes, we suggest
                  augmentation to solve the problem. The purpose of
                  the resulting model, RVM*, is primarily to
                  corroborate the theoretical and experimental
                  analysis. Although RVM* could be used in practical
                  applications, it is no longer a truly sparse
                  model. Experiments show that sparsity comes at the
                  expense of worse predictive distributions.},
  location =	 {Bonn, Germany},
  url =		 {.}
}

@book{RasWil06,
  cat =		 {gp},
  author =	 {Carl Edward Rasmussen and Christopher
                  K.~I.~Williams},
  title =	 {Gaussian Processes for Machine Learning},
  publisher =	 mit,
  pages =	 272,
  year =	 2006,
  url =		 {.},
  abstract =	 {Gaussian processes (GPs) provide a principled,
                  practical, probabilistic approach to learning in
                  kernel machines. GPs have received increased
                  attention in the machine-learning community over the
                  past decade, and this book provides a long-needed
                  systematic and unified treatment of theoretical and
                  practical aspects of GPs in machine learning. The
                  treatment is comprehensive and self-contained,
                  targeted at researchers and students in machine
                  learning and applied statistics.},
  annote =	 {Winner of the 2009 <a
                  href="http://bayesian.org/project/degroot-prize">DeGroot
                  Prize</a>. Book <a
                  href="http://www.gaussianprocess.org/gpml">web
                  page</a>, <a
                  href="//www.gaussianprocess.org/gpml/chapters">chapters</a>
                  and <a
                  href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">entire
                  book pdf</a>. <a href="/pub/#RasNic10">GPML
                  Toolbox</a>.}
}

@article{RasWil93,
  author =	 {Carl Edward Rasmussen and David J. Willshaw},
  title =	 {Presynaptic and postsynaptic comptetition in models
                  for the development of neuromuscular connections},
  journal =	 {Biological Cybernetics},
  publisher =	 {Springer},
  volume =	 68,
  number =	 5,
  pages =	 {409--419},
  year =	 1993,
  url =		 {http://dx.doi.org/10.1007/BF00198773},
  abstract =	 {In the establishment of connections between nerve
                  and muscle there is an initial stage when each
                  muscle fibre is innervated by several different
                  motor axons. Withdrawal of connections then takes
                  place until each fibre has contact from just a
                  single axon. The evidence suggests that the
                  withdrawal process involves competition between
                  nerve terminals. We examine in formal models several
                  types of competitive mechanism that have been
                  proposed for this phenomenon. We show that a model
                  which combines competition for a presynaptic
                  resource with competition for a postsynaptic
                  resource is superior to others. This model accounts
                  for many anatomical and physiological findings and
                  has a biologically plausible
                  implementation. Intrinsic withdrawal appears to be a
                  side effect of the competitive mechanism rather than
                  a separate non-competitive feature. The model's
                  capabilities are confirmed by theoretical analysis
                  and full scale computer simulations.},
  doi =		 {10.1007/BF00198773}
}

@article{RavGhaWil02a,
  author =	 {Raval, A. and Ghahramani, Zoubin and Wild, David L.},
  cat =		 {bioinf},
  journal =	 {Bioinformatics},
  number =	 6,
  pages =	 {788-801},
  title =	 {A {B}ayesian network model for protein fold and
                  remote homologue recognition},
  url =		 {.},
  volume =	 18,
  year =	 2002,
  abstract =	 {Motivation: The Bayesian network approach is a
                  framework which combines graphical representation
                  and probability theory, which includes, as a special
                  case, hidden Markov models. Hidden Markov models
                  trained on amino acid sequence or secondary
                  structure data alone have been shown to have
                  potential for addressing the problem of protein fold
                  and superfamily classification.  Results: This paper
                  describes a novel implementation of a Bayesian
                  network which simultaneously learns amino acid
                  sequence, secondary structure and residue
                  accessibility for proteins of known
                  three-dimensional structure. An awareness of the
                  errors inherent in predicted secondary structure may
                  be incorporated into the model by means of a
                  confusion matrix. Training and validation data have
                  been derived for a number of protein superfamilies
                  from the Structural Classification of Proteins
                  (SCOP) database. Cross validation results using
                  posterior probability classification demonstrate
                  that the Bayesian network performs better in
                  classifying proteins of known structural superfamily
                  than a hidden Markov model trained on amino acid
                  sequences alone.}
}

@inproceedings{RawReqBruetal22,
  cat =		 {approx},
  title =	 {Challenges and Pitfalls of {B}ayesian Unlearning},
  author =	 {Ambrish Rawat and James Requeima and Wessel Bruinsma
                  and Richard Turner},
  booktitle =	 {ICML 2022 Workshop on Updatable Machine Learning
                  (UpML)},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2207.03227},
  abstract =	 {Machine unlearning refers to the task of removing a
                  subset of training data, thereby removing its
                  contributions to a trained model. Approximate
                  unlearning are one class of methods for this task
                  which avoid the need to retrain the model from
                  scratch on the retained data. Bayes’ rule can be
                  used to cast approximate unlearning as an inference
                  problem where the objective is to obtain the updated
                  posterior by dividing out the likelihood of deleted
                  data. However this has its own set of challenges as
                  one often doesn’t have access to the exact posterior
                  of the model parameters. In this work we examine the
                  use of the Laplace approximation and Variational
                  Inference to obtain the updated posterior. With a
                  neural network trained for a regression task as the
                  guiding example, we draw insights on the
                  applicability of Bayesian unlearning in practical
                  scenarios.}
}

@inproceedings{ReeGha13a,
  author =	 {Reed, Colorado and Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {approx, np},
  pages =	 {1013-1021},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {Scaling the {I}ndian Buffet Process via Submodular
                  Maximization},
  url =		 {.},
  volume =	 28,
  year =	 2013,
  abstract =	 {Inference for latent feature models is inherently
                  difficult as the inference space grows exponentially
                  with the size of the input data and number of latent
                  features. In this work, we use Kurihara & Welling
                  (2008)'s maximization-expectation framework to
                  perform approximate MAP inference for
                  linear-Gaussian latent feature models with an Indian
                  Buffet Process (IBP) prior. This formulation yields
                  a submodular function of the features that
                  corresponds to a lower bound on the model
                  evidence. By adding a constant to this function, we
                  obtain a nonnegative submodular function that can be
                  maximized via a greedy algorithm that obtains at
                  least a one-third approximation to the optimal
                  solution. Our inference method scales linearly with
                  the size of the input data, and we show the efficacy
                  of our method on the largest datasets currently
                  analyzed using an IBP model.  }
}

@inproceedings{Reizingeretal22,
  cat =		 {deep causal},
  title =	 {Embrace the Gap: VAEs Perform Independent Mechanism
                  Analysis},
  author =	 {Reizinger*, P. and Gresele*, L. and Brady*, J. and
                  von K{\"u}gelgen, J. and Zietlow, D. and
                  Sch{\"o}lkopf, B. and Martius, G. and Brendel,
                  W. and Besserve, M.},
  booktitle =	 nips35,
  publisher =	 {Curran Associates, Inc.},
  year =	 2022,
  note =	 {*equal first authorship},
  url =		 {https://arxiv.org/abs/2206.02416},
  abstract =	 {Variational autoencoders (VAEs) are a popular
                  framework for modeling complex data distributions;
                  they can be efficiently trained via variational
                  inference by maximizing the evidence lower bound
                  (ELBO), at the expense of a gap to the exact
                  (log-)marginal likelihood. While VAEs are commonly
                  used for representation learning, it is unclear why
                  ELBO maximization would yield useful
                  representations, since unregularized maximum
                  likelihood estimation cannot invert the
                  data-generating process. Yet, VAEs often succeed at
                  this task. We seek to elucidate this apparent
                  paradox by studying nonlinear VAEs in the limit of
                  near-deterministic decoders. We first prove that, in
                  this regime, the optimal encoder approximately
                  inverts the decoder -- a commonly used but unproven
                  conjecture -- which we refer to as {\em
                  self-consistency}. Leveraging self-consistency, we
                  show that the ELBO converges to a regularized
                  log-likelihood. This allows VAEs to perform what has
                  recently been termed independent mechanism analysis
                  (IMA): it adds an inductive bias towards decoders
                  with column-orthogonal Jacobians, which helps
                  recovering the true latent factors. The gap between
                  ELBO and log-likelihood is therefore welcome, since
                  it bears unanticipated benefits for nonlinear
                  representation learning. In experiments on synthetic
                  and image data, we show that VAEs uncover the true
                  latent factors when the data generating process
                  satisfies the IMA assumption.},
}

@inproceedings{ReqGorBro19,
  cat =		 {deep},
  author =	 {James Requeima and Jonathan Gordon and John
                  Bronskill and S<!>ebastian Nowozin and Richard
                  E. Turner},
  title =	 {Fast and Flexible Multi-Task Classification using
                  Conditional Neural Adaptive Processes},
  booktitle =	 nips33,
  year =	 2019,
  url =
                  {https://papers.nips.cc/paper/9009-fast-and-flexible-multi-task-classification-using-conditional-neural-adaptive-processes},
  abstract =	 {The goal of this paper is to design image
                  classification systems that, after an initial
                  multi-task training phase, can automatically adapt
                  to new tasks encountered at test time. We introduce
                  a conditional neural process based approach to the
                  multi-task classification setting for this purpose,
                  and establish connections to the meta- and few-shot
                  learning literature. The resulting approach, called
                  CNAPs, comprises a classifier whose parameters are
                  modulated by an adaptation network that takes the
                  current task's dataset as input. We demonstrate that
                  CNAPs achieves state-of-the-art results on the
                  challenging Meta-Dataset benchmark indicating
                  high-quality transfer-learning. We show that the
                  approach is robust, avoiding both over-fitting in
                  low-shot regimes and under-fitting in high-shot
                  regimes.  Timing experiments reveal that CNAPs is
                  computationally efficient at test-time as it does
                  not involve gradient based adaptation. Finally, we
                  show that trained models are immediately deployable
                  to continual learning and active learning where they
                  can outperform existing approaches that do not
                  leverage transfer learning.}
}

@inproceedings{ReqTebBruTur19,
  cat =		 {gp},
  author =	 {James Requeima and William Tebbutt and Wessel
                  Bruinsma and Richard E. Turner},
  title =	 {The {Gaussian} Process Autoregressive Regression
                  Model {(GPAR)}},
  booktitle =	 aistats22,
  year =	 2019,
  publisher =	 pmlr,
  url =		 {http://proceedings.mlr.press/v89/requeima19a.html},
  abstract =	 {Multi-output regression models must exploit
                  dependencies between outputs to maximise predictive
                  performance. The application of Gaussian processes
                  (GPs) to this setting typically yields models that
                  are computationally demanding and have limited
                  representational power. We present the Gaussian
                  Process Autoregressive Regression (GPAR) model, a
                  scalable multi-output GP model that is able to
                  capture nonlinear, possibly input-varying,
                  dependencies between outputs in a simple and
                  tractable way: the product rule is used to decompose
                  the joint distribution over the outputs into a set
                  of conditionals, each of which is modelled by a
                  standard GP. GPAR’s efficacy is demonstrated on a
                  variety of synthetic and real-world problems,
                  outperforming existing GP models and achieving
                  state-of-the-art performance on established
                  benchmarks.}
}

@article{RojSchTurPet15,
  title =	 {A causal perspective on domain adaptation},
  author =	 {Mateo Rojas-Carulla and Bernhard Sch{\"o}lkopf and
                  Richard Turner and Jonas Peters},
  journal =	 {arXiv preprint arXiv:1507.05333)},
  url =		 {http://arxiv.org/abs/1507.05333},
  year =	 2015,
  abstract =	 {From training data from several related domains (or
                  tasks), methods of domain adaptation try to combine
                  knowledge to improve performance. This paper
                  discusses an approach to domain adaptation which is
                  inspired by a causal interpretation of the
                  multi-task problem. We assume that a covariate shift
                  assumption holds true for a subset of predictor
                  variables: the conditional of the target variable
                  given this subset of predictors is invariant with
                  respect to shifts in those predictors
                  (covariates). We propose to learn the corresponding
                  conditional expectation in the training domains and
                  use it for estimation in the target domain. We
                  further introduce a method which allows for
                  automatic inference of the above subset in
                  regression and classification. We study the
                  performance of this approach in an adversarial
                  setting, in the case where no additional examples
                  are available in the test domain. If a labeled
                  sample is available, we provide a method for using
                  both the transferred invariant conditional and task
                  specific information. We present results on
                  synthetic data sets and a sentiment analysis
                  problem.}
}

@inproceedings{RotVanMooGha10,
  cat =		 {ssl},
  author =	 {Rotsos, C. and {Van Gael}, J. and Moore, A.W. and
                  Ghahramani, Z.},
  booktitle =	 {1st International Workshop on Traffic Analysis and
                  Classification (IWCMC '10)},
  keywords =	 {Machine Learning,Network,Semi Supervised},
  mendeley-tags ={Machine Learning,Network,Semi Supervised},
  title =	 {Traffic Classification in Information Poor
                  Environments},
  url =		 {.},
  year =	 2010,
  month =	 {July},
  address =	 {Caen, France},
  abstract =	 {Traffic classification using machine learning
                  continues to be an active research area. The
                  majority of work in this area uses
                  \emph{off-the-shelf} machine learning tools and
                  treats them as \emph{black-box} classifiers. This
                  approach turns all the modelling complexity into a
                  feature selection problem. In this paper, we build a
                  problem-specific solution to the traffic
                  classification problem by designing a custom
                  probabilistic graphical model. Graphical models are
                  a modular framework to design classifiers which
                  incorporate domain-specific knowledge.  More
                  specifically, our solution introduces
                  semi-supervised learning which means we learn from
                  both labelled and unlabelled traffic flows. We show
                  that our solution performs competitively compared to
                  previous approaches while using less data and
                  simpler features.}
}

@inproceedings{RotVanMooetal10,
  cat =		 {gm},
  author =	 {Charalampos Rotsos and Jurgen {Van Gael} and Andrew
                  W. Moore and Zoubin Ghahramani},
  year =	 2010,
  title =	 {Probabilistic Graphical Models for Semi-Supervised
                  Traffic Classification},
  booktitle =	 {The 6th International Wireless Communications and
                  Mobile Computing Conference},
  pages =	 {752--757},
  address =	 {Caen, France},
  url =		 {.},
  abstract =	 {Traffic classification using machine learning
                  continues to be an active research area. The
                  majority of work in this area uses off-the-shelf
                  machine learning tools and treats them as black-box
                  classifiers. This approach turns all the modelling
                  complexity into a feature selection problem. In this
                  paper, we build a problem-specific solution to the
                  traffic classification problem by designing a custom
                  probabilistic graphical model. Graphical models are
                  a modular framework to design classifiers which
                  incorporate domain-specific knowledge. More
                  specifically, our solution introduces
                  semi-supervised learning which means we learn from
                  both labelled and unlabelled traffic flows. We show
                  that our solution performs competitively compared to
                  previous approaches while using less data and
                  simpler features.}
}

@inproceedings{RowBelDabetal18,
  cat =		 {rl},
  title =	 {An Analysis of Categorical Distributional
                  Reinforcement Learning},
  booktitle =	 aistats21,
  year =	 2018,
  author =	 {Mark Rowland and Marc G. Bellemare and Will Dabney
                  and R{\'e}mi Munos and Yee Whye Teh},
  month =	 {April},
  address =	 {Playa Blanca, Lanzarote, Canary Islands},
  url =		 {http://arxiv.org/abs/1802.08163},
  abstract =	 {Distributional approaches to value-based
                  reinforcement learning model the entire distribution
                  of returns, rather than just their expected values,
                  and have recently been shown to yield
                  state-of-the-art empirical performance. This was
                  demonstrated by the recently proposed C51 algorithm,
                  based on categorical distributional reinforcement
                  learning (CDRL) [Bellemare et al., 2017]. However,
                  the theoretical properties of CDRL algorithms are
                  not yet well understood. In this paper, we introduce
                  a framework to analyse CDRL algorithms, establish
                  the importance of the projected distributional
                  Bellman operator in distributional RL, draw
                  fundamental connections between CDRL and the
                  Cram\'er distance, and give a proof of convergence
                  for sample-based categorical distributional
                  reinforcement learning algorithms.}
}

@inproceedings{RowChoCha18,
  cat =		 {mc},
  title =	 {Geometrically coupled {Monte Carlo} sampling},
  author =	 {Mark Rowland and Krzysztof Choromanski and Francois
                  Chalus and Aldo Pacchiano and Tamas Sarlos and
                  Richard Turner and Adrian Weller},
  booktitle =	 nips32,
  year =	 2018,
  month =	 {December},
  address =	 {Montreal Canada},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/NIPS18-gcmc.pdf},
  abstract =	 {Monte Carlo sampling in high-dimensional, low-sample
                  settings is important in many machine learning
                  tasks. We improve current methods for sampling in
                  Euclidean spaces by avoiding independence, and
                  instead consider ways to couple samples. We show
                  fundamental connections to optimal transport theory,
                  leading to novel sampling algorithms, and providing
                  new theoretical grounding for existing
                  strategies. We compare our new strategies against
                  prior methods for improving sample efficiency,
                  including quasi-Monte Carlo, by studying
                  discrepancy. We explore our findings empirically,
                  and observe benefits of our sampling schemes for
                  reinforcement learning and generative modelling.  }
}

@article{RowGha99a,
  author =	 {Roweis, Sam T. and Ghahramani, Zoubin},
  cat =		 {review},
  journal =	 {Neural Computation},
  number =	 2,
  pages =	 {305-345},
  title =	 {A Unifying Review of Linear {G}aussian Models},
  url =		 {.},
  volume =	 11,
  year =	 1999,
  abstract =	 {Factor analysis, principal component analysis,
                  mixtures of gaussian clusters, vector quantization,
                  Kalman filter models, and hidden Markov models can
                  all be unified as variations of unsupervised
                  learning under a single basic generative model. This
                  is achieved by collecting together disparate
                  observations and derivations made by many previous
                  authors and introducing a new way of linking
                  discrete and continuous state models using a simple
                  nonlinearity. Through the use of other
                  nonlinearities, we show how independent component
                  analysis is also a variation of the same basic
                  generative model. We show that factor analysis and
                  mixtures of gaussians can be implemented in
                  autoencoder neural networks and learned using
                  squared error plus the same regularization term. We
                  introduce a new model for static data, known as
                  sensible principal component analysis, as well as a
                  novel concept of spatially adaptive observation
                  noise. We also review some of the literature
                  involving global and local mixtures of the basic
                  models and provide pseudocode for inference and
                  learning for all the basic models.  }
}

@inproceedings{RowHroTan19,
  cat =		 {mc},
  title =	 {Orthogonal Estimation of {W}asserstein Distances},
  author =	 {Mark Rowland and Jiri Hron and Yunhao Tang and
                  Krzysztof Choromanski and Tamas Sarlos and Adrian
                  Weller},
  booktitle =	 aistats22,
  year =	 2019,
  month =	 {April},
  address =	 {Okinawa, Japan},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/AISTATS19-slicedwasserstein.pdf},
  abstract =	 {Wasserstein distances are increasingly used in a
                  wide variety of applications in machine
                  learning. Sliced Wasserstein distances form an
                  important subclass which may be estimated
                  efficiently through one-dimensional sorting
                  operations. In this paper, we propose a new variant
                  of sliced Wasserstein distance, study the use of
                  orthogonal coupling in Monte Carlo estimation of
                  Wasserstein distances and draw connections with
                  stratified sampling, and evaluate our approaches
                  experimentally in a range of large-scale experiments
                  in generative modelling and reinforcement learning.
                  }
}

@inproceedings{RowPacWel17,
  cat =		 {gm},
  title =	 {Conditions beyond treewidth for tightness of
                  higher-order {LP} relaxations},
  booktitle =	 aistats20,
  year =	 2017,
  month =	 {April},
  address =	 {Fort Lauderdale, Florida},
  author =	 {Mark Rowland and Aldo Pacchiano and Adrian Weller},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/conditions.pdf},
  abstract =	 {Linear programming (LP) relaxations are a popular
                  method to attempt to find a most likely
                  configuration of a discrete graphical model. If a
                  solution to the relaxed problem is obtained at an
                  integral vertex then the solution is guaranteed to
                  be exact and we say that the relaxation is tight. We
                  consider binary pairwise models and introduce new
                  methods which allow us to demonstrate refined
                  conditions for tightness of LP relaxations in the
                  Sherali-Adams hierarchy. Our results include showing
                  that for higher order LP relaxations, treewidth is
                  not precisely the right way to characterize
                  tightness. This work is primarily theoretical, with
                  insights that can improve efficiency in practice.}
}

@inproceedings{RowWel17,
  cat =		 {gm},
  title =	 {Uprooting and rerooting higher-order graphical
                  models},
  booktitle =	 nips31,
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California},
  author =	 {Mark Rowland and Adrian Weller},
  abstract =	 {The idea of uprooting and rerooting graphical models
                  was introduced specifically for binary pairwise
                  models by Weller [18] as a way to transform a model
                  to any of a whole equivalence class of related
                  models, such that inference on any one model yields
                  inference results for all others. This is very
                  helpful since inference, or relevant bounds, may be
                  much easier to obtain or more accurate for some
                  model in the class. Here we introduce methods to
                  extend the approach to models with higher-order
                  potentials and develop theoretical insights. For
                  example, we demonstrate that the triplet-consistent
                  polytope TRI is unique in being 'universally
                  rooted'. We demonstrate empirically that rerooting
                  can significantly improve accuracy of methods of
                  inference for higher-order models at negligible
                  computational cost.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/uprooting-higher-order.pdf}
}

@inproceedings{Roy11,
  cat =		 {np gm},
  author =	 {Daniel M. Roy},
  title =	 {On the computability and complexity of {B}ayesian
                  reasoning},
  booktitle =	 {NIPS Workshop on Philosophy and Machine Learning},
  year =	 2011,
  url =		 {http://danroy.org/papers/Roy-NIPSPML-2011.pdf},
  abstract =	 { If we consider the claim made by some cognitive
                  scientists that the mind performs Bayesian
                  reasoning, and if we simultaneously accept the
                  Physical Church-Turing thesis and thus believe that
                  the computational power of the mind is no more than
                  that of a Turing machine, then what limitations are
                  there to the reasoning abilities of the mind?  I
                  give an overview of joint work with Nathanael
                  Ackerman (Harvard, Mathematics) and Cameron Freer
                  (MIT, CSAIL) that bears on the computability and
                  complexity of Bayesian reasoning.  In particular, we
                  prove that conditional probability is in general not
                  computable in the presence of continuous random
                  variables.  However, in light of additional
                  structure in the prior distribution, such as the
                  presence of certain types of noise, or of
                  exchangeability, conditioning is possible.  These
                  results cover most of statistical practice.  At the
                  workshop on Logic and Computational Complexity, we
                  presented results on the computational complexity of
                  conditioning, embedding sharp-P-complete problems in
                  the task of computing conditional probabilities for
                  diffuse continuous random variables.  This work
                  complements older work. For example, under
                  cryptographic assumptions, the computational
                  complexity of producing samples and computing
                  probabilities was separated by Ben-David, Chor,
                  Goldreich and Luby.  In recent work, we also make
                  use of cryptographic assumptions to show that
                  different representations of exchangeable sequences
                  may have vastly different complexity.  However, when
                  faced with an adversary that is computational
                  bounded, these different representations have the
                  same complexity, highlighting the fact that
                  knowledge representation and approximation play a
                  fundamental role in the possibility and plausibility
                  of Bayesian reasoning.}
}

@phdthesis{Saa11,
  cat =		 {gp},
  author =	 {Yunus Saat\c{c}i},
  title =	 {Scalable Inference for Structured {G}aussian Process
                  Models},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2011,
  url =		 {.},
  address =	 {Cambridge, UK},
  abstract =	 {The generic inference and learning algorithm for
                  Gaussian Process (GP) regression has
                  O(N<sup>3</sup>) runtime and O(N<sup>2</sup>) memory
                  complexity, where N is the number of observations in
                  the dataset. Given the computational resources
                  available to a present-day workstation, this implies
                  that GP regression simply <em>cannot be run</em> on
                  large datasets. The need to use non- Gaussian
                  likelihood functions for tasks such as
                  classification adds even more to the computational
                  burden involved.<br> The majority of algorithms
                  designed to improve the scaling of GPs are founded
                  on the idea of approximating the true covariance
                  matrix, which is usually of rank N, with a matrix of
                  rank P, where P&lt;&lt;N. Typically, the true
                  training set is replaced with a smaller,
                  representative (pseudo-) training set such that a
                  specific measure of information loss is minimized.
                  These algorithms typically attain O(P<sup>2</sup>N)
                  runtime and O(PN) space complexity. They are also
                  general in the sense that they are designed to work
                  with <em>any</em> covariance function. In essence,
                  they trade off accuracy with computational
                  complexity. The central contribution of this thesis
                  is to improve scaling instead by exploiting any
                  structure that is present in the covariance matrices
                  generated by <em>particular</em> covariance
                  functions.  Instead of settling for a
                  kernel-independent accuracy/complexity trade off, as
                  is done in much the literature, we often obtain
                  accuracies close to, or exactly equal to the full GP
                  model at a fraction of the computational cost.<br>
                  We define a <em>structured</em> GP as any GP model
                  that is endowed with a kernel which produces
                  structured covariance matrices. A trivial example of
                  a structured GP is one with the linear regression
                  kernel. In this case, given inputs living in
                  R<sup>D</sup>, the covariance matrices generated
                  have rank D -- this results in significant
                  computational gains in the usual case where
                  D&lt;&lt;N. Another case arises when a stationary
                  kernel is evaluated on equispaced, scalar
                  inputs. This results in <em>Toeplitz</em> covariance
                  matrices and all necessary computations can be
                  carried out exactly in O(N log N).<br> This thesis
                  studies four more types of structured GP. First, we
                  comprehensively review the case of kernels
                  corresponding to <em>Gauss-Markov</em> processes
                  evaluated on scalar inputs. Using state-space models
                  we show how (generalised) regression (including
                  hyperparameter learning) can be performed in O(N log
                  N) runtime and O(N) space. Secondly, we study the
                  case where we introduce block structure into the
                  covariance matrix of a GP time-series model by
                  assuming a particular form of nonstationarity a
                  priori. Third, we extend the efficiency of scalar
                  Gauss-Markov processes to higher-dimensional input
                  spaces by assuming <em>additivity</em>. We
                  illustrate the connections between the classical
                  backfitting algorithm and approximate Bayesian
                  inference techniques including Gibbs sampling and
                  variational Bayes. We also show that it is possible
                  to relax the rather strong assumption of additivity
                  without sacrificing O(N log N) complexity, by means
                  of a projection-pursuit style GP regression
                  model. Finally, we study the properties of a GP
                  model with a tensor product kernel evaluated on a
                  multivariate grid of inputs locations. We show that
                  for an <em>arbitrary</em> (regular or irregular)
                  grid the resulting covariance matrices are Kronecker
                  and full GP regression can be implemented in O(N)
                  time and memory usage.<br> We illustrate the power
                  of these methods on several real-world regression
                  datasets which satisfy the assumptions inherent in
                  the structured GP employed. In many cases we obtain
                  performance comparable to the generic GP
                  algorithm. We also analyse the performance
                  degradation when these assumptions are not met, and
                  in several cases show that it is comparable to that
                  observed for sparse GP methods. We provide similar
                  results for regression tasks with non-Gaussian
                  likelihoods, an extension rarely addressed by sparse
                  GP techniques.}
}

@inproceedings{SaaTurRas10,
  cat =		 {gp time},
  author =	 {Yunus {Saat\c{c}i} and Ryan T<!>urner and Carl
                  Edward Rasmussen},
  title =	 {Gaussian Process Change Point Models},
  booktitle =	 icml27,
  pages =	 {927--934},
  year =	 2010,
  month =	 {June},
  address =	 {Haifa, Israel},
  url =		 {.},
  abstract =	 {We combine Bayesian online change point detection
                  with Gaussian processes to create a nonparametric
                  time series model which can handle change
                  points. The model can be used to locate change
                  points in an online manner; and, unlike other
                  Bayesian online change point detection algorithms,
                  is applicable when temporal correlations in a regime
                  are expected. We show three variations on how to
                  apply Gaussian processes in the change point
                  context, each with their own advantages. We present
                  methods to reduce the computational burden of these
                  models and demonstrate it on several real world data
                  sets.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/rdturner/ICML2010poster.pdf">poster</a>,
                  <a
                  href="http://mlg.eng.cam.ac.uk/rdturner/ICML2010talk.pdf">slides</a>.}
}

@inproceedings{SalRowGha03a,
  author =	 {Salakhutdinov, Ruslan and Roweis, Sam T. and
                  Ghahramani, Zoubin},
  booktitle =	 {UAI},
  cat =		 {approx},
  editor =	 {Meek, Christopher and Kj{\ae}rulff, Uffe},
  isbn =	 {0-127-05664-5},
  pages =	 {509-516},
  publisher =	 {Morgan Kaufmann},
  title =	 {On the Convergence of Bound Optimization Algorithms},
  url =		 {.},
  year =	 2003,
  abstract =	 {Many practitioners who use EM and related algorithms
                  complain that they are sometimes slow. When does
                  this happen, and what can be done about it? In this
                  paper, we study the general class of bound
                  optimization algorithms - including EM, Iterative
                  Scaling, Non-negative Matrix Factorization, CCCP -
                  and their relationship to direct optimization
                  algorithms such as gradientbased methods for
                  parameter learning. We derive a general relationship
                  between the updates performed by bound optimization
                  methods and those of gradient and second-order
                  methods and identify analytic conditions under which
                  bound optimization algorithms exhibit quasi-Newton
                  behavior, and under which they possess poor,
                  first-order convergence. Based on this analysis, we
                  consider several specific algorithms, interpret and
                  analyze their convergence properties and provide
                  some recipes for preprocessing input to these
                  algorithms to yield faster convergence behavior. We
                  report empirical results supporting our analysis and
                  showing that simple data preprocessing can result in
                  dramatically improved performance of bound
                  optimizers in practice.}
}

@inproceedings{SalRowGha03b,
  author =	 {Salakhutdinov, Ruslan and Roweis, Sam T. and
                  Ghahramani, Zoubin},
  booktitle =	 {ICML},
  cat =		 {approx},
  editor =	 {Fawcett, Tom and Mishra, Nina},
  isbn =	 {1-57735-189-4},
  pages =	 {672-679},
  publisher =	 {AAAI Press},
  title =	 {Optimization with {EM} and
                  Expectation-Conjugate-Gradient},
  url =		 {.},
  year =	 2003,
  abstract =	 {We show a close relationship between the
                  Expectation- Maximization (EM) algorithm and direct
                  optimization algorithms such as gradientbased
                  methods for parameter learning. We identify analytic
                  conditions under which EM exhibits Newton-like
                  behavior, and conditions under which it possesses
                  poor, first-order convergence. Based on this
                  analysis, we propose two novel algorithms for
                  maximum likelihood estimation of latent variable
                  models, and report empirical results showing that,
                  as predicted by theory, the proposed new algorithms
                  can substantially outperform standard EM in terms of
                  speed of convergence in certain cases. }
}

@article{SavGhaGrietal10,
  cat =		 {bioinf},
  author =	 {R. S. Savage and Z. Ghahramani and J. E. Griffin and
                  B. de la Cruz and D. L. Wild},
  year =	 2010,
  title =	 {Discovering Transcriptional Modules by {Bayesian}
                  Data Integration},
  journal =	 {Bioinformatics},
  volume =	 26,
  pages =	 {i158--i167},
  url =		 {.},
  abstract =	 {Motivation: We present a method for directly
                  inferring transcriptional modules (TMs) by
                  integrating gene expression and transcription factor
                  binding (ChIP-chip) data. Our model extends a
                  hierarchical Dirichlet process mixture model to
                  allow data fusion on a gene-by-gene basis. This
                  encodes the intuition that co-expression and
                  co-regulation are not necessarily equivalent and
                  hence we do not expect all genes to group similarly
                  in both datasets. In particular, it allows us to
                  identify the subset of genes that share the same
                  structure of transcriptional modules in both
                  datasets.<br>Results: We find that by working on a
                  gene-by-gene basis, our model is able to extract
                  clusters with greater functional coherence than
                  existing methods. By combining gene expression and
                  transcription factor binding (ChIP-chip) data in
                  this way, we are better able to determine the groups
                  of genes that are most likely to represent
                  underlying TMs.<br>Availability: If interested in
                  the code for the work presented in this article,
                  please contact the authors.}
}

@article{SavHelXuetal09,
  cat =		 {clust bioinf},
  volume =	 10,
  number =	 242,
  pages =	 {1--9},
  month =	 {August},
  author =	 {R.~Savage and K.~A.~Heller and Y.~Xu and Zoubin
                  Ghahramani and W.~Truman and M.~Grant and K.~Denby
                  and D.~L.~Wild},
  title =	 {{R/BHC}: fast {B}ayesian hierarchical clustering for
                  microarray data},
  publisher =	 {BioMed Central},
  journal =	 {BMC Bioinformatics 2009},
  year =	 2009,
  url =		 {.},
  abstract =	 {Background: Although the use of clustering methods
                  has rapidly become one of the standard computational
                  approaches in the literature of microarray gene
                  expression data analysis, little attention has been
                  paid to uncertainty in the results obtained.<br>
                  Results: We present an R/Bioconductor port of a fast
                  novel algorithm for Bayesian agglomerative
                  hierarchical clustering and demonstrate its use in
                  clustering gene expression microarray data. The
                  method performs bottom-up hierarchical clustering,
                  using a Dirichlet Process (infinite mixture) to
                  model uncertainty in the data and Bayesian model
                  selection to decide at each step which clusters to
                  merge.<br> Conclusion: Biologically plausible
                  results are presented from a well studied data set:
                  expression profiles of \emph{A. thaliana} subjected
                  to a variety of biotic and abiotic stresses. Our
                  method avoids several limitations of traditional
                  methods, for example how many clusters there should
                  be and how to choose a principled distance metric.},
  doi =		 {10.1186/1471-2105-10-242},
  issn =	 {1471-2105},
  pubmedid =	 19660130
}

@inproceedings{Sch09,
  title =	 {Function factorization using warped {G}aussian
                  processes},
  author =	 {Mikkel N. Schmidt},
  booktitle =	 icml26,
  address =	 {Montr\'{e}al, QC, Canada},
  pages =	 {921--928},
  editor =	 {L\'{e}on Bottou and Michael Littman},
  month =	 {June},
  publisher =	 {Omnipress},
  year =	 2009,
  url =		 {.},
  abstract =	 {We introduce a new approach to non-linear regression
                  called function factorization, that is suitable for
                  problems where an output variable can reasonably be
                  modeled by a number of multiplicative interaction
                  terms between non-linear functions of the
                  inputs. The idea is to approximate a complicated
                  function on a high-dimensional space by the sum of
                  products of simpler functions on lower-dimensional
                  subspaces. Function factorization can be seen as a
                  generalization of matrix and tensor factorization
                  methods, in which the data are approximated by the
                  sum of outer products of vectors. We present a
                  non-parametric Bayesian approach to function
                  factorization where the priors over the factorizing
                  functions are warped Gaussian processes, and we do
                  inference using Hamiltonian Markov chain Monte
                  Carlo. We demonstrate the superior predictive
                  performance of the method on a food science data set
                  compared to Gaussian process regression and tensor
                  factorization using PARAFAC and GEMANOVA models.},
  annote =	 {<a
                  href="http://mikkelschmidt.dk/uploads/media/talk_icml2009_01.pdf">slides</a>. <a
                  href="http://mikkelschmidt.dk/uploads/media/poster_icml2009_01.pdf">poster</a>. <a
                  href="http://videolectures.net/icml09_schmidt_ffuw/">video</a>.}
}

@inproceedings{Sch09b,
  title =	 {Linearly constrained {B}ayesian matrix factorization
                  for blind source separation},
  author =	 {Mikkel N. Schmidt},
  booktitle =	 nips22,
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  editor =	 {Y. Bengio and D. Schuurmans and J. Lafferty and
                  C. K. I. Williams and A. Culotta},
  month =	 {December},
  pages =	 {1624--1632},
  year =	 2009,
  url =		 {.},
  abstract =	 {We present a general Bayesian approach to
                  probabilistic matrix factorization subject to linear
                  constraints. The approach is based on a Gaussian
                  observation model and Gaussian priors with bilinear
                  equality and inequality constraints. We present an
                  efficient Markov chain Monte Carlo inference
                  procedure based on Gibbs sampling. Special cases of
                  the proposed model are Bayesian formulations of
                  non-negative matrix factorization and factor
                  analysis. The method is evaluated on a blind source
                  separation problem. We demonstrate that our
                  algorithm can be used to extract meaningful and
                  interpretable features that are remarkably different
                  from features extracted using existing related
                  matrix factorization techniques.},
  annote =	 {<a
                  href="http://mikkelschmidt.dk/index.php?id=23&amp;cHash=9da63395ee586d4767b1fd35337bd5dd&amp;tx_ttnews[tt_news]=47&amp;tx_ttnews[backPid]=2">code</a>.}
}

@article{ SchDuvHen2014,
  cat =		 {gp},
  title =	 "Probabilistic {ODE} Solvers with {R}unge-{K}utta
                  Means",
  author =	 "Michael Schober and David Duvenaud and Philipp
                  Hennig",
  url =		 {http://arxiv.org/abs/1406.2582},
  journal =	 "arXiv preprint arXiv:1406.2582",
  month =	 {June},
  year =	 2014,
  abstract =	 {Runge-Kutta methods are the classic family of
                  solvers for ordinary differential equations (ODEs),
                  and the basis for the state-of-the-art. Like most
                  numerical methods, they return point estimates. We
                  construct a family of probabilistic numerical
                  methods that instead return a Gauss-Markov process
                  defining a probability distribution over the ODE
                  solution. In contrast to prior work, we construct
                  this family such that posterior means match the
                  outputs of the Runge-Kutta family exactly, thus
                  inheriting their proven good properties. Remaining
                  degrees of freedom not identified by the match to
                  Runge-Kutta are chosen such that the posterior
                  probability measure fits the observed structure of
                  the ODE. Our results shed light on the structure of
                  Runge-Kutta solvers from a new direction, provide a
                  richer, probabilistic output, have low computational
                  cost, and raise new research questions.}
}

@inproceedings{SchJorObeetal22,
  title =	 {Last layer marginal likelihood for invariance
                  learning},
  author =	 {Pola E. Schw\"{o}bel and Martin J\o{}rgensen and
                  Sebastian W. Ober and Mark van der Wilk},
  booktitle =	 aistats25,
  abstract =	 {Data augmentation is often used to incorporate
                  inductive biases into models. Traditionally, these
                  are hand-crafted and tuned with cross validation.
                  The Bayesian paradigm for model selection provides a
                  path towards end-to-end learning of invariances
                  using only the training data, by optimising the
                  marginal likelihood. Computing the marginal
                  likelihood is hard for neural networks, but success
                  with tractable approaches that compute the marginal
                  likelihood for the last layer only raises the
                  question of whether this convenient approach might
                  be employed for learning invariances. We show
                  partial success on standard benchmarks, in the
                  low-data regime and on a medical imaging dataset by
                  designing a custom optimisation routine. Introducing
                  a new lower bound to the marginal likelihood allows
                  us to perform inference for a larger class of
                  likelihood functions than before. On the other hand,
                  we demonstrate failure modes on the CIFAR10 dataset,
                  where the last layer approximation is not sufficient
                  due to the increased complexity of our neural
                  network. Our results indicate that once more
                  sophisticated approximations become available the
                  marginal likelihood is a promising approach for
                  invariance learning in neural networks.},
  year =	 2022,
  cat =		 {deep},
  url =
                  {https://proceedings.mlr.press/v151/schwobel22a/schwobel22a.pdf}
}

@inproceedings{SchMoh09,
  title =	 {Probabilistic non-negative tensor factorization
                  using {M}arkov chain {M}onte {C}arlo},
  author =	 {Mikkel N. Schmidt and Shakir Mohamed},
  booktitle =	 {European Signal Processing Conference (EUSIPCO)},
  address =	 {Glasgow, Scotland},
  month =	 {August},
  year =	 2009,
  pages =	 {1918--1922},
  url =		 {.},
  abstract =	 {We present a probabilistic model for learning
                  non-negative tensor factorizations (NTF), in which
                  the tensor factors are latent variables associated
                  with each data dimension. The non-negativity
                  constraint for the latent factors is handled by
                  choosing priors with support on the non-negative
                  numbers. Two Bayesian inference procedures based on
                  Markov chain Monte Carlo sampling are described:
                  Gibbs sampling and Hamiltonian Markov chain Monte
                  Carlo. We evaluate the model on two food science
                  data sets, and show that the probabilistic NTF model
                  leads to better predictions and avoids overfitting
                  compared to existing NTF approaches.},
  annote =	 {Rated by reviewers amongst the top 5% of the
                  presented papers.}
}

@article{SchVenKnoetal11,
  cat =		 {bioinf},
  author =	 {Cornelia Schone and Anne Venner and David A. Knowles
                  and Mahesh M Karnani and Denis Burdakov},
  title =	 {Dichotomous cellular properties of mouse
                  orexin/hypocretin neurons},
  url =
                  {http://jp.physoc.org/content/early/2011/04/11/jphysiol.2011.208637.abstract},
  journal =	 {The Journal of Physiology},
  year =	 2011,
  abstract =	 {Hypothalamic hypocretin/orexin (hcrt/orx) neurons
                  recently emerged as critical regulators of
                  sleep-wake cycles, reward-seeking, and body energy
                  balance. However, at the level of cellular and
                  network properties, it remains unclear whether
                  hcrt/orx neurons are one homogenous population, or
                  whether there are several distinct types of hcrt/orx
                  cells. Here, we collated diverse structural and
                  functional information about individual hcrt/orx
                  neurons in mouse brain slices, by combining
                  patch-clamp analysis of spike firing, membrane
                  currents, and synaptic inputs with confocal imaging
                  of cell shape and subsequent 3-dimensional Sholl
                  analysis of dendritic architecture. Statistical
                  cluster analysis of intrinsic firing properties
                  revealed that hcrt/orx neurons fall into two
                  distinct types. These two cell types also differ in
                  the complexity of their dendritic arbour, the
                  strength of AMPA and GABAA receptor-mediated
                  synaptic drive that they receive, and the density of
                  low-threshold, 4-aminopyridine-sensitive, transient
                  K+ current. Our results provide quantitative
                  evidence that, at the cellular level, the mouse
                  hcrt/orx system is composed of two classes of
                  neurons with different firing properties,
                  morphologies, and synaptic input organization.}
}

@inproceedings{SchWinHan09,
  title =	 {Bayesian non-negative matrix factorization},
  author =	 {Mikkel N. Schmidt and Ole Winther and Lars Kai
                  Hansen},
  booktitle =	 {8th International Conference on Independent
                  Component Analysis and Signal Separation},
  pages =	 {540--547},
  publisher =	 {Springer},
  address =	 {Paraty, Brazil},
  series =	 lncs,
  volume =	 5441,
  year =	 2009,
  month =	 {March},
  url =		 {.},
  abstract =	 {We present a Bayesian treatment of non-negative
                  matrix factorization (NMF), based on a normal
                  likelihood and exponential priors, and derive an
                  efficient Gibbs sampler to approximate the posterior
                  density of the NMF factors. On a chemical brain
                  imaging data set, we show that this improves
                  interpretability by providing uncertainty
                  estimates. We discuss how the Gibbs sampler can be
                  used for model order selection by estimating the
                  marginal likelihood, and compare with the Bayesian
                  information criterion. For computing the maximum a
                  posteriori estimate we present an iterated
                  conditional modes algorithm that rivals existing
                  state-of-the-art NMF algorithms on an image feature
                  extraction problem.},
  annote =	 {<a
                  href="http://mikkelschmidt.dk/uploads/media/talk_ica2009.pdf">slides</a>. <a
                  href="http://mikkelschmidt.dk/uploads/media/bayesnmf_01.zip">code</a>.}
}

@inproceedings{Schottetal22,
  cat =		 {deep mvision},
  title =	 {Visual Representation Learning Does Not Generalize
                  Strongly Within the Same Domain},
  author =	 {Schott, L. and von K{\"u}gelgen, J. and Tr{\"a}uble,
                  F. and Gehler, P. and Russell, C. and Bethge, M. and
                  Sch{\"o}lkopf, B. and Locatello, F. and Brendel, W.},
  booktitle =	 iclr10,
  year =	 2022,
  url =		 {https://openreview.net/forum?id=9RUHPlladgh},
  abstract =	 {An important component for generalization in machine
                  learning is to uncover underlying latent factors of
                  variation as well as the mechanism through which
                  each factor acts in the world.  In this paper, we
                  test whether 17 unsupervised, weakly supervised, and
                  fully supervised representation learning approaches
                  correctly infer the generative factors of variation
                  in simple datasets (dSprites, Shapes3D, MPI3D) from
                  controlled environments, and on our contributed
                  CelebGlow dataset.  In contrast to prior robustness
                  work that introduces novel factors of variation
                  during test time, such as blur or other
                  (un)structured noise, we here recompose,
                  interpolate, or extrapolate only existing factors of
                  variation from the training data set (e.g., small
                  and medium-sized objects during training and large
                  objects during testing). Models that learn the
                  correct mechanism should be able to generalize to
                  this benchmark.  In total, we train and test 2000+
                  models and observe that all of them struggle to
                  learn the underlying mechanism regardless of
                  supervision signal and architectural bias. Moreover,
                  the generalization capabilities of all tested models
                  drop significantly as we move from artificial
                  datasets towards more realistic real-world datasets.
                  Despite their inability to identify the correct
                  mechanism, the models are quite modular as their
                  ability to infer other in-distribution factors
                  remains fairly stable, providing only a single
                  factor is out-of-distribution. These results point
                  to an important yet understudied problem of learning
                  mechanistic models of observations that can
                  facilitate generalization.},
}

@phdthesis{Sci19,
  author =	 {Adam \'Scibior},
  title =	 {Formally justified and modular {B}ayesian inference
                  for probabilistic programs},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2019,
  address =	 {Cambridge, UK},
  url =		 {.},
  abstract =	 {Probabilistic modelling offers a simple and coherent
                  framework to describe the real world in the face of
                  uncertainty. Furthermore, by applying Bayes' rule it
                  is possible to use probabilistic models to make
                  inferences about the state of the world from partial
                  observations. While traditionally probabilistic
                  models were constructed on paper, more recently the
                  approach of probabilistic programming enables users
                  to write the models in executable languages
                  resembling computer programs and to freely mix them
                  with deterministic code.  It has long been
                  recognised that the semantics of programming
                  languages is complicated and the intuitive
                  understanding that programmers have is often
                  inaccurate, resulting in difficult to understand
                  bugs and unexpected program behaviours. Programming
                  languages are therefore studied in a rigorous way
                  using formal languages with mathematically defined
                  semantics. Traditionally formal semantics of
                  probabilistic programs are defined using exact
                  inference results, but in practice exact Bayesian
                  inference is not tractable and approximate methods
                  are used instead, posing a question of how the
                  results of these algorithms relate to the exact
                  results. Correctness of such approximate methods is
                  usually argued somewhat less rigorously, without
                  reference to a formal semantics.  In this
                  dissertation we formally develop denotational
                  semantics for probabilistic programs that correspond
                  to popular sampling algorithms often used in
                  practice. The semantics is defined for an expressive
                  typed lambda calculus with higher-order functions
                  and inductive types, extended with probabilistic
                  effects for sampling and conditioning, allowing
                  continuous distributions and unbounded
                  likelihoods. It makes crucial use of the recently
                  developed formalism of quasi-Borel spaces to bring
                  all these elements together. We provide semantics
                  corresponding to several variants of Markov chain
                  Monte Carlo and Sequential Monte Carlo methods and
                  formally prove a notion of correctness for these
                  algorithms in the context of probabilistic
                  programming.  We also show that the semantic
                  construction can be directly mapped to an
                  implementation using established functional
                  programming abstractions called monad
                  transformers. We develop a compact Haskell library
                  for probabilistic programming closely corresponding
                  to the semantic construction, giving users a high
                  level of assurance in the correctness of the
                  implementation. We also demonstrate on a collection
                  of benchmarks that the library offers performance
                  competitive with existing systems of similar scope.
                  An important property of our construction, both the
                  semantics and the implementation, is the high degree
                  of modularity it offers. All the inference
                  algorithms are constructed by combining small
                  building blocks in a setup where the type system
                  ensures correctness of compositions. We show that
                  with basic building blocks corresponding to vanilla
                  Metropolis-Hastings and Sequential Monte Carlo we
                  can implement more advanced algorithms known in the
                  literature, such as Resample-Move Sequential Monte
                  Carlo, Particle Marginal Metropolis-Hastings, and
                  Sequential Monte Carlo squared. These
                  implementations are very concise, reducing the
                  effort required to produce them and the scope for
                  bugs. On top of that, our modular construction
                  enables in some cases deterministic testing of
                  randomised inference algorithms, further increasing
                  reliability of the implementation.}
}

@inproceedings{SciGhaGor15,
  author =	 {Adam \'Scibior and Zoubin Ghahramani and Andrew
                  D. Gordon},
  title =	 {Practical Probabilistic Programming with Monads},
  booktitle =	 {Proceedings of the 8th ACM SIGPLAN Symposium on
                  Haskell},
  year =	 2015,
  publisher =	 acm,
  url =		 {.},
  abstract =	 {The machine learning community has recently shown a
                  lot of interest in practical probabilistic
                  programming systems that target the problem of
                  Bayesian inference. Such systems come in different
                  forms, but they all express probabilistic models as
                  computational processes using syntax resembling
                  programming languages. In the functional programming
                  community monads are known to offer a convenient and
                  elegant abstraction for programming with probability
                  distributions, but their use is often limited to
                  very simple inference problems. We show that it is
                  possible to use the monad abstraction to construct
                  probabilistic models for machine learning, while
                  still offering good performance of inference in
                  challenging models. We use a GADT as an underlying
                  representation of a probability distribution and
                  apply Sequential Monte Carlo-based methods to
                  achieve efficient inference. We define a formal
                  semantics via measure theory. We demonstrate a clean
                  and elegant implementation that achieves performance
                  comparable with Anglican, a state-of-the-art
                  probabilistic programming system.},
  doi =		 {10.1145/2804302.2804317}
}

@article{SciKamGha18,
  author =	 {Adam \'Scibior and Ohad Kammar and Zoubin
                  Ghahramani},
  journal =	 {Proceedings of the ACM on Programming Languages},
  title =	 {Functional programming for modular {B}ayesian
                  inference},
  year =	 2018,
  volume =	 2,
  issue =	 {ICFP},
  url =		 {http://mlg.eng.cam.ac.uk/adam/icfp2018.pdf},
  abstract =	 { We present an architectural design of a library for
                  Bayesian modelling and inference in modern
                  functional programming languages. The novel aspect
                  of our approach are modular implementations of
                  existing state-of-the-art inference algorithms.  Our
                  design relies on three inherently functional
                  features: higher-order functions, inductive
                  data-types, and support for either type-classes or
                  an expressive module system.  We provide a
                  performant Haskell implementation of this
                  architecture, demonstrating that high-level and
                  modular probabilistic programming can be added as a
                  library in sufficiently expressive languages.  We
                  review the core abstractions in this architecture:
                  inference representations, inference
                  transformations, and inference representation
                  transformers. We then implement concrete instances
                  of these abstractions, counterparts to particle
                  filters and Metropolis-Hastings samplers, which form
                  the basic building blocks of our library. By
                  composing these building blocks we obtain
                  state-of-the-art inference algorithms: Resample-Move
                  Sequential Monte Carlo, Particle Marginal
                  Metropolis-Hastings, and Sequential Monte Carlo
                  Squared. We evaluate our implementation against
                  existing probabilistic programming systems and find
                  it is already competitively performant, although we
                  conjecture that existing functional programming
                  optimisation techniques could reduce the overhead
                  associated with the abstractions we use. We show
                  that our modular design enables deterministic
                  testing of inherently stochastic Monte Carlo
                  algorithms. Finally, we demonstrate using OCaml that
                  an expressive module system can also implement our
                  design.}
}

@article{SciKamVaketal18,
  author =	 {Adam \'Scibior and Ohad Kammar and Matthijs
                  V\'ak\'ar and Sam Staton and H<!>ongseok Yang and
                  Yufei Cai and Klaus Ostermann and Sean K.~Moss and
                  Chris Heunen and Zoubin Ghahramani},
  journal =	 {Proceedings of the ACM on Programming Languages},
  title =	 {Denotational Validation of Higher-Order {B}ayesian
                  Inference},
  year =	 2018,
  volume =	 2,
  url =		 {.},
  abstract =	 {We present a modular semantic account of Bayesian
                  inference algorithms for probabilistic programming
                  languages, as used in data science and machine
                  learning. Sophisticated inference algorithms are
                  often explained in terms of composition of smaller
                  parts. However, neither their theoretical
                  justification nor their implementation reflects this
                  modularity. We show how to conceptualise and analyse
                  such inference algorithms as manipulating
                  intermediate representations of probabilistic
                  programs using higher-order functions and inductive
                  types, and their denotational semantics.  Semantic
                  accounts of continuous distributions use measurable
                  spaces. However, our use of higher-order functions
                  presents a substantial technical difficulty: it is
                  impossible to define a measurable space structure
                  over the collection of measurable functions between
                  arbitrary measurable spaces that is compatible with
                  standard operations on those functions, such as
                  function application. We overcome this difficulty
                  using quasi-Borel spaces, a recently proposed
                  mathematical structure that supports both function
                  spaces and continuous distributions.  We define a
                  class of semantic structures for representing
                  probabilistic programs, and semantic validity
                  criteria for transformations of these
                  representations in terms of distribution
                  preservation. We develop a collection of building
                  blocks for composing representations. We use these
                  building blocks to validate common inference
                  algorithms such as Sequential Monte Carlo and Markov
                  Chain Monte Carlo. To emphasize the connection
                  between the semantic manipulation and its
                  traditional measure theoretic origins, we use Kock’s
                  synthetic measure theory. We demonstrate its
                  usefulness by proving a quasi-Borel counterpart to
                  the Metropolis-Hastings-Green theorem.}
}

@inproceedings{Sejetal12,
  cat =		 {gp mcm},
  booktitle =	 icml31,
  title =	 {Kernel adaptive {Metropolis-H}astings},
  author =	 {Dino Sejdinovic and Heiko Strathmann and Maria
                  Lomeli and Christophe Andrieu and Arthur Gretton},
  year =	 2012,
  address =	 {Beijing, China},
  month =	 {June},
  pages =	 {1--9},
  url =		 {http://proceedings.mlr.press/v32/sejdinovic14.pdf},
  abstract =	 {A Kernel Adaptive Metropolis-Hastings algo- rithm is
                  introduced, for the purpose of sampling from a
                  target distribution with strongly nonlin- ear
                  support. The algorithm embeds the trajec- tory of
                  the Markov chain into a reproducing ker- nel Hilbert
                  space (RKHS), such that the fea- ture space
                  covariance of the samples informs the choice of
                  proposal. The procedure is com- putationally
                  efficient and straightforward to im- plement, since
                  the RKHS moves can be inte- grated out analytically:
                  our proposal distribu- tion in the original space is
                  a normal distribution whose mean and covariance
                  depend on where the current sample lies in the
                  support of the tar- get distribution, and adapts to
                  its local covari- ance structure. Furthermore, the
                  procedure re- quires neither gradients nor any other
                  higher or- der information about the target, making
                  it par- ticularly attractive for contexts such as
                  Pseudo- Marginal MCMC. Kernel Adaptive Metropolis-
                  Hastings outperforms competing fixed and adap- tive
                  samplers on multivariate, highly nonlinear target
                  distributions, arising in both real-world and
                  synthetic examples.}
}

@inproceedings{SenAmoHosetal20,
  cat =		 {deep},
  author =	 {Ushnish Sengupta and Matt Amos and J. Scott Hosking and Carl
                  Edward Rasmussen and Matthew P. Juniper and Paul J. Young},
  title =	 "Ensembling geophysical models with {B}ayesian Neural
                  Networks",
  year =	 2021,
  booktitle =	 nips34,
  abstract =	 {Ensembles of geophysical models improve projection
                  accuracy and express uncertainties. We develop a
                  novel data-driven ensembling strategy for combining
                  geophysical models using Bayesian Neural Networks,
                  which infers spatiotemporally varying model weights
                  and bias while accounting for heteroscedastic
                  uncertainties in the observations. This produces
                  more accurate and uncertainty-aware projections
                  without sacrificing interpretability. Applied to the
                  prediction of total column ozone from an ensemble of
                  15 chemistry-climate models, we find that the
                  Bayesian neural network ensemble (BayNNE)
                  outperforms existing ensembling methods, achieving a
                  49.4\% reduction in RMSE for temporal extrapolation,
                  and a 67.4\% reduction in RMSE for polar data voids,
                  compared to a weighted mean. Uncertainty is also
                  well-characterized, with 90.6\% of the data points
                  in our extrapolation validation dataset lying within
                  2 standard deviations and 98.5\% within 3 standard
                  deviations.},
  url =
                  {https://proceedings.neurips.cc/paper_files/paper/2020/file/0d5501edb21a59a43435efa67f200828-Paper.pdf}
}

@article{ShaGha13a,
  author =	 {Shah, Amar and Ghahramani, Zoubin},
  cat =		 {clust, np},
  journal =	 {UAI},
  title =	 {Determinantal Clustering Processes - {A}
                  Nonparametric {B}ayesian Approach to Kernel Based
                  Semi-Supervised Clustering},
  url =		 {.},
  year =	 2013,
  abstract =	 {Semi-supervised clustering is the task of clustering
                  data points into clusters where only a fraction of
                  the points are labelled. The true number of clusters
                  in the data is often unknown and most models require
                  this parameter as an input. Dirichlet process
                  mixture models are appealing as they can infer the
                  number of clusters from the data. However, these
                  models do not deal with high dimensional data well
                  and can encounter difficulties in inference. We
                  present a novel nonparameteric Bayesian kernel based
                  method to cluster data points without the need to
                  prespecify the number of clusters or to model
                  complicated densities from which data points are
                  assumed to be generated from. The key insight is to
                  use determinants of submatrices of a kernel matrix
                  as a measure of how close together a set of points
                  are. We explore some theoretical properties of the
                  model and derive a natural Gibbs based algorithm
                  with MCMC hyperparameter learning. The model is
                  implemented on a variety of synthetic and real world
                  data sets.  }
}

@inproceedings{ShaQuaLam12,
  cat =		 {mvision},
  title =	 {Augmented Attributes Representations},
  author =	 {Viktoriia Sharmanska and Novi Quadrianto and
                  Christoph Lampert},
  booktitle =	 {12th European Conference on Computer Vision},
  pages =	 {242--255},
  year =	 2012,
  url =		 {.},
  abstract =	 {We propose a new learning method to infer a
                  mid-level feature representation that combines the
                  advantage of semantic attribute representations with
                  the higher expressive power of non-semantic
                  features. The idea lies in augmenting an existing
                  attribute-based representation with additional
                  dimensions for which an autoencoder model is coupled
                  with a large-margin principle. This construction
                  allows a smooth transition between the zero-shot
                  regime with no training example, the unsupervised
                  regime with training examples but without class
                  labels, and the supervised regime with training
                  examples and with class labels. The resulting
                  optimization problem can be solved efficiently,
                  because several of the necessity steps have
                  closed-form solutions. Through extensive experiments
                  we show that the augmented representation achieves
                  better results in terms of object categorization
                  accuracy than the semantic representation alone.}
}

@inproceedings{ShaQuaLam13,
  cat =		 {mvision},
  title =	 {Learning to Rank Using Privileged Information},
  author =	 {Viktoriia Sharmanska and Novi Quadrianto and
                  Christoph Lampert},
  booktitle =	 {International Conference on Computer Vision},
  year =	 2013,
  url =		 {.},
  abstract =	 {Many computer vision problems have an asymmetric
                  distribution of information between training and
                  test time.  In this work, we study the case where we
                  are given additional information about the training
                  data, which however will not be available at test
                  time. This situation is called learning using
                  privileged information (LUPI).  We introduce two
                  maximum-margin techniques that are able to make use
                  of this additional source of information, and we
                  show that the framework is applicable to several
                  scenarios that have been studied in computer vision
                  before. Experiments with attributes, bounding boxes,
                  image tags and rationales as additional information
                  in object classification show promising results.}
}

@inproceedings{ShaWilGha14a,
  author =	 {Shah, Amar and Wilson, Andrew Gordon and Ghahramani,
                  Zoubin},
  booktitle =	 {AISTATS},
  cat =		 {gp},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {Student-t Processes as Alternatives to {G}aussian
                  Processes},
  url =		 {.},
  year =	 2014,
  abstract =	 {We investigate the Student-t process as an
                  alternative to the Gaussian process as a
                  nonparametric prior over functions. We derive closed
                  form expressions for the marginal likelihood and
                  predictive distribution of a Student-t process, by
                  integrating away an inverse Wishart process prior
                  over the covariance kernel of a Gaussian process
                  model. We show surprising equivalences between
                  different hierarchical Gaussian process models
                  leading to Student-t processes, and derive a new
                  sampling scheme for the inverse Wishart process,
                  which helps elucidate these equivalences. Overall,
                  we show that a Student-t process can retain the
                  attractive properties of a Gaussian process -- a
                  nonparametric representation, analytic marginal and
                  predictive distributions, and easy model selection
                  through covariance kernels -- but has enhanced
                  flexibility, and predictive covariances that, unlike
                  a Gaussian process, explicitly depend on the values
                  of training observations. We verify empirically that
                  a Student-t process is especially useful in
                  situations where there are changes in covariance
                  structure, or in applications like Bayesian
                  optimization, where accurate predictive covariances
                  are critical for good performance. These advantages
                  come at no additional computational cost over
                  Gaussian processes.  }
}

@article{SidRajMahetal22,
  title =	 {Metadata Archaeology: Unearthing Data Subsets by
                  Leveraging Training Dynamics},
  author =	 {Siddiqui, Shoaib Ahmed and Rajkumar, Nitarshan and
                  Maharaj, Tegan and Krueger, David and Hooker, Sara},
  journal =	 {arXiv preprint arXiv:2209.10015},
  url =		 {https://arxiv.org/abs/2209.10015},
  year =	 2022,
  abstract =	 {Modern machine learning research relies on
                  relatively few carefully curated datasets. Even in
                  these datasets, and typically in `untidy' or raw
                  data, practitioners are faced with significant
                  issues of data quality and diversity which can be
                  prohibitively labor intensive to address. Existing
                  methods for dealing with these challenges tend to
                  make strong assumptions about the particular issues
                  at play, and often require a priori knowledge or
                  metadata such as domain labels. Our work is
                  orthogonal to these methods: we instead focus on
                  providing a unified and efficient framework for
                  Metadata Archaeology -- uncovering and inferring
                  metadata of examples in a dataset. We curate
                  different subsets of data that might exist in a
                  dataset (e.g. mislabeled, atypical, or
                  out-of-distribution examples) using simple
                  transformations, and leverage differences in
                  learning dynamics between these probe suites to
                  infer metadata of interest. Our method is on par
                  with far more sophisticated mitigation methods
                  across different tasks: identifying and correcting
                  mislabeled examples, classifying minority-group
                  samples, prioritizing points relevant for training
                  and enabling scalable human auditing of relevant
                  examples.},
  annote =	 {Project webpage: <a
                  href="https://metadata-archaeology.github.io/">https://metadata-archaeology.github.io/</a>}
}

@inproceedings{SilChuGha08,
  cat =		 {gm},
  booktitle =	 nips20,
  month =	 {December},
  title =	 {Hidden common cause relations in relational
                  learning},
  author =	 {R.~Silva and W.~Chu and Zoubin Ghahramani},
  year =	 2008,
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  editor =	 {J.~C.~Platt and D.~Koller and Y.~Singer and
                  S.~Roweis},
  pages =	 {1345--1352},
  url =		 {.},
  abstract =	 {When predicting class labels for objects within a
                  relational database, it is often helpful to consider
                  a model for relationships: this allows for
                  information between class labels to be shared and to
                  improve prediction performance. However, there are
                  different ways by which objects can be related
                  within a relational database.  One traditional way
                  corresponds to a Markov network structure: each
                  existing relation is represented by an undirected
                  edge. This encodes that, conditioned on input
                  features, each object label is independent of other
                  object labels given its neighbors in the
                  graph. However, there is no reason why Markov
                  networks should be the only representation of choice
                  for symmetric dependence structures. Here we discuss
                  the case when relationships are postulated to exist
                  due to \emph{hidden common causes}. We discuss how
                  the resulting graphical model differs from Markov
                  networks, and how it describes different types of
                  real-world relational processes.  A Bayesian
                  nonparametric classification model is built upon
                  this graphical representation and evaluated with
                  several empirical studies.},
  annote =	 {Code at <a
                  href="http://www.homepages.ucl.ac.uk/~ucgtrbd/code/xgp">http://www.homepages.ucl.ac.uk/~ucgtrbd/code/xgp</a>}
}

@inproceedings{SilGha06a,
  author =	 {Silva, Ricardo and Ghahramani, Zoubin},
  booktitle =	 {UAI},
  cat =		 {gm},
  isbn =	 {0-9749039-2-2},
  publisher =	 {AUAI Press},
  title =	 {Bayesian Inference for {G}aussian Mixed Graph
                  Models},
  url =		 {.},
  year =	 2006,
  abstract =	 {We introduce priors and algorithms to perform
                  Bayesian inference in Gaussian models defined by
                  acyclic directed mixed graphs. Such a class of
                  graphs, composed of directed and bi-directed edges,
                  is a representation of conditional independencies
                  that is closed under marginalization and arises
                  naturally from causal models which allow for
                  unmeasured confounding. Monte Carlo methods and a
                  variational approximation for such models are
                  presented. Our algorithms for Bayesian inference
                  allow the evaluation of posterior distributions for
                  several quantities of interest, including causal
                  effects that are not identifiable from data alone
                  but could otherwise be inferred where informative
                  prior knowledge about confounding is available.}
}

@article{SilGha09,
  cat =		 {gm},
  volume =	 10,
  month =	 {June},
  author =	 {R. Silva and Z. Ghahramani},
  title =	 {The hidden life of latent variables: {Bayesian}
                  learning with mixed graph models},
  publisher =	 {Association for Computing Machinery},
  journal =	 jmlr,
  pages =	 {1187--1238},
  year =	 2009,
  url =		 {.},
  abstract =	 {Directed acyclic graphs (DAGs) have been widely used
                  as a representation of conditional independence in
                  machine learning and statistics. Moreover, hidden or
                  latent variables are often an important component of
                  graphical models. However, DAG models suffer from an
                  important limitation: the family of DAGs is not
                  closed under marginalization of hidden
                  variables. This means that in general we cannot use
                  a DAG to represent the independencies over a subset
                  of variables in a larger DAG. Directed mixed graphs
                  (DMGs) are a representation that includes DAGs as a
                  special case, and overcomes this limitation. This
                  paper introduces algorithms for performing Bayesian
                  inference in Gaussian and probit DMG models. An
                  important requirement for inference is the
                  specification of the distribution over parameters of
                  the models. We introduce a new distribution for
                  covariance matrices of Gaussian DMGs. We discuss and
                  illustrate how several Bayesian machine learning
                  tasks can benefit from the principle presented here:
                  the power to model dependencies that are generated
                  from hidden variables, but without necessarily
                  modeling such variables explicitly.}
}

@inproceedings{SilGha09b,
  cat =		 {gm},
  volume =	 5,
  author =	 {R. Silva and Z. Ghahramani},
  annote =	 {Code at <a
                  href="http://www.homepages.ucl.ac.uk/~ucgtrbd/code/fmog-version0.zip">http://www.homepages.ucl.ac.uk/~ucgtrbd/code/fmog-version0.zip</a>},
  note =	 {ISSN: 1938-7228},
  booktitle =	 aistats12,
  title =	 {Factorial mixture of {Gaussians} and the marginal
                  independence model},
  publisher =	 jmlr,
  pages =	 {520--527},
  year =	 2009,
  month =	 {April},
  address =	 {Clearwater Beach, FL, USA},
  url =		 {.},
  abstract =	 {Marginal independence constraints play an important
                  role in learning with graphical models. One way of
                  parameterizing a model of marginal independencies is
                  by building a latent variable model where two
                  independent observed variables have no common latent
                  source. In sparse domains, however, it might be
                  advantageous to model the marginal observed
                  distribution directly, without explicitly including
                  latent variables in the model.  There have been
                  recent advances in Gaussian and binary models of
                  marginal independence, but no models with non-linear
                  dependencies between continuous variables has been
                  proposed so far. In this paper, we describe how to
                  generalize the Gaussian model of marginal
                  independencies based on mixtures, and how to learn
                  parameters. This requires a non-standard
                  parameterization and raises difficult non-linear
                  optimization issues.}
}

@inproceedings{SilHelGha07a,
  author =	 {Silva, Ricardo and Heller, Katherine A. and
                  Ghahramani, Zoubin},
  booktitle =	 {AISTATS},
  cat =		 {ir},
  editor =	 {Meila, Marina and Shen, Xiaotong},
  pages =	 {500-507},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {Analogical Reasoning with Relational {B}ayesian
                  Sets},
  url =		 {.},
  volume =	 2,
  year =	 2007,
  abstract =	 {Analogical reasoning depends fundamentally on the
                  ability to learn and generalize about relations
                  between objects. There are many ways in which
                  objects can be related, making automated analogical
                  reasoning very chal- lenging. Here we develop an
                  approach which, given a set of pairs of related
                  objects S = {A1:B1,A2:B2,...,AN:BN}, measures how
                  well other pairs A:B fit in with the set S. This
                  addresses the question: is the relation between
                  objects A and B analogous to those relations found
                  in S? We recast this classi- cal problem as a
                  problem of Bayesian analy- sis of relational
                  data. This problem is non- trivial because direct
                  similarity between ob- jects is not a good way of
                  measuring analo- gies. For instance, the analogy
                  between an electron around the nucleus of an atom
                  and a planet around the Sun is hardly justified by
                  isolated, non-relational, comparisons of an electron
                  to a planet, and a nucleus to the Sun. We develop a
                  generative model for predicting the existence of
                  relationships and extend the framework of Ghahramani
                  and Heller (2005) to provide a Bayesian measure for
                  how analogous a relation is to other relations. This
                  sheds new light on an old problem, which we motivate
                  and illustrate through practical applications in
                  exploratory data analysis.}
}

@article{SilHelGhaetal10,
  cat =		 {ir bioinf},
  author =	 {R. Silva and K. A. Heller and Z. Ghahramani and
                  E. M. Airoldi},
  year =	 2010,
  pages =	 {615--644},
  volume =	 4,
  number =	 2,
  title =	 {Ranking Relations Using Analogies in Biological and
                  Information Networks},
  journal =	 {Annals of Applied Statistics},
  url =		 {.},
  abstract =	 {Analogical reasoning depends fundamentally on the
                  ability to learn and generalize about relations
                  between objects. We develop an approach to
                  relational learning which, given a set of pairs of
                  objects S = {A(1):B(1), A(2):B(2), ..., A(N):B(N)},
                  measures how well other pairs A:B fit in with the
                  set S. Our work addresses the question: is the
                  relation between objects A and B analogous to those
                  relations found in S? Such questions are
                  particularly relevant in information retrieval,
                  where an investigator might want to search for
                  analogous pairs of objects that match the query set
                  of interest. There are many ways in which objects
                  can be related, making the task of measuring
                  analogies very challenging. Our approach combines a
                  similarity measure on function spaces with Bayesian
                  analysis to produce a ranking.  It requires data
                  containing features of the objects of interest and a
                  link matrix specifying which relationships exist; no
                  further attributes of such relationships are
                  necessary. We illustrate the potential of our method
                  on text analysis and information networks. An
                  application on discovering functional interactions
                  between pairs of proteins is discussed in detail,
                  where we show that our approach can work in practice
                  even if a small set of protein pairs is provided.}
}

@inproceedings{SimDavLalEtal21,
  title =	 {Kernel Identification Through Transformers},
  cat =		 {gp deep},
  author =	 {Simpson, Fergus and Davies, Ian and Lalchand, Vidhi
                  and Vullo, Alessandro and Durrande, Nicolas and
                  Rasmussen, Carl Edward},
  booktitle =	 nips34,
  pages =	 {10483--10495},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/56c3b2c6ea3a83aaeeff35eeb45d700d-Paper.pdf},
  volume =	 34,
  year =	 2021,
  abstract =	 {Kernel selection plays a central role in determining
                  the performance of Gaussian Process (GP) models, as
                  the chosen kernel determines both the inductive
                  biases and prior support of functions under the GP
                  prior. This work addresses the challenge of
                  constructing custom kernel functions for
                  high-dimensional GP regression models. Drawing
                  inspiration from recent progress in deep learning,
                  we introduce a novel approach named KITT: Kernel
                  Identification Through Transformers. KITT exploits a
                  transformer-based architecture to generate kernel
                  recommendations in under 0.1 seconds, which is
                  several orders of magnitude faster than conventional
                  kernel search algorithms. We train our model using
                  synthetic data generated from priors over a
                  vocabulary of known kernels. By exploiting the
                  nature of the self-attention mechanism, KITT is able
                  to process datasets with inputs of arbitrary
                  dimension. We demonstrate that kernels chosen by
                  KITT yield strong performance over a diverse
                  collection of regression benchmarks.}
}

@inproceedings{SimLalRas21,
  title =	 {Marginalised {G}aussian {P}rocesses with {N}ested
                  {S}ampling},
  cat =		 {gp np mcmc},
  author =	 {Simpson, Fergus and Lalchand, Vidhi and Rasmussen,
                  Carl Edward},
  booktitle =	 nips34,
  pages =	 {13613--13625},
  publisher =	 {Curran Associates, Inc.},
  url =
                  {https://proceedings.neurips.cc/paper/2021/file/712a67567ec10c52c2b966224cf94d1e-Paper.pdf},
  volume =	 34,
  year =	 2021,
  abstract =	 {Gaussian Process models are a rich distribution over
                  functions with inductive biases controlled by a
                  kernel function. Learning occurs through
                  optimisation of the kernel hyperparameters using the
                  marginal likelihood as the objective. This work
                  proposes nested sampling as a means of marginalising
                  kernel hyperparameters, because it is a technique
                  that is well-suited to exploring complex,
                  multi-modal distributions. We benchmark against
                  Hamiltonian Monte Carlo on time-series and
                  two-dimensional regression tasks, finding that a
                  principled approach to quantifying hyperparameter
                  uncertainty substantially improves the quality of
                  prediction intervals.}
}

@inproceedings{SimSciToletal16,
  cat =		 {gp},
  author =	 {Carl-Johann Simon-Gabriel and Adam \'Scibior and
                  Ilya Tolstikhin and Bernhard Sch\"olkopf},
  title =	 {Consistent Kernel Mean Estimation for Functions of
                  Random Variables},
  booktitle =	 nips30,
  year =	 2016,
  url =		 {.},
  abstract =	 {We provide a theoretical foundation for
                  non-parametric estimation of functions of random
                  variables using kernel mean embeddings. We show that
                  for any continuous function f, consistent estimators
                  of the mean embedding of a random variable X lead to
                  consistent estimators of the mean embedding of f(X).
                  For Mat\'ern kernels and sufficiently smooth
                  functions we also provide rates of convergence.  Our
                  results extend to functions of multiple random
                  variables. If the variables are dependent, we
                  require an estimator of the mean embedding of their
                  joint distribution as a starting point; if they are
                  independent, it is sufficient to have separate
                  estimators of the mean embeddings of their marginal
                  distributions. In either case, our results cover
                  both mean embeddings based on i.i.d. samples as well
                  as "reduced set" expansions in terms of dependent
                  expansion points. The latter serves as a
                  justification for using such expansions to limit
                  memory resources when applying the approach as a
                  basis for probabilistic programming.}
}

@inproceedings{SinQuiBaketal04,
  cat =		 {gp},
  author =	 {Fabian Sinz and Joaquin Qui{\~n}onero-Candela and
                  G{\"o}khan H.~Bakir and Carl Edward Rasmussen and
                  Matthias O.~Franz},
  title =	 {Learning Depth From Stereo},
  year =	 2004,
  series =	 lncs,
  volume =	 3175,
  publisher =	 {Springer},
  pages =	 {245--252},
  month =	 09,
  journal =	 {Pattern Recognition: Proceedings of the 26th DAGM
                  Symposium},
  editor =	 {C.~E.~Rasmussen and H.~H.~B{\"u}lthoff and
                  B.~Sch{\"o}lkopf and M.~A.~Giese},
  address =	 {Berlin, Germany},
  abstract =	 {We compare two approaches to the problem of
                  estimating the depth of a point in space from
                  observing its image position in two different
                  cameras: 1.~The classical photogrammetric approach
                  explicitly models the two cameras and estimates
                  their intrinsic and extrinsic parameters using a
                  tedious calibration procedure; 2.~A generic machine
                  learning approach where the mapping from image to
                  spatial coordinates is directly approximated by a
                  Gaussian Process regression. Our results show that
                  the generic learning approach, in addition to
                  simplifying the procedure of calibration, can lead
                  to higher depth accuracies than classical
                  calibration although no specific domain knowledge is
                  used.},
  booktitle =	 {26th DAGM Symposium},
  location =	 {T{\"u}bingen, Germany},
  url =		 {.}
}

@inproceedings{SkaHowKraKru22,
  title =	 {Defining and Characterizing Reward Hacking},
  author =	 {Skalse, Joar and Howe, Nikolaus HR and
                  Krasheninnikov, Dmitrii and Krueger, David},
  booktitle =	 nips35,
  year =	 2022,
  url =		 {https://arxiv.org/abs/2209.13085}
}

@inproceedings{SkoSjaKok17,
  cat =		 {sigproc},
  author =	 {Martin A. Skoglund and Zoran Sjanic and Manon Kok},
  title =	 {On orientation estimation using iterative methods in
                  {E}uclidean space},
  booktitle =	 {Proceedings of the 20th International Conference on
                  Information Fusion},
  year =	 2017,
  month =	 {July},
  address =	 {Xi'an, China},
  url =
                  {http://mlg.eng.cam.ac.uk/manon/publications/SkoglundSK2017.pdf},
  doi =		 {10.23919/ICIF.2017.8009830},
  abstract =	 {This paper presents three iterative methods for
                  orientation estimation. The first two are based on
                  iterated Extended Kalman filter (IEKF) formulations
                  with different state representations. The first is
                  using the well-known unit quaternion as state
                  (q-IEKF) while the other is using orientation
                  deviation which we call IMEKF. The third method is
                  based on nonlinear least squares (NLS) estimation of
                  the angular velocity which is used to parametrise
                  the orientation. The results are obtained using
                  Monte Carlo simulations and the comparison is done
                  with the non-iterative EKF and multiplicative EKF
                  (MEKF) as baseline. The result clearly shows that
                  the IMEKF and the NLS-based method are superior to
                  q-IEKF and all three outperform the non-iterative
                  methods.}
}

@InProceedings{SneGha05,
  cat =		 {approx},
  author =	 {Edward Snelson and Zoubin Ghahramani},
  title =	 {Compact Approximations to {B}ayesian Predictive
                  Distributions},
  booktitle =	 icml22,
  year =	 2005,
  address =	 {Bonn, Germany},
  month =	 {August},
  URL =		 {http://www.gatsby.ucl.ac.uk/~snelson/Bayes_pred.pdf},
  publisher =	 {Omnipress},
  abstract =	 {We provide a general framework for learning precise,
                  compact, and fast representations of the Bayesian
                  predictive distribution for a model. This framework
                  is based on minimizing the KL divergence between the
                  true predictive density and a suitable compact
                  approximation. We consider various methods for doing
                  this, both sampling based approximations, and
                  deterministic approximations such as expectation
                  propagation. These methods are tested on a mixture
                  of Gaussians model for density estimation and on
                  binary linear classification, with both synthetic
                  data sets for visualization and several real data
                  sets. Our results show significant reductions in
                  prediction time and memory footprint.}
}

@InCollection{SneGha06,
  cat =		 {gp},
  title =	 {Sparse {G}aussian Processes using Pseudo-inputs},
  author =	 {Edward Snelson and Zoubin Ghahramani},
  booktitle =	 nips18,
  editor =	 {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
  publisher =	 MIT,
  address =	 {Cambridge, MA},
  pages =	 {1257--1264},
  year =	 2006,
  url =		 {http://www.gatsby.ucl.ac.uk/~snelson/SPGP_up.pdf},
  abstract =	 {We present a new Gaussian process (GP) regression
                  model whose covariance is parameterized by the the
                  locations of M pseudo-input points, which we learn
                  by a gradient based optimization. We take
                  M&lt;&lt;N, where N is the number of real data
                  points, and hence obtain a sparse regression method
                  which has O(NM<sup>2</sup>) training cost and
                  O(M<sup>2</sup>) prediction cost per test case. We
                  also find hyperparameters of the covariance function
                  in the same joint optimization. The method can be
                  viewed as a Bayesian regression model with
                  particular input dependent noise. The method turns
                  out to be closely related to several other sparse GP
                  approaches, and we discuss the relation in
                  detail. We finally demonstrate its performance on
                  some large data sets, and make a direct comparison
                  to other sparse GP methods. We show that our method
                  can match full GP performance with small M,
                  i.e. very sparse solutions, and it significantly
                  outperforms other approaches in this regime.}
}

@InProceedings{SneGha06b,
  cat =		 {gp},
  author =	 {Edward Snelson and Zoubin Ghahramani},
  title =	 {Variable noise and dimensionality reduction for
                  sparse {G}aussian processes},
  booktitle =	 uai22,
  editor =	 {R. Dechter and T. S. Richardson},
  publisher =	 {AUAI Press},
  year =	 2006,
  url =
                  {http://www.gatsby.ucl.ac.uk/~snelson/snelson_uai.pdf},
  abstract =	 {The sparse pseudo-input {G}aussian process (SPGP) is
                  a new approximation method for speeding up GP
                  regression in the case of a large number of data
                  points N. The approximation is controlled by the
                  gradient optimization of a small set of M
                  pseudo-inputs, thereby reducing complexity from
                  O(N<sup>3</sup>) to O(NM<sup>2</sup>). One
                  limitation of the SPGP is that this optimization
                  space becomes impractically big for high dimensional
                  data sets. This paper addresses this limitation by
                  performing automatic dimensionality reduction. A
                  projection of the input space to a low dimensional
                  space is learned in a supervised manner, alongside
                  the pseudo-inputs, which now live in this reduced
                  space. The paper also investigates the suitability
                  of the SPGP for modeling data with input-dependent
                  noise. A further extension of the model is made to
                  make it even more powerful in this regard - we learn
                  an uncertainty parameter for each pseudo-input. The
                  combination of sparsity, reduced dimension, and
                  input-dependent noise makes it possible to apply GPs
                  to much larger and more complex data sets than was
                  previously practical. We demonstrate the benefits of
                  these methods on several synthetic and real world
                  problems.}
}

@InProceedings{SneGha07,
  cat =		 {gp},
  author =	 {Edward Snelson and Zoubin Ghahramani},
  title =	 {Local and global sparse {G}aussian process
                  approximations},
  booktitle =	 aistats11,
  editor =	 {M. Meila and X. Shen},
  publisher =	 {Omnipress},
  year =	 2007,
  url =		 {http://www.gatsby.ucl.ac.uk/~snelson/localGP.pdf},
  abstract =	 {{G}aussian process (GP) models are flexible
                  probabilistic nonparametric models for regression,
                  classification and other tasks. Unfortunately they
                  suffer from computational intractability for large
                  data sets. Over the past decade there have been many
                  different approximations developed to reduce this
                  cost. Most of these can be termed global
                  approximations, in that they try to summarize all
                  the training data via a small set of support
                  points. A different approach is that of local
                  regression, where many local experts account for
                  their own part of space. In this paper we start by
                  investigating the regimes in which these different
                  approaches work well or fail. We then proceed to
                  develop a new sparse GP approximation which is a
                  combination of both the global and local
                  approaches. Theoretically we show that it is derived
                  as a natural extension of the framework developed by
                  <a href="/pub/#QuiRas05">Qui{\~n}onero-Candela and
                  Rasmussen</a> for sparse GP approximations. We
                  demonstrate the benefits of the combined
                  approximation on some 1D examples for illustration,
                  and on some large real-world data sets.}
}

@inproceedings{SneRasGha04,
  cat =		 {gp},
  author =	 {Edward Snelson and Carl Edward Rasmussen and Zoubin
                  Ghahramani},
  title =	 {Warped {G}aussian Processes},
  booktitle =	 nips16,
  pages =	 {337--344},
  year =	 2004,
  month =	 {December},
  editor =	 {S.~Thrun and L.~Saul and B.~Sch{\"o}lkopf},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  isbn =	 {0-262-20152-6},
  url =		 {.},
  abstract =	 {We generalise the Gaussian process (GP) framework
                  for regression by learning a nonlinear
                  transformation of the GP outputs. This allows for
                  non-Gaussian processes and non-Gaussian noise. The
                  learning algorithm chooses a nonlinear
                  transformation such that transformed data is
                  well-modelled by a GP. This can be seen as including
                  a preprocessing transformation as an integral part
                  of the probabilistic modelling problem, rather than
                  as an ad-hoc step. We demonstrate on several real
                  regression problems that learning the transformation
                  can lead to significantly better performance than
                  using a regular GP, or a GP with a fixed
                  transformation.}
}

@article{SohGhaXin12,
  cat =		 {bioinf np},
  author =	 {Kyung-Ah Sohn and Zoubin Ghahramani and Eric
                  P. Xing},
  year =	 2012,
  title =	 {Robust estimation of local genetic ancestry in
                  admixed populations using a non-parametric
                  {B}ayesian approach},
  journal =	 {Genetics},
  volume =	 191,
  number =	 4,
  annote =	 {doi: 10.1534/genetics.112.140228},
  abstract =	 {We present a new haplotype-based approach for
                  inferring local genetic ancestry of individuals in
                  an admixed population. Most existing approaches for
                  local ancestry estimation ignore the latent genetic
                  relatedness between ancestral populations and treat
                  them as independent. In this paper, we exploit such
                  information by building an inheritance model that
                  describes both the ancestral populations and the
                  admixed population jointly in a unified
                  framework. Based on an assumption that the common
                  hypothetical founder haplotypes give rise to both
                  the ancestral and admixed population haplotypes, we
                  employ an infinite hidden Markov model to
                  characterize each ancestral population and further
                  extend it to generate the admixed
                  population. Through an effective utilization of the
                  population structural information under a principled
                  nonparametric Bayesian framework, the resulting
                  model is significantly less sensitive to the choice
                  and the amount of training data for ancestral
                  populations than state-of-the-arts algorithms. We
                  also improve the robustness under deviation from
                  common modeling assumptions by incorporating
                  population-specific scale parameters that allow
                  variable recombination rates in different
                  populations. Our method is applicable to an admixed
                  population from an arbitrary number of ancestral
                  populations and also performs competitively in terms
                  of spurious ancestry proportions under general
                  multi-way admixture assumption. We validate the
                  proposed method by simulation under various admixing
                  scenarios and present empirical analysis results on
                  worldwide distributed dataset from Human Genome
                  Diversity Project.},
  url =		 {.}
}

@inproceedings{SolMurLeietal03,
  cat =		 {gp},
  author =	 {Ercan Solak and Roderick Murray-Smith and William
                  E.~Leithead and Douglas Leith and Carl Edward
                  Rasmussen},
  title =	 {Derivative observations in {G}aussian Process models
                  of dynamic systems},
  booktitle =	 nips15,
  pages =	 {1033--1040},
  month =	 {December},
  year =	 2003,
  address =	 {Cambridge, MA, USA},
  editor =	 {S.~Becker and S.~Thrun and K.~Obermayer},
  publisher =	 mit,
  url =		 {.},
  abstract =	 {Gaussian processes provide an approach to
                  nonparametric modelling which allows a
                  straightforward combination of function and
                  derivative observations in an empirical model. This
                  is of particular importance in identification of
                  nonlinear dynamic systems from experimental data. 1)
                  It allows us to combine derivative information, and
                  associated uncertainty with normal function
                  observations into the learning and inference
                  process.  This derivative information can be in the
                  form of priors specified by an expert or identified
                  from perturbation data close to equilibrium. 2) It
                  allows a seamless fusion of multiple local linear
                  models in a consistent manner, inferring consistent
                  models and ensuring that integrability constraints
                  are met. 3) It improves dramatically the
                  computational efficiency of Gaussian process models
                  for dynamic system identification, by summarising
                  large quantities of near-equilibrium data by a
                  handful of linearisations, reducing the training set
                  size - traditionally a problem for Gaussian process
                  models.}
}

@article{SonBraOngetal07,
  cat =		 {review},
  author =	 {S{\"o}ren Sonnenburg and Mikio L.~Braun and Cheng
                  Soon Ong and Samy Bengio and Leon Bottou and
                  Geoffrey Holmes and Yann LeCun and Klaus-Robert
                  M{\"u}ller and Fernando Pereira and Carl Edward
                  Rasmussen and Gunnar R{\"a}tsch and Bernhard
                  Sch{\"o}lkopf and Alexander Smola and Pascal Vincent
                  and Jason Weston and Robert C.~Williamson},
  title =	 {The Need for Open Source Software in Machine
                  Learning},
  year =	 2007,
  volume =	 8,
  pages =	 {2443--2466},
  month =	 {October},
  journal =	 jmlr,
  abstract =	 {Open source tools have recently reached a level of
                  maturity which makes them suitable for building
                  large-scale real-world systems. At the same time,
                  the field of machine learning has developed a large
                  body of powerful learning algorithms for diverse
                  applications. However, the true potential of these
                  methods is not realized, since existing
                  implementations are not openly shared, resulting in
                  software with low usability, and weak
                  interoperability. We argue that this situation can
                  be significantly improved by increasing incentives
                  for researchers to publish their software under an
                  open source model. Additionally, we outline the
                  problems authors are faced with when trying to
                  publish algorithmic implementations of machine
                  learning methods. We believe that a resource of peer
                  reviewed software accompanied by short articles
                  would be highly valuable to both the machine
                  learning and the general scientific community.},
  url =
                  {http://www.jmlr.org/papers/volume8/sonnenburg07a/sonnenburg07a.pdf}
}

@inproceedings{SonRoy11,
  cat =		 {approx},
  author =	 {David Sontag and Daniel M. Roy},
  title =	 {The {C}omplexity of {I}nference in {L}atent
                  {D}irichlet {A}llocation},
  booktitle =	 nips24,
  year =	 2011,
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  url =		 {http://danroy.org/papers/SonRoy-NIPS-2011.pdf},
  abstract =	 { We consider the computational complexity of
                  probabilistic inference in Latent Dirichlet
                  Allocation (LDA).  First, we study the problem of
                  finding the maximum a posteriori (MAP) assignment of
                  topics to words, where the document's topic
                  distribution is integrated out.  We show that, when
                  the effective number of topics per document is
                  small, exact inference takes polynomial time. In
                  contrast, we show that, when a document has a large
                  number of topics, finding the MAP assignment of
                  topics to words in LDA is NP-hard.  Next, we
                  consider the problem of finding the MAP topic
                  distribution for a document, where the topic-word
                  assignments are integrated out.  We show that this
                  problem is also NP-hard.  Finally, we briefly
                  discuss the problem of sampling from the posterior,
                  showing that this is NP-hard in one restricted
                  setting, but leaving open the general question. }
}

@inproceedings{SpeHeiGrg18,
  cat =		 {fairness},
  title =	 {A Unified Approach to Quantifying Algorithmic
                  Unfairness: Measuring Individual and Group
                  Unfairness via Inequality Indices},
  booktitle =	 {KDD},
  year =	 2018,
  author =	 {Till Speicher and Hoda Heidari and Nina Grgic-Hlaca
                  and Krishna P. Gummadi and Adish Singla and Adrian
                  Weller and Muhammad Bilal Zafar},
  abstract =	 {Discrimination via algorithmic decision making has
                  received considerable attention. Prior work largely
                  focuses on defining conditions for fairness, but
                  does not define satisfactory measures of algorithmic
                  unfairness. In this paper, we focus on the following
                  question: Given two unfair algorithms, how should we
                  determine which of the two is more unfair? Our core
                  idea is to use existing inequality indices from
                  economics to measure how unequally the outcomes of
                  an algorithm benefit different individuals or groups
                  in a population.  Our work offers a justified and
                  general framework to compare and contrast the
                  (un)fairness of algorithmic predictors. This
                  unifying approach enables us to quantify unfairness
                  both at the individual and the group level. Further,
                  our work reveals overlooked tradeoffs between
                  different fairness notions: using our proposed
                  measures, the overall individual-level unfairness of
                  an algorithm can be decomposed into a between-group
                  and a within-group component. Earlier methods are
                  typically designed to tackle only between-group
                  unfairness, which may be justified for legal or
                  other reasons. However, we demonstrate that
                  minimizing exclusively the between-group component
                  may, in fact, increase the within-group, and hence
                  the overall unfairness. We characterize and
                  illustrate the tradeoffs between our measures of
                  (un)fairness and the prediction accuracy.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/KDD2018_inequality_indices.pdf}
}

@inproceedings{StaBroMazetal21,
  cat =		 {deep},
  author =	 {Megan Stanley and John Bronskill and Krzysztof
                  Maziarz and Hubert Misztela and Jessica Lanini and
                  Marwin Segler and Nadine Schneider and Marc
                  Brockschmidt},
  title =	 {Fs-mol: A few-shot learning dataset of molecules},
  booktitle =	 {Thirty-fifth Conference on Neural Information
                  Processing Systems Datasets and Benchmarks Track
                  (Round 2)},
  year =	 2021,
  url =		 {https://openreview.net/pdf?id=701FtuyLlAd},
  abstract =	 {Small datasets are ubiquitous in drug discovery as
                  data generation is expensive and can be restricted
                  for ethical reasons (eg in vivo experiments). A
                  widely applied technique in early drug discovery to
                  identify novel active molecules against a protein
                  target is modelling quantitative structure-activity
                  relationships (QSAR). It is known to be extremely
                  challenging, as available measurements of compound
                  activities range in the low dozens or
                  hundreds. However, many such related datasets exist,
                  each with a small number of datapoints, opening up
                  the opportunity for few-shot learning after
                  pre-training on a substantially larger corpus of
                  data. At the same time, many few-shot learning
                  methods are currently evaluated in the
                  computer-vision domain.  We propose that expansion
                  into a new application, as well as the possibility
                  to use explicitly graph-structured data, will drive
                  exciting progress in few-shot learning. Here, we
                  provide a few-shot learning dataset (FS-Mol) and
                  complementary benchmarking procedure.  We define a
                  set of tasks on which few-shot learning methods can
                  be evaluated, with a separate set of tasks for use
                  in pre-training. In addition, we implement and
                  evaluate a number of existing single-task,
                  multi-task, and meta-learning approaches as
                  baselines for the community.  We hope that our
                  dataset, support code release, and baselines will
                  encourage future work on this extremely challenging
                  new domain for few-shot learning.}
}

@article{Ste14,
  title =	 {Compressing Sets and Multisets of Sequences},
  author =	 {Christian Steinruecken},
  year =	 2015,
  month =	 mar,
  journal =	 {{IEEE} Transactions on Information Theory},
  volume =	 61,
  number =	 3,
  pages =	 {1485--1490},
  doi =		 {10.1109/TIT.2015.2392093},
  issn =	 {0018-9448},
  publisher =	 {IEEE},
  url =		 {.},
  note =	 {A previous version was published at the Data
                  Compression Conference 2014.},
  abstract =	 { This article describes lossless compression
                  algorithms for multisets of sequences, taking
                  advantage of the multiset's unordered structure.
                  Multisets are a generalisation of sets where members
                  are allowed to occur multiple times.  A multiset can
                  be encoded naively by simply storing its elements in
                  some sequential order, but then information is
                  wasted on the ordering.  We propose a technique that
                  transforms the multiset into an order-invariant tree
                  representation, and derive an arithmetic code that
                  optimally compresses the tree. Our method achieves
                  compression even if the sequences in the multiset
                  are individually incompressible (such as
                  cryptographic hash sums). The algorithm is
                  demonstrated practically by compressing collections
                  of SHA-1 hash sums, and multisets of arbitrary,
                  individually encodable objects. },
}

@inproceedings{Ste15,
  title =	 {Improving {PPM} with dynamic parameter updates},
  author =	 {Christian Steinruecken and Zoubin Ghahramani and
                  David MacKay},
  year =	 2015,
  month =	 apr,
  location =	 {Snowbird, UT, USA},
  booktitle =	 {Proceedings of the Data Compression Conference},
  issn =	 {1068-0314},
  editor =	 {Ali Bilgin and Michael W. Marcellin and Joan
                  Serra-Sagrist\`a and James A. Storer},
  publisher =	 {IEEE Computer Society},
  url =		 {http://cstein.kings.cam.ac.uk/~chris/ppm-dp.pdf},
  abstract =	 { This article makes several improvements to the
                  classic PPM algorithm, resulting in a new algorithm
                  with superior compression effectiveness on human
                  text.  The key differences of our algorithm to
                  classic PPM are that (A) rather than the original
                  escape mechanism, we use a generalised blending
                  method with explicit hyper-parameters that control
                  the way symbol counts are combined to form
                  predictions; (B) different hyper-parameters are used
                  for classes of different contexts; and (C) these
                  hyper-parameters are updated dynamically using
                  gradient information.  The resulting algorithm
                  (PPM-DP) compresses human text better than all
                  currently published variants of PPM, CTW, DMC, LZ,
                  CSE and BWT, with runtime only slightly slower than
                  classic PPM. },
}

@misc{Ste16,
  title =	 {Compressing combinatorial objects},
  author =	 {Christian Steinruecken},
  year =	 2016,
  month =	 jan,
  eprint =	 {arXiv:1601.03689 [cs.IT]},
  howpublished = {{\tt arXiv:1401.03689 [cs.IT]}},
  url =		 {http://arxiv.org/abs/1601.03689},
  abstract =	 { Most of the world's digital data is currently
                  encoded in a sequential form, and compression
                  methods for sequences have been studied
                  extensively. However, there are many types of
                  non-sequential data for which good compression
                  techniques are still largely unexplored. This paper
                  contributes insights and concrete techniques for
                  compressing various kinds of non-sequential data via
                  arithmetic coding, and derives re-usable
                  probabilistic data models from fairly generic
                  structural assumptions. Near-optimal compression
                  methods are described for certain types of
                  permutations, combinations and multisets; and the
                  conditions for optimality are made explicit for each
                  method.  },
}

@article{SteDenCooetal10,
  cat =		 {bioinf},
  author =	 {O. Stegle and K. J. Denby and E. J. Cooke and
                  D. L. Wild and Z. Ghahramani and K. M. Borgwardt},
  year =	 2010,
  title =	 {A robust {Bayesian} two-sample test for detecting
                  intervals of differential gene expression in
                  microarray time series},
  journal =	 {Journal of Computational Biology},
  volume =	 17,
  number =	 3,
  pages =	 {1--13},
  doi =		 {10.1089/cmb.2009.0175},
  url =
                  {http://www.liebertonline.com/doi/abs/10.1089/cmb.2009.0175},
  abstract =	 {Understanding the regulatory mechanisms that are
                  responsible for an organism's response to
                  environmental change is an important issue in
                  molecular biology. A first and important step
                  towards this goal is to detect genes whose
                  expression levels are affected by altered external
                  conditions. A range of methods to test for
                  differential gene expression, both in static as well
                  as in time-course experiments, have been
                  proposed. While these tests answer the question
                  \emph{whether} a gene is differentially expressed,
                  they do not explicitly address the question
                  \emph{when} a gene is differentially expressed,
                  although this information may provide insights into
                  the course and causal structure of regulatory
                  programs. In this article, we propose a twosample
                  test for identifying intervals of differential gene
                  expression in microarray time series.  Our approach
                  is based on Gaussian process regression, can deal
                  with arbitrary numbers of replicates, and is robust
                  with respect to outliers. We apply our algorithm to
                  study the response of \emph{Arabidopsis thaliana}
                  genes to an infection by a fungal pathogen using a
                  microarray time series dataset covering 30,336 gene
                  probes at 24 observed time points. In classification
                  experiments, our test compares favorably with
                  existing methods and provides additional insights
                  into time-dependent differential expression.}
}

@inproceedings{SteDenMcHetal09,
  cat =		 {bioinf time},
  booktitle =	 {German Conference on Bioinformatics},
  title =	 {Discovering temporal patterns of differential gene
                  expression in microarray time series},
  author =	 {O. Stegle and K. Denby and S. McHattie and A. Meade
                  and D. Wild and Z. Ghahramani and K Borgwardt},
  year =	 2009,
  month =	 {September},
  address =	 {Halle, Germany},
  pages =	 {133--142},
  url =		 {.},
  abstract =	 {A wealth of time series of microarray measurements
                  have become available over recent years. Several
                  two-sample tests for detecting differential gene
                  expression in these time series have been defined,
                  but they can only answer the question \emph{whether}
                  a gene is differentially expressed across the whole
                  time series, not \emph{in which intervals} it is
                  differentially expressed. In this article, we
                  propose a Gaussian process based approach for
                  studying these dynamics of differential gene
                  expression. In experiments on \emph{Arabidopsis
                  thaliana} gene expression levels, our novel
                  technique helps us to uncover that the family of
                  WRKY transcription factors appears to be involved in
                  the early response to infection by a fungal
                  pathogen.}
}

@inproceedings{SteDenWiletal09,
  cat =		 {bioinf},
  booktitle =	 {13th Annual International Conference on Research in
                  Computational Molecular Biology (RECOMB 2009)},
  title =	 {A robust {Bayesian} two-sample test for detecting
                  intervals of differential gene expression in
                  microarray time series},
  author =	 {O.~Stegle and K.~Denby and David L.~Wild and Zoubin
                  Ghahramani and Karsten Borgwardt},
  year =	 2009,
  pages =	 {201--216},
  address =	 {Tucson, AZ, USA},
  publisher =	 {Springer-Verlag},
  series =	 {Lecture Notes in Bioinformatics},
  volume =	 5541,
  isbn =	 {978-3-642-02007-0},
  doi =		 {10.1007/978-3-642-02008-7_14},
  url =		 {.},
  abstract =	 {Understanding the regulatory mechanisms that are
                  responsible for an organism's response to
                  environmental changes is an important question in
                  molecular biology. A first and important step
                  towards this goal is to detect genes whose
                  expression levels are affected by altered external
                  conditions. A range of methods to test for
                  differential gene expression, both in static as well
                  as in time-course experiments, have been
                  proposed. While these tests answer the question
                  \emph{whether} a gene is differentially expressed,
                  they do not explicitly address the question
                  \emph{when} a gene is differentially expressed,
                  although this information may provide insights into
                  the course and causal structure of regulatory
                  programs.  In this article, we propose a two-sample
                  test for identifying \emph{intervals} of
                  differential gene expression in microarray time
                  series. Our approach is based on Gaussian process
                  regression, can deal with arbitrary numbers of
                  replicates and is robust with respect to
                  outliers. We apply our algorithm to study the
                  response of \emph{Arabidopsis thaliana} genes to an
                  infection by a fungal pathogen using a microarray
                  time series dataset covering 30,336 gene probes at
                  24 time points. In classification experiments our
                  test compares favorably with existing methods and
                  provides additional insights into time-dependent
                  differential expression.}
}

@inproceedings{SteGha12,
  cat =		 {np},
  author =	 {Jacob Steinhardt and Zoubin Ghahramani},
  year =	 2012,
  title =	 {Flexible Martingale Priors for Deep Hierarchies},
  booktitle =	 aistats15,
  abstract =	 {When building priors over trees for Bayesian
                  hierarchical models, there is a tension between
                  maintaining desirable theoretical properties such as
                  infinite exchangeability and important practical
                  properties such as the ability to increase the depth
                  of the tree to accommodate new data. We resolve this
                  tension by presenting a family of infinitely
                  exchangeable priors over discrete tree structures
                  that allows the depth of the tree to grow with the
                  data, and then showing that our family contains all
                  hierarchical models with certain mild symmetry
                  properties. We also show that deep hierarchical
                  models are in general intimately tied to a process
                  called a martingale, and use Doob’s martingale
                  convergence theorem to demonstrate some unexpected
                  properties of deep hierarchies.},
  url =		 {.}
}

@inproceedings{SteGhaGoretal09,
  cat =		 {np time},
  volume =	 5,
  month =	 {April},
  author =	 {T. Stepleton and Z. Ghahramani and G.~Gordon and
                  T.-S.~Lee},
  note =	 {ISSN 1938-7228},
  booktitle =	 aistats12,
  editor =	 {D. van Dyk and M. Welling},
  title =	 {The block diagonal infinite hidden {M}arkov model},
  publisher =	 {Microtome Publishing (paper) Journal of Machine
                  Learning Research},
  year =	 2009,
  pages =	 {552--559},
  address =	 {Clearwater Beach, FL, USA},
  url =		 {.},
  abstract =	 {The Infinite Hidden Markov Model (IHMM) extends
                  hidden Markov models to have a countably infinite
                  number of hidden states (<a
                  href="/pub/#BeaGhaRas02">Beal et al., 2002</a>; Teh
                  et al., 2006). We present a generalization of this
                  framework that introduces nearly block-diagonal
                  structure in the transitions between the hidden
                  states, where blocks correspond to "sub-behaviors"
                  exhibited by data sequences. In identifying such
                  structure, the model classifies, or partitions,
                  sequence data according to these sub-behaviors in an
                  unsupervised way. We present an application of this
                  model to artificial data, a video gesture
                  classification task, and a musical theme labeling
                  task, and show that components of the model can also
                  be applied to graph segmentation.}
}

@inproceedings{SunBanCho05a,
  author =	 {Sung, JaeMo and Bang, Sung Yang and Choi, Seungjin
                  and Ghahramani, Zoubin},
  booktitle =	 {ECML},
  editor =	 {Gama, Jo{\~a}o and Camacho, Rui and Brazdil, Pavel
                  and Jorge, Al\'{\i}pio and Torgo, Lu\'{\i}s},
  isbn =	 {3-540-29243-8},
  pages =	 {377-388},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {U-Likelihood and U-Updating Algorithms: Statistical
                  Inference in Latent Variable Models},
  url =		 {.},
  volume =	 3720,
  year =	 2005,
  abstract =	 {In this paper we consider latent variable models and
                  introduce a new U-likelihood concept for estimating
                  the distribution over hidden variables. One can
                  derive an estimate of parameters from this
                  distribution. Our approach differs from the Bayesian
                  and Maximum Likelihood (ML) approaches. It gives an
                  alternative to Bayesian inference when we don't want
                  to define a prior over parameters and gives an
                  alternative to the ML method when we want a better
                  estimate of the distribution over hidden
                  variables. As a practical implementation, we present
                  a U-updating algorithm based on the mean field
                  theory to approximate the distribution over hidden
                  variables from the U-likelihood. This algorithm
                  captures some of the correlations among hidden
                  variables by estimating reaction terms. Those
                  reaction terms are found to penalize the
                  likelihood. We show that the U-updating algorithm
                  becomes the EM algorithm as a special case in the
                  large sample limit. The useful behavior of our
                  method is confirmed for the case of mixture of
                  Gaussians by comparing to the EM algorithm.}
}

@article{SunGhaBan08,
  cat =		 {approx},
  volume =	 30,
  number =	 12,
  month =	 {November},
  author =	 {J.M. Sung and Z. Ghahramani and S.Y. Bang},
  title =	 {Latent space variational {Bayes}},
  publisher =	 {IEEE},
  year =	 2008,
  journal =	 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  pages =	 {2236--2242},
  url =		 {.},
  abstract =	 {Variational Bayesian Expectation-Maximization
                  (VBEM), an approximate inference method for
                  probabilistic models based on factorizing over
                  latent variables and model parameters, has been a
                  standard technique for practical Bayesian
                  inference. In this paper, we introduce a more
                  general approximate inference framework for
                  conjugate-exponential family models, which we call
                  Latent-Space Variational Bayes (LSVB). In this
                  approach, we integrate out the model parameters in
                  an exact way, leaving only the latent variables. It
                  can be shown that the LSVB approach gives better
                  estimates of the model evidence as well as the
                  distribution over the latent variables than the VBEM
                  approach, but, in practice, the distribution over
                  the latent variables has to be approximated. As a
                  practical implementation, we present a First-order
                  LSVB (FoLSVB) algorithm to approximate the
                  distribution over the latent variables. From this
                  approximate distribution, one can also estimate the
                  model evidence and the posterior over the model
                  parameters. The FoLSVB algorithm is directly
                  comparable to the VBEM algorithm and has the same
                  computational complexity. We discuss how LSVB
                  generalizes the recently proposed collapsed
                  variational methods to general conjugate-exponential
                  families. Examples based on mixtures of Gaussians
                  and mixtures of Bernoullis with synthetic and
                  real-world data sets are used to illustrate some
                  advantages of our method over VBEM.}
}

@article{SunGhaBan08b,
  cat =		 {approx},
  volume =	 15,
  month =	 {December},
  author =	 {J.M. Sung and Z. Ghahramani and S.Y. Bang},
  title =	 {Second-order latent space variational {Bayes} for
                  approximate {B}ayesian inference},
  publisher =	 {IEEE},
  journal =	 {IEEE Signal Processing Letters},
  pages =	 {918--921},
  year =	 2008,
  url =		 {.},
  abstract =	 {In this letter, we consider a variational
                  approximate Bayesian inference framework,
                  latent-space variational Bayes (LSVB), in the
                  general context of conjugate-exponential family
                  models with latent variables. In the LSVB approach,
                  we integrate out model parameters in an exact way
                  and then perform the variational inference over only
                  the latent variables. It can be shown that LSVB can
                  achieve better estimates of the model evidence as
                  well as the distribution over the latent variables
                  than the popular variational Bayesian
                  expectation-maximization (VBEM). However, the
                  distribution over the latent variables in LSVB has
                  to be approximated in practice. As an approximate
                  implementation of LSVB, we propose a second-order
                  LSVB (SoLSVB) method. In particular, VBEM can be
                  derived as a special case of a first-order
                  approximation in LSVB. SoLSVB can capture higher
                  order statistics neglected in VBEM and can therefore
                  achieve a better approximation. Examples of Gaussian
                  mixture models are used to illustrate the comparison
                  between our method and VBEM, demonstrating the
                  improvement.}
}

@misc{Tangemannetal21,
  cat =		 {causal mvision},
  title =	 {Unsupervised Object Learning via Common Fate},
  author =	 {Tangemann, M. and Schneider, S. and von
                  K{\"u}gelgen, J. and Locatello, F. and Gehler,
                  P. and Brox, T. and K{\"u}mmerer, M. and Bethge,
                  M. and Sch{\"o}lkopf, B.},
  year =	 2021,
  url =		 {https://arxiv.org/abs/2110.06562},
  abstract =	 {Learning generative object models from unlabelled
                  videos is a long standing problem and required for
                  causal scene modeling. We decompose this problem
                  into three easier subtasks, and provide candidate
                  solutions for each of them. Inspired by the Common
                  Fate Principle of Gestalt Psychology, we first
                  extract (noisy) masks of moving objects via
                  unsupervised motion segmentation. Second, generative
                  models are trained on the masks of the background
                  and the moving objects, respectively. Third,
                  background and foreground models are combined in a
                  conditional "dead leaves" scene model to sample
                  novel scene configurations where occlusions and
                  depth layering arise naturally. To evaluate the
                  individual stages, we introduce the Fishbowl dataset
                  positioned between complex real-world scenes and
                  common object-centric benchmarks of simplistic
                  objects. We show that our approach allows learning
                  generative models that generalize beyond the
                  occlusions present in the input videos, and
                  represent scenes in a modular fashion that allows
                  sampling plausible scenes outside the training
                  distribution by permitting, for instance, object
                  numbers or densities not observed in the training
                  set.},
}

@phdthesis{Teb22,
  title =	 { Advances in Software and Spatio-Temporal Modelling
                  with Gaussian Processes },
  author =	 {Tebbutt, Will},
  year =	 2022,
  url =		 {https://willtebbutt.github.io/resources/thesis.pdf},
  cat =		 {gp approx},
  school =	 {University of Cambridge, Department of Engineering},
  abstract =	 { This thesis concerns the use of Gaussian processes
                  (GPs) as distributions over unknown functions in
                  Machine Learning and probabilistic modeling. GPs
                  have been found to have utility in a wide range of
                  applications owing to their flexibility,
                  interpretability, and tractability. I advance their
                  use in three directions.  Firstly, the abstractions
                  upon which software is built for their use in
                  practice. In modern GP software libraries such as
                  GPML, GPy, GPflow, and GPyTorch, the kernel is
                  undoubtedly the dominant abstraction. While it
                  remains highly successful it of course has
                  limitations, and I propose to address some of these
                  through a complementary abstraction: affine
                  transformations of GPs.  Specifically I show how a
                  collection of GPs, and affine transformations
                  thereof, can themselves be treated as a single GP.
                  This in turn leads to a design for software,
                  including exact and approximate inference
                  algorithms. I demonstrate the utility of this
                  software through a collection of worked examples,
                  focussing on models which are more cleanly and
                  easily expressed using this new software.  Secondly,
                  I develop a new scalable approximate inference
                  algorithm for a class of GPs commonly utilised in
                  spatio-temporal problems.  This is a setting in
                  which GPs excel, for example enabling the
                  incorporation of important inductive biases, and
                  observations made at arbitrary points in time and
                  space. However, the computation required to perform
                  exact inference and learning in GPs scales cubically
                  in the number of observations, necessitating
                  approximation, to which end I combine two important
                  complementary classes of approximation: pseudo-point
                  and Markovian. The key contribution is the insight
                  that a simple and useful way to combine them turns
                  out to be well-justified. This resolves an open
                  question in the literature, provides new insight
                  into existing work, and a new family of
                  approximations. The efficacy of an important member
                  of this family is demonstrated empirically.  Finally
                  I develop a GP model and associated approximate
                  inference techniques for the prediction of sea
                  surface temperatures (SSTs) on decadal time scales,
                  which are relevant when taking planning decisions
                  which consider resilience to climate change. There
                  remains a large degree of uncertainty as to the
                  state of the climate on such time scales, but it is
                  thought to be possible to reduce this by exploiting
                  the predictability of natural variability in the
                  climate. The developed GP-based model incorporates a
                  key assumption used by the existing statistical
                  models employed for decadal prediction, thus
                  retaining a valuable inductive bias, while offering
                  several advantages. Amongst these is the lack of
                  need for spatial aggregation of data, which is
                  especially relevant when data are sparse, as is the
                  case with historical ocean SST data.  In summary,
                  this thesis contributes to the practical use of GPs
                  through a set of abstractions that are useful in the
                  design of software, algorithms for approximate
                  inference in spatial-temporal settings, and their
                  use in decadal climate prediction.  }
}

@inproceedings{TebSolTur21,
  title =	 { Combining pseudo-point and state space
                  approximations for sum-separable {G}aussian
                  Processes },
  author =	 {Tebbutt, Will and Solin, Arno and Turner, Richard
                  E.},
  year =	 2021,
  booktitle =	 { Proceedings of the Thirty-Seventh Conference on
                  Uncertainty in Artificial Intelligence },
  publisher =	 {PMLR},
  series =	 {Proceedings of Machine Learning Research},
  pages =	 {1607--1617},
  url =		 {https://proceedings.mlr.press/v161/tebbutt21a.html},
  editor =	 {de Campos, Cassio and Maathuis, Marloes H.},
  cat =		 {gp approx},
  abstract =	 { Gaussian processes (GPs) are important
                  probabilistic tools for inference and learning in
                  spatio-temporal modelling problems such as those in
                  climate science and epidemiology. However, existing
                  GP approximations do not simultaneously support
                  large numbers of off-the-grid spatial data-points
                  and long time-series which is a hallmark of many
                  applications. Pseudo-point approximations, one of
                  the gold-standard methods for scaling GPs to large
                  data sets, are well suited for handling off-the-grid
                  spatial data. However, they cannot handle long
                  temporal observation horizons effectively reverting
                  to cubic computational scaling in the time
                  dimension.  State space GP approximations are well
                  suited to handling temporal data, if the temporal GP
                  prior admits a Markov form, leading to linear
                  complexity in the number of temporal observations,
                  but have a cubic spatial cost and cannot handle
                  off-the-grid spatial data.  In this work we show
                  that there is a simple and elegant way to combine
                  pseudo-point methods with the state space GP
                  approximation framework to get the best of both
                  worlds. The approach hinges on a surprising
                  conditional independence property which applies to
                  space–time separable GPs. We demonstrate empirically
                  that the combined approach is more scalable and
                  applicable to a greater range of spatio-temporal
                  problems than either method on its own.  }
}

@inproceedings{TehGorGha07a,
  author =	 {Teh, Yee Whye and G{\"o}r{\"u}r, Dilan and
                  Ghahramani, Zoubin},
  booktitle =	 {AISTATS},
  cat =		 {np},
  editor =	 {Meila, Marina and Shen, Xiaotong},
  pages =	 {556-563},
  publisher =	 {JMLR.org},
  series =	 {JMLR Proceedings},
  title =	 {Stick-breaking Construction for the {I}ndian Buffet
                  Process},
  url =		 {.},
  volume =	 2,
  year =	 2007,
  abstract =	 {The Indian buffet process (IBP) is a Bayesian
                  nonparametric distribution whereby objects are
                  modelled using an unbounded number of latent
                  features. In this paper we derive a stick-breaking
                  representation for the IBP. Based on this new
                  representation, we develop slice samplers for the
                  IBP that are efficient, easy to implement and are
                  more generally applicable than the currently
                  available Gibbs sampler. This representation, along
                  with the work of Thibaux and Jordan, also
                  illuminates interesting theoretical connections
                  between the IBP, Chinese restaurant processes, Beta
                  processes and Dirichlet processes. }
}

@inproceedings{TenGao19,
  cat =		 {deep approx},
  title =	 {Leader stochastic gradient descent ({LSGD}) for
                  distributed training of deep learning models},
  author =	 {Yunfei Teng and Wenbo Gao and Francois Chalus and
                  Anna Choromanska and Donald Goldfarb and Adrian
                  Weller},
  booktitle =	 nips33,
  year =	 2019,
  month =	 {December},
  address =	 {Vancouver},
  url =		 {https://arxiv.org/abs/1905.10395},
  abstract =	 {We consider distributed optimization under
                  communication constraints for training deep learning
                  models. We propose a new algorithm, whose parameter
                  updates rely on two forces: a regular gradient step,
                  and a corrective direction dictated by the currently
                  best-performing worker (leader). Our method differs
                  from the parameter-averaging scheme EASGD in a
                  number of ways: (i) our objective formulation does
                  not change the location of stationary points
                  compared to the original optimization problem; (ii)
                  we avoid convergence decelerations caused by pulling
                  local workers descending to different local minima
                  to each other (i.e. to the average of their
                  parameters); (iii) our update by design breaks the
                  curse of symmetry (the phenomenon of being trapped
                  in poorly generalizing sub-optimal solutions in
                  symmetric non-convex landscapes); and (iv) our
                  approach is more communication efficient since it
                  broadcasts only parameters of the leader rather than
                  all workers. We provide theoretical analysis of the
                  batch version of the proposed algorithm, which we
                  call Leader Gradient Descent (LGD), and its
                  stochastic variant (LSGD). Finally, we implement an
                  asynchronous version of our algorithm and extend it
                  to the multi-leader setting, where we form groups of
                  workers, each represented by its own local leader
                  (the best performer in a group), and update each
                  worker with a corrective direction comprised of two
                  attractive forces: one to the local, and one to the
                  global leader (the best performer among all
                  workers). The multi-leader setting is well-aligned
                  with current hardware architecture, where local
                  workers forming a group lie within a single
                  computational node and different groups correspond
                  to different nodes. For training convolutional
                  neural networks, we empirically demonstrate that our
                  approach compares favorably to state-of-the-art
                  baselines.}
}

@article{TerBurArt22,
  cat =		 {gp},
  author =	 {Alexander Terenin and David R. Burt and Artemev,
                  Artem and Seth Flaxman and Mark van der Wilk and
                  Carl Edward Rasmussen and Hong Ge},
  journal =	 {arXiv},
  title =	 {Numerically Stable Sparse {G}aussian Processes via
                  Minimum Separation using Cover Trees},
  url =		 {https://arxiv.org/abs/2210.07893},
  year =	 2022,
  abstract =	 {As Gaussian processes mature, they are increasingly
                  being deployed as part of larger machine learning
                  and decision-making systems, for instance in
                  geospatial modeling, Bayesian optimization, or in
                  latent Gaussian models. Within a system, the
                  Gaussian process model needs to perform in a stable
                  and reliable manner to ensure it interacts correctly
                  with other parts the system. In this work, we study
                  the numerical stability of scalable sparse
                  approximations based on inducing points. We derive
                  sufficient and in certain cases necessary conditions
                  on the inducing points for the computations
                  performed to be numerically stable. For
                  low-dimensional tasks such as geospatial modeling,
                  we propose an automated method for computing
                  inducing points satisfying these conditions.  This
                  is done via a modification of the cover tree data
                  structure, which is of independent interest. We
                  additionally propose an alternative sparse
                  approximation for regression with a Gaussian
                  likelihood which trades off a small amount of
                  performance to further improve stability. We
                  evaluate the proposed techniques on a number of
                  examples, showing that, in geospatial settings,
                  sparse approximations with guaranteed numerical
                  stability often perform comparably to those
                  without.}
}

@article{ThrLiuKol04a,
  author =	 {Thrun, S<!>ebastian and Liu, Yufeng and Koller,
                  Daphne and Ng, Andrew Y. and Ghahramani, Zoubin and
                  Durrant-Whyte, Hugh F.},
  cat =		 {rl},
  journal =	 {I. J. Robotic Res.},
  number =	 {7-8},
  pages =	 {693-716},
  title =	 {Simultaneous Localization and Mapping with Sparse
                  Extended Information Filters},
  url =		 {.},
  volume =	 23,
  year =	 2004,
  abstract =	 {This paper describes a scalable algorithm for the
                  simultaneous mapping and localization (SLAM)
                  problem. SLAM is the problem of acquiring a map of a
                  static environment with a mobile robot. The vast
                  majority of SLAM algorithms are based on the
                  extended Kalman filter (EKF). This paper advocates
                  an algorithm that relies on the dual of the EKF, the
                  extended information filter (EIF). We show that when
                  represented in the information form, map posteriors
                  are dominated by a small number of links that tie
                  together nearby features in the map. This insight is
                  developed into a sparse variant of the EIF, called
                  the sparse extended information filters
                  (SEIF). SEIFs represent maps by graphical networks
                  of features that are locally interconnected, where
                  links represent relative information between pairs
                  of nearby features, as well as information about the
                  robot's pose relative to the map. We show that all
                  essential update equations in SEIFs can be executed
                  in constant time, irrespective of the size of the
                  map. We also provide empirical results obtained for
                  a benchmark data set collected in an outdoor
                  environment, and using a multi-robot mapping
                  simulation.  }
}

@inproceedings{TobBuiTur15,
  cat =		 {approx gp time sigproc},
  title =	 {Learning Stationary Time Series using Gaussian
                  Process with Nonparametric Kernels},
  author =	 {Felipe Tobar and Thang D.~Bui and Richard E.~Turner},
  booktitle =	 nips29,
  year =	 2015,
  month =	 {Dec},
  address =	 {Montréal CANADA},
  url =
                  {http://www.cmm.uchile.cl/~ftobar/data/Tobar_Bui_Turner_NIPS2015.pdf},
  abstract =	 {We introduce the Gaussian Process Convolution Model
                  (GPCM), a two-stage nonparametric generative
                  procedure to model stationary signals as the
                  convolution between a continuous-time white-noise
                  process and a continuous-time linear filter drawn
                  from Gaussian process. The GPCM is a continuous-time
                  nonparametricwindow moving average process and,
                  conditionally, is itself a Gaussian process with a
                  nonparametric kernel defined in a probabilistic
                  fashion. The generative model can be equivalently
                  considered in the frequency domain, where the power
                  spectral density of the signal is specified using a
                  Gaussian process.  One of the main contributions of
                  the paper is to develop a novel variational
                  freeenergy approach based on inter-domain inducing
                  variables that efficiently learns the
                  continuous-time linear filter and infers the driving
                  white-noise process. In turn, this scheme provides
                  closed-form probabilistic estimates of the
                  covariance kernel and the noise-free signal both in
                  denoising and prediction scenarios. Additionally,
                  the variational inference procedure provides
                  closed-form expressions for the approximate
                  posterior of the spectral density given the observed
                  data, leading to new Bayesian nonparametric
                  approaches to spectrum estimation. The proposed GPCM
                  is validated using synthetic and real-world
                  signals.}
}

@article{TobDjuMan15,
  cat =		 {gp time sigproc},
  author =	 {Felipe Tobar and Petar M. Djuri\'c and Danilo
                  P. Mandic},
  title =	 {Unsupervised State-Space Modeling Using Reproducing
                  Kernels},
  journal =	 {IEEE Transactions on Signal Processing},
  year =	 2015,
  volume =	 63,
  issue =	 19,
  pages =	 {5210 - 5221},
  abstract =	 {A novel framework for the design of state-space
                  models (SSMs) is proposed whereby the
                  state-transition function of the model is
                  parametrized using reproducing kernels. The nature
                  of SSMs requires learning a latent function that
                  resides in the state space and for which
                  input-output sample pairs are not available, thus
                  prohibiting the use of gradient-based supervised
                  kernel learning.  To this end, we then propose to
                  learn the mixing weights of the kernel estimate by
                  sampling from their posterior density using Monte
                  Carlo methods. We first introduce an offline version
                  of the proposed algorithm, followed by an online
                  version which performs inference on both the
                  parameters and the hidden state through particle
                  filtering. The accuracy of the estimation of the
                  state-transition function is first validated on
                  synthetic data.  Next, we show that the proposed
                  algorithm outperforms kernel adaptive filters in the
                  prediction of real-world time series, while also
                  providing probabilistic estimates, a key advantage
                  over standard methods.},
  url =
                  {http://mlg.eng.cam.ac.uk/fat25/data/KSSM_Tobar_Djuric_Mandic_2015.pdf},
}

@incollection{TobMan15a,
  cat =		 {gp sigproc},
  author =	 {Felipe Tobar and Danilo P. Mandic},
  title =	 {High-Dimensional Kernel Regression: A Guide for
                  Practitioners},
  booktitle =	 {Trends in Digital Signal Processing: A Festschrift
                  in Honour of A.G. Constantinides},
  pages =	 {287--310},
  publisher =	 {CRC Press},
  year =	 2015,
  editor =	 {Y. C. Lim, H. K. Kwan, W.-C. Siu},
  chapter =	 9
}

@article{TobMan15b,
  cat =		 {gp sigproc},
  author =	 {Felipe Tobar and Danilo P. Mandic},
  title =	 {Design of Positive-Definite Quaternion Kernels},
  journal =	 {IEEE Signal Processing Letters},
  year =	 2015,
  volume =	 22,
  issue =	 11,
  pages =	 {2117 - 2121},
  abstract =	 {Quaternion reproducing kernel Hilbert spaces (QRKHS)
                  have been proposed recently and provide a
                  high-dimensional feature space (alternative to the
                  real-valued multikernel approach) for general
                  kernel-learning applications. The current challenge
                  within quaternion-kernel learning is the lack of
                  general quaternion-valued kernels, which are
                  necessary to exploit the full advantages of the
                  QRKHS theory in real-world problems. This letter
                  proposes a novel way to design quaternion-valued
                  kernels, this is achieved by transforming three
                  complex kernels into quaternion ones and then
                  combining their real and imaginary parts. Building
                  on this general construction, our emphasis is on a
                  new quaternion kernel of polynomial features, which
                  is assessed in the prediction of bodysensor networks
                  applications.},
  url =
                  {http://mlg.eng.cam.ac.uk/fat25/data/C2Q_Tobar_Mandic_2015.pdf},
}

@InProceedings{TobTur15,
  cat =		 {time sigproc gp},
  author =	 {Felipe Tobar and Richard E. Turner},
  title =	 {Modelling of Complex Signals using {G}aussian
                  Processes},
  booktitle =	 {Proceedings of the IEEE International Conference on
                  Acoustics, Speech, and Signal Processing (ICASSP)},
  pages =	 {2209 - 2213},
  year =	 2015,
  abstract =	 {In complex-valued signal processing, estimation
                  algorithms require complete knowledge (or accurate
                  estimation) of the second order statistics, this
                  makes Gaussian processes (GP) well suited for
                  modelling complex signals, as they are designed in
                  terms of covariance functions. Dealing with
                  bivariate signals using GPs require four covariance
                  matrices, or equivalently, two complex matrices. We
                  propose a GP-based approach for modelling complex
                  signals, whereby the second-order statistics are
                  learnt through maximum likelihood; in particular,
                  the complex GP approach allows for circularity
                  coefficient estimation in a robust manner when the
                  observed signal is corrupted by (circular) white
                  noise. The proposed model is validated using climate
                  signals, for both circular and noncircular
                  cases. The results obtained open new possibilities
                  for collaboration between the complex signal
                  processing and Gaussian processes communities
                  towards an appealing representation and statistical
                  description of bivariate signals.},
  url =
                  {http://mlg.eng.cam.ac.uk/fat25/data/Tobar_ICASSP2015.pdf},
}

@inproceedings{TomQuaCapLam12,
  cat =		 {mvision},
  title =	 {Beyond Dataset Bias: Multi-task Unaligned Shared
                  Knowledge Transfer},
  author =	 {Tatiana Tommasi and Novi Quadrianto and Barbara
                  Caputo and Christoph Lampert},
  booktitle =	 {11th Asian Conference on Computer Vision},
  year =	 2012,
  url =		 {.},
  abstract =	 {Many visual datasets are traditionally used to
                  analyze the performance of different learning
                  techniques. The evaluation is usually done within
                  each dataset, therefore it is questionable if such
                  results are a reliable indicator of true
                  generalization ability. We propose here an algorithm
                  to exploit the existing data resources when learning
                  on a new multiclass problem. Our main idea is to
                  identify an image representation that decomposes
                  orthogonally into two subspaces: a part specific to
                  each dataset, and a part generic to, and therefore
                  shared between, all the considered source sets.
                  This allows us to use the generic representation as
                  un-biased reference knowledge for a novel
                  classification task. By casting the method in the
                  multi-view setting, we also make it possible to use
                  different features for different databases.  We call
                  the algorithm MUST, Multitask Unaligned Shared
                  knowledge Transfer.  Through extensive experiments
                  on five public datasets, we show that MUST
                  consistently improves the cross-datasets
                  generalization performance.}
}

@inproceedings{Tothetal22,
  cat =		 {active causal},
  title =	 {Active {B}ayesian Causal Inference},
  author =	 {Toth, C. and Lorch, L. and Knoll, C. and Krause,
                  A. and Pernkopf, F. and Peharz*, R. and von
                  K{\"u}gelgen*, J.},
  booktitle =	 nips35,
  publisher =	 {Curran Associates, Inc.},
  year =	 2022,
  note =	 {*shared last author},
  url =		 {https://arxiv.org/abs/2206.02063},
  abstract =	 {Causal discovery and causal reasoning are
                  classically treated as separate and consecutive
                  tasks: one first infers the causal graph, and then
                  uses it to estimate causal effects of
                  interventions. However, such a two-stage approach is
                  uneconomical, especially in terms of actively
                  collected interventional data, since the causal
                  query of interest may not require a fully-specified
                  causal model. From a Bayesian perspective, it is
                  also unnatural, since a causal query (e.g., the
                  causal graph or some causal effect) can be viewed as
                  a latent quantity subject to posterior inference --
                  other unobserved quantities that are not of direct
                  interest (e.g., the full causal model) ought to be
                  marginalized out in this process and contribute to
                  our epistemic uncertainty. In this work, we propose
                  Active Bayesian Causal Inference (ABCI), a
                  fully-Bayesian active learning framework for
                  integrated causal discovery and reasoning, which
                  jointly infers a posterior over causal models and
                  queries of interest. In our approach to ABCI, we
                  focus on the class of causally-sufficient, nonlinear
                  additive noise models, which we model using Gaussian
                  processes. We sequentially design experiments that
                  are maximally informative about our target causal
                  query, collect the corresponding interventional
                  data, and update our beliefs to choose the next
                  experiment. Through simulations, we demonstrate that
                  our approach is more data-efficient than several
                  baselines that only focus on learning the full
                  causal graph. This allows us to accurately learn
                  downstream causal queries from fewer samples while
                  providing well-calibrated uncertainty estimates for
                  the quantities of interest.},
}

@inproceedings{TraMadPehetal17,
  cat =		 {gm ssl},
  title =	 {Safe semi-supervised learning of sum-product
                  networks},
  author =	 {Martin Trapp and Tamas Madl and Robert Peharz and
                  Franz Pernkopf and Robert Trappl},
  abstract =	 {In several domains obtaining class annotations is
                  expensive while at the same time unlabelled data are
                  abundant. While most semi-supervised approaches
                  enforce restrictive assumptions on the data
                  distribution, recent work has managed to learn
                  semi-supervised models in a non-restrictive regime.
                  However, so far such approaches have only been
                  proposed for linear models.  In this work, we
                  introduce semi-supervised parameter learning for
                  Sum-Product Networks (SPNs).  SPNs are deep
                  probabilistic models admitting inference in linear
                  time in number of network edges. Our approach has
                  several advantages, as it (1) allows generative and
                  discriminative semi-supervised learning, (2)
                  guarantees that adding unlabelled data can increase,
                  but not degrade, the performance (safe), and (3) is
                  computationally efficient and does not enforce
                  restrictive assumptions on the data distribution. We
                  show on a variety of data sets that safe
                  semi-supervised learning with SPNs is competitive
                  compared to state-of-the-art and can lead to a
                  better generative and discriminative objective value
                  than a purely supervised approach.},
  url =		 {http://auai.org/uai2017/proceedings/papers/108.pdf},
  booktitle =	 uai33,
  year =	 2017,
  address =	 {Sidney, Australia},
  month =	 {August}
}

@inproceedings{TraPehPerGha19,
  cat =		 {np gm deep},
  title =	 {Bayesian learning of sum-product networks},
  author =	 {Martin Trapp and Robert Peharz and Hong Ge and Franz
                  Pernkopf and Zoubin Ghahramani},
  booktitle =	 nips33,
  month =	 {December},
  year =	 2019,
  address =	 {Vancouver},
  url =
                  {http://papers.nips.cc/paper/8864-bayesian-learning-of-sum-product-networks},
  abstract =	 {Sum-product networks (SPNs) are flexible density
                  estimators and have received significant attention
                  due to their attractive inference properties. While
                  parameter learning in SPNs is well developed,
                  structure learning leaves something to be desired:
                  Even though there is a plethora of SPN structure
                  learners, most of them are somewhat ad-hoc and based
                  on intuition rather than a clear learning
                  principle. In this paper, we introduce a
                  well-principled Bayesian framework for SPN structure
                  learning. First, we decompose the problem into i)
                  laying out a computational graph, and ii) learning
                  the so-called scope function over the graph. The
                  first is rather unproblematic and akin to neural
                  network architecture validation. The second
                  represents the effective structure of the SPN and
                  needs to respect the usual structural constraints in
                  SPN, i.e. completeness and decomposability. While
                  representing and learning the scope function is
                  somewhat involved in general, in this paper, we
                  propose a natural parametrisation for an important
                  and widely used special case of SPNs. These
                  structural parameters are incorporated into a
                  Bayesian model, such that simultaneous structure and
                  parameter learning is cast into monolithic Bayesian
                  posterior inference. In various experiments, our
                  Bayesian SPNs often improve test likelihoods over
                  greedy SPN learners. Further, since the Bayesian
                  framework protects against overfitting, we can
                  evaluate hyper-parameters directly on the Bayesian
                  model score, waiving the need for a separate
                  validation set, which is especially beneficial in
                  low data regimes. Bayesian SPNs can be applied to
                  heterogeneous domains and can easily be extended to
                  nonparametric formulations. Moreover, our Bayesian
                  approach is the first, which consistently and
                  robustly learns SPN structures under missing data.}
}

@inproceedings{TraPehPerRas20,
  cat =		 {gp deep gm},
  title =	 {Deep Structured Mixtures of {G}aussian Processes},
  author =	 {Martin Trapp and Robert Peharz and Franz Pernkopf
                  and Carl Edward Rasmussen},
  booktitle =	 aistats23,
  month =	 {August},
  year =	 2020,
  address =	 {Online},
  url =
                  {http://proceedings.mlr.press/v108/trapp20a/trapp20a.pdf},
  abstract =	 {Gaussian Processes (GPs) are powerful non-parametric
                  Bayesian regression models that allow exact
                  posterior inference, but exhibit high computational
                  and memory costs. In order to improve scalability of
                  GPs, approximate posterior inference is frequently
                  employed, where a prominent class of approximation
                  techniques is based on local GP experts. However,
                  local-expert techniques proposed so far are either
                  not well-principled, come with limited approximation
                  guarantees, or lead to intractable models. In this
                  paper, we introduce deep structured mixtures of GP
                  experts, a stochastic process model which i) allows
                  exact posterior inference, ii) has attractive
                  computational and memory costs, and iii) when used
                  as GP approximation, captures predictive
                  uncertainties consistently better than previous
                  expert-based approximations. In a variety of
                  experiments, we show that deep structured mixtures
                  have a low approximation error and often perform
                  competitive or outperform prior work.}
}

@inproceedings{Traubleetal21b,
  title =	 {Backward-Compatible Prediction Updates: A
                  Probabilistic Approach},
  author =	 {Tr{\"a}uble, F. and von K{\"u}gelgen, J. and
                  Kleindessner, M. and Locatello, F. and
                  Sch{\"o}lkopf, B. and Gehler, P.},
  booktitle =	 nips34,
  pages =	 {116--128},
  editors =	 {M. Ranzato and A. Beygelzimer and Y. Dauphin and
                  P.S. Liang and J. Wortman Vaughan},
  publisher =	 {Curran Associates, Inc.},
  year =	 2021,
  url =
                  {https://papers.nips.cc/paper/2021/file/012d9fe15b2493f21902cd55603382ec-Paper.pdf},
  abstract =	 {When machine learning systems meet real world
                  applications, accuracy is only one of several
                  requirements. In this paper, we assay a
                  complementary perspective originating from the
                  increasing availability of pre-trained and regularly
                  improving state-of-the-art models. While new
                  improved models develop at a fast pace, downstream
                  tasks vary more slowly or stay constant. Assume that
                  we have a large unlabelled data set for which we
                  want to maintain accurate predictions. Whenever a
                  new and presumably better ML models becomes
                  available, we encounter two problems: (i) given a
                  limited budget, which data points should be
                  re-evaluated using the new model?; and (ii) if the
                  new predictions differ from the current ones, should
                  we update? Problem (i) is about compute cost, which
                  matters for very large data sets and models. Problem
                  (ii) is about maintaining consistency of the
                  predictions, which can be highly relevant for
                  downstream applications; our demand is to avoid
                  negative flips, i.e., changing correct to incorrect
                  predictions. In this paper, we formalize the
                  Prediction Update Problem and present an efficient
                  probabilistic approach as answer to the above
                  questions. In extensive experiments on standard
                  classification benchmark data sets, we show that our
                  method outperforms alternative strategies along key
                  metrics for backward-compatible prediction updates.},
}

@inproceedings{TriCheHer22,
  cat =		 {bioinf},
  title =	 {An Evaluation Framework for the Objective Functions
                  of De Novo Drug Design Benchmarks},
  author =	 {Austin Tripp and Wenlin Chen and Jos{\'e} Miguel
                  Hern{\'a}ndez-Lobato},
  booktitle =	 {ICLR 2022 Workshop on Machine Learning for Drug
                  Discovery},
  year =	 2022,
  url =		 {https://openreview.net/forum?id=W1tcNQNG1S},
  abstract =	 {De novo drug design has recently received increasing
                  attention from the machine learning community. It is
                  important that the field is aware of the actual
                  goals and challenges of drug design and the roles
                  that de novo molecule design algorithms could play
                  in accelerating the process, so that algorithms can
                  be evaluated in a way that reflects how they would
                  be applied in real drug design scenarios. In this
                  paper, we propose a framework for critically
                  assessing the merits of benchmarks, and argue that
                  most of the existing de novo drug design benchmark
                  functions are either highly unrealistic or depend
                  upon a surrogate model whose performance is not well
                  characterized. In order for the field to achieve its
                  long-term goals, we recommend that poor benchmarks
                  (especially logP and QED) be deprecated in favour of
                  better benchmarks. We hope that our proposed
                  framework can play a part in developing new de novo
                  drug design benchmarks that are more realistic and
                  ideally incorporate the intrinsic goals of drug
                  design.}
}

@inproceedings{TriGuGeGha17,
  cat =		 {approx gm np},
  title =	 {Particle Gibbs for Infinite Hidden Markov Models},
  author =	 {Nilesh Tripuraneni and Shixiang Gu and Hong Ge and
                  Zoubin Ghahramani},
  booktitle =	 nips29,
  year =	 2015,
  month =	 {May},
  address =	 {Montreal CANADA},
  url =
                  {https://papers.nips.cc/paper/5968-particle-gibbs-for-infinite-hidden-markov-models.pdf},
  abstract =	 {Infinite Hidden Markov Models (iHMM’s) are an
                  attractive, nonparametric gener- alization of the
                  classical Hidden Markov Model which can
                  automatically infer the number of hidden states in
                  the system.  However, due to the
                  infinite-dimensional nature of the transition
                  dynamics, performing inference in the iHMM is
                  difficult.  In this paper, we present an
                  infinite-state Particle Gibbs (PG) algorithm to re-
                  sample state trajectories for the iHMM. The proposed
                  algorithm uses an efficient proposal optimized for
                  iHMMs and leverages ancestor sampling to improve the
                  mixing of the standard PG algorithm. Our algorithm
                  demonstrates significant con- vergence improvements
                  on synthetic and real world data sets.}
}

@inproceedings{TriRowZouTur17,
  author =	 {Nilesh Tripuraneni and Mark Rowland and Zoubin
                  Ghahramani and Richard E. Turner},
  cat =		 {mcmc},
  title =	 {{M}agnetic {H}amiltonian {M}onte {C}arlo},
  booktitle =	 icml34,
  year =	 2017,
  abstract =	 {Hamiltonian Monte Carlo (HMC) exploits Hamiltonian
                  dynamics to construct efficient proposals for Markov
                  chain Monte Carlo (MCMC). In this paper, we present
                  a generalization of HMC which exploits non-canonical
                  Hamiltonian dynamics. We refer to this algorithm as
                  magnetic HMC, since in 3 dimensions a subset of the
                  dynamics map onto the mechanics of a charged
                  particle coupled to a magnetic field. We establish a
                  theoretical basis for the use of non-canonical
                  Hamiltonian dynamics in MCMC, and construct a
                  symplectic, leapfrog-like integrator allowing for
                  the implementation of magnetic HMC. Finally, we
                  exhibit several examples where these non-canonical
                  dynamics can lead to improved mixing of magnetic HMC
                  relative to ordinary HMC.},
  url =
                  {http://proceedings.mlr.press/v70/tripuraneni17a.html},
}

@phdthesis{Tur10a,
  cat =		 {approx time gp sigproc mhearing neuro},
  author =	 {Richard E. Turner},
  title =	 {Statistical Models for Natural Sounds},
  school =	 {Gatsby Computational Neuroscience Unit, UCL},
  year =	 2010,
  abstract =	 {It is important to understand the rich structure of
                  natural sounds in order to solve important tasks,
                  like automatic speech recognition, and to understand
                  auditory processing in the brain. This thesis takes
                  a step in this direction by characterising the
                  statistics of simple natural sounds. We focus on the
                  statistics because perception often appears to
                  depend on them, rather than on the raw waveform. For
                  example the perception of auditory textures, like
                  running water, wind, fire and rain, depends on
                  summary-statistics, like the rate of falling rain
                  droplets, rather than on the exact details of the
                  physical source.  In order to analyse the statistics
                  of sounds accurately it is necessary to improve a
                  number of traditional signal processing methods,
                  including those for amplitude demodulation,
                  time-frequency analysis, and sub-band
                  demodulation. These estimation tasks are ill-posed
                  and therefore it is natural to treat them as
                  Bayesian inference problems. The new probabilistic
                  versions of these methods have several
                  advantages. For example, they perform more
                  accurately on natural signals and are more robust to
                  noise, they can also fill-in missing sections of
                  data, and provide error-bars. Furthermore,
                  free-parameters can be learned from the signal.
                  Using these new algorithms we demonstrate that the
                  energy, sparsity, modulation depth and modulation
                  time-scale in each sub-band of a signal are critical
                  statistics, together with the dependencies between
                  the sub-band modulators. In order to validate this
                  claim, a model containing co-modulated coloured
                  noise carriers is shown to be capable of generating
                  a range of realistic sounding auditory textures.
                  Finally, we explored the connection between the
                  statistics of natural sounds and perception. We
                  demonstrate that inference in the model for auditory
                  textures qualitatively replicates the primitive
                  grouping rules that listeners use to understand
                  simple acoustic scenes. This suggests that the
                  auditory system is optimised for the statistics of
                  natural sounds.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-2010.html},
}

@phdthesis{Tur11,
  cat =		 {gp},
  author =	 {Ryan Darby T<!>urner},
  title =	 {Gaussian Processes for State Space Models and Change
                  Point Detection},
  school =	 {University of Cambridge, Department of Engineering},
  year =	 2011,
  url =		 {.},
  address =	 {Cambridge, UK},
  abstract =	 {This thesis details several applications of Gaussian
                  processes (GPs) for enhanced time series
                  modeling. We first cover different approaches for
                  using Gaussian processes in time series
                  problems. These are extended to the state space
                  approach to time series in two different problems.
                  We also combine Gaussian processes and Bayesian
                  online change point detection (BOCPD) to increase
                  the generality of the Gaussian process time series
                  methods. These methodologies are evaluated on
                  predictive performance on six real world data sets,
                  which include three environmental data sets, one
                  financial, one biological, and one from industrial
                  well drilling.<br> Gaussian processes are capable of
                  generalizing standard linear time series models. We
                  cover two approaches: the Gaussian process time
                  series model (GPTS) and the autoregressive Gaussian
                  process (ARGP).  We cover a variety of methods that
                  greatly reduce the computational and memory
                  complexity of Gaussian process approaches, which are
                  generally cubic in computational complexity.<br> Two
                  different improvements to state space based
                  approaches are covered.  First, Gaussian process
                  inference and learning (GPIL) generalizes linear
                  dynamical systems (LDS), for which the Kalman filter
                  is based, to general nonlinear systems for
                  nonparametric system identification. Second, we
                  address pathologies in the unscented Kalman filter
                  (UKF). We use Gaussian process optimization (GPO) to
                  learn UKF settings that minimize the potential for
                  sigma point collapse.<br> We show how to embed
                  mentioned Gaussian process approaches to time series
                  into a change point framework. Old data, from an old
                  regime, that hinders predictive performance is
                  automatically and elegantly phased out. The
                  computational improvements for Gaussian process time
                  series approaches are of even greater use in the
                  change point framework. We also present a supervised
                  framework learning a change point model when change
                  point labels are available in training.<br> These
                  mentioned methodologies significantly improve
                  predictive performance on the diverse set of data
                  sets selected.}
}

@inproceedings{TurBotGha10,
  cat =		 {time},
  author =	 {Ryan T<!>urner and Steven Bottone and Zoubin
                  Ghahramani},
  title =	 {Fast Online Anomaly Detection Using Scan Statistics},
  booktitle =	 {Machine Learning for Signal Processing (MLSP 2010)},
  year =	 2010,
  address =	 {Kittil\"{a}, Finland},
  month =	 {August},
  pages =	 {385--390},
  isbn =	 {978-1-4244-7876-7},
  editor =	 {Samuel Kaski and David J. Miller and Erkki Oja and
                  Antti Honkela},
  abstract =	 {We present methods to do fast online anomaly
                  detection using scan statistics. Scan statistics
                  have long been used to detect statistically
                  significant bursts of events. We extend the scan
                  statistics framework to handle many practical issues
                  that occur in application: dealing with an unknown
                  background rate of events, allowing for slow natural
                  changes in background frequency, the inverse problem
                  of finding an unusual lack of events, and setting
                  the test parameters to maximize power. We
                  demonstrate its use on real and synthetic data sets
                  with comparison to other methods.},
  url =		 {.}
}

@inproceedings{TurDeiRas09,
  cat =		 {gp time},
  author =	 {Ryan T<!>urner and Marc Peter Deisenroth and Carl
                  Edward Rasmussen},
  title =	 {System Identification in {G}aussian Process
                  Dynamical Systems},
  booktitle =	 {NIPS Workshop on Nonparametric Bayes},
  year =	 2009,
  editor =	 {Dilan G\"or\"ur},
  address =	 {Whistler, BC, Canada},
  month =	 {December},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/rdturner/NLDSposter.pdf">poster</a>.}
}

@inproceedings{TurDeiRas10,
  cat =		 {gp time},
  author =	 {Ryan T<!>urner and Marc Peter Deisenroth and Carl
                  Edward Rasmussen},
  title =	 {State-Space Inference and Learning with {G}aussian
                  Processes},
  booktitle =	 aistats13,
  year =	 2010,
  editor =	 {Yee Whye Teh and Mike Titterington},
  volume =	 9,
  series =	 {W \& CP},
  pages =	 {868--875},
  address =	 {Chia Laguna, Sardinia, Italy},
  month =	 {May 13--15},
  organization = jmlr,
  abstract =	 {State-space inference and learning with Gaussian
                  processes (GPs) is an unsolved problem. We propose a
                  new, general methodology for inference and learning
                  in nonlinear state-space models that are described
                  probabilistically by non-parametric GP models. We
                  apply the expectation maximization algorithm to
                  iterate between inference in the latent state-space
                  and learning the parameters of the underlying GP
                  dynamics model.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/rdturner/NLDSposterAISTATS.pdf">poster</a>.}
}

@inproceedings{TurRas10,
  cat =		 {gp time},
  author =	 {Ryan T<!>urner and Carl Edward Rasmussen},
  title =	 {Model Based Learning of Sigma Points in Unscented
                  {K}alman Filtering},
  booktitle =	 {Machine Learning for Signal Processing (MLSP 2010)},
  year =	 2010,
  address =	 {Kittil\"{a}, Finland},
  pages =	 {178--183},
  month =	 {August},
  isbn =	 {978-1-4244-7876-7},
  editor =	 {Samuel Kaski and David J. Miller and Erkki Oja and
                  Antti Honkela},
  abstract =	 {The unscented Kalman filter (UKF) is a widely used
                  method in control and time series applications. The
                  UKF suffers from arbitrary parameters necessary for
                  a step known as sigma point placement, causing it to
                  perform poorly in nonlinear problems. We show how to
                  treat sigma point placement in a UKF as a learning
                  problem in a model based view.  We demonstrate that
                  learning to place the sigma points correctly from
                  data can make sigma point collapse much less
                  likely. Learning can result in a significant
                  increase in predictive performance over default
                  settings of the parameters in the UKF and other
                  filters designed to avoid the problems of the UKF,
                  such as the GP-ADF. At the same time, we maintain a
                  lower computational complexity than the other
                  methods. We call our method UKF-L.},
  url =		 {.}
}

@article{TurRas12,
  cat =		 {gp time},
  author =	 {Ryan D.~T<!>urner and Carl Edward Rasmussen},
  volume =	 80,
  title =	 {Model based learning of sigma points in unscented
                  {K}alman filtering},
  journal =	 {Neurocomputing},
  pages =	 {47--53},
  year =	 2012,
  url =		 {.},
  doi =		 {10.1016/j.neucom.2011.07.029},
  abstract =	 {The unscented Kalman filter (UKF) is a widely used
                  method in control and time series applications. The
                  UKF suffers from arbitrary parameters necessary for
                  sigma point placement, potentially causing it to
                  perform poorly in nonlinear problems. We show how to
                  treat sigma point placement in a UKF as a learning
                  problem in a model based view. We demonstrate that
                  learning to place the sigma points correctly from
                  data can make sigma point collapse much less
                  likely. Learning can result in a significant
                  increase in predictive performance over default
                  settings of the parameters in the UKF and other
                  filters designed to avoid the problems of the UKF,
                  such as the GP-ADF. At the same time, we maintain a
                  lower computational complexity than the other
                  methods. We call our method UKF-L.}
}

@inproceedings{TurSaaRas09,
  cat =		 {time},
  author =	 {Ryan T<!>urner and Yunus Saat\c{c}i and Carl Edward
                  Rasmussen},
  title =	 {Adaptive Sequential {B}ayesian Change Point
                  Detection},
  booktitle =	 {NIPS Workshop on Temporal Segmentation},
  year =	 2009,
  editor =	 {Za\"{i}d Harchaoui},
  address =	 {Whistler, BC, Canada},
  month =	 {December},
  abstract =	 {Real-world time series are often nonstationary with
                  respect to the parameters of some underlying
                  prediction model (UPM). Furthermore, it is often
                  desirable to adapt the UPM to incoming regime
                  changes as soon as possible, necessitating
                  sequential inference about change point locations. A
                  Bayesian algorithm for online change point detection
                  (BOCPD) has been introduced recently by Adams and
                  MacKay (2007).  In this algorithm, uncertainty about
                  the last change point location is updated
                  sequentially, and is integrated out to make online
                  predictions robust to parameter changes. BOCPD
                  requires a set of fixed hyper-parameters which allow
                  the user to fully specify the hazard function for
                  change points and the prior distribution over the
                  parameters of the UPM.  In practice, finding the
                  "right" hyper-parameters can be quite difficult. We
                  therefore extend BOCPD by introducing
                  hyper-parameter learning, without sacrificing the
                  online nature of the algorithm.  Hyper-parameter
                  learning is performed by optimizing the marginal
                  likelihood of the BOCPD model, a closed-form
                  quantity which can be computed sequentially. We
                  illustrate performance on three real-world
                  datasets.},
  url =		 {.},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/rdturner/BOCPDposter.pdf">poster</a>,
                  <a
                  href="http://mlg.eng.cam.ac.uk/rdturner/BOCPDtalk.pdf">slides</a>.}
}

@inproceedings{TurSah07b,
  cat =		 {time approx mhearing sigproc},
  author =	 {Richard E. Turner and M Sahani},
  title =	 {Probabilistic Amplitude Demodulation},
  booktitle =	 {7th International Conference on Independent
                  Component Analysis and Signal Separation},
  year =	 2007,
  pages =	 {544--551},
  abstract =	 {Auditory scene analysis is extremely
                  challenging. One approach, perhaps that adopted by
                  the brain, is to shape useful representations of
                  sounds on prior knowledge about their statistical
                  structure.  For example, sounds with harmonic
                  sections are common and so time-frequency
                  representations are efficient. Most current
                  representations concentrate on the shorter
                  components. Here, we propose representations for
                  structures on longer time-scales, like the phonemes
                  and sentences of speech. We decompose a sound into a
                  product of processes, each with its own
                  characteristic time-scale. This demodulation cascade
                  relates to classical amplitude demodulation, but
                  traditional algorithms fail to realise the
                  representation fully. A new approach, probabilistic
                  amplitude demodulation, is shown to out-perform the
                  established methods, and to easily extend to
                  representation of a full demodulation cascade.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2007b.html},
}

@inproceedings{TurSah08a,
  cat =		 {time approx sigproc mhearing},
  author =	 {Richard E. Turner and Maneesh Sahani},
  title =	 {Modeling natural sounds with modulation cascade
                  processes},
  booktitle =	 {nips20},
  year =	 2008,
  editor =	 {J. C. Platt and D. Koller and Y. Singer and
                  S. Roweis},
  volume =	 20,
  publisher =	 {mit},
  abstract =	 {Natural sounds are structured on many time-scales. A
                  typical segment of speech, for example, contains
                  features that span four orders of magnitude:
                  Sentences ($\sim1$s); phonemes ($\sim10$−$1$ s);
                  glottal pulses ($\sim 10$−$2$s); and formants ($\sim
                  10$−$3$s).  The auditory system uses information
                  from each of these time-scales to solve complicated
                  tasks such as auditory scene analysis [1]. One route
                  toward understanding how auditory processing
                  accomplishes this analysis is to build
                  neuroscience-inspired algorithms which solve similar
                  tasks and to compare the properties of these
                  algorithms with properties of auditory
                  processing. There is however a discord: Current
                  machine-audition algorithms largely concentrate on
                  the shorter time-scale structures in sounds, and the
                  longer structures are ignored. The reason for this
                  is two-fold. Firstly, it is a difficult technical
                  problem to construct an algorithm that utilises both
                  sorts of information. Secondly, it is
                  computationally demanding to simultaneously process
                  data both at high resolution (to extract short
                  temporal information) and for long duration (to
                  extract long temporal information). The contribution
                  of this work is to develop a new statistical model
                  for natural sounds that captures structure across a
                  wide range of time-scales, and to provide efficient
                  learning and inference algorithms. We demonstrate
                  the success of this approach on a missing data
                  task.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2008a.html},
}

@InProceedings{TurSah10a,
  cat =		 {approx time sigproc mhearing},
  author =	 {Richard E. Turner and Maneesh Sahani},
  title =	 {Statistical inference for single- and multi-band
                  probabilistic amplitude demodulation.},
  booktitle =	 {Proceedings of the IEEE International Conference on
                  Acoustics, Speech, and Signal Processing (ICASSP)},
  pages =	 {5466--5469 },
  year =	 2010,
  abstract =	 {Amplitude demodulation is an ill-posed problem and
                  so it is natural to treat it from a Bayesian
                  viewpoint, inferring the most likely carrier and
                  envelope under probabilistic constraints. One such
                  treatment is Probabilistic Amplitude Demodulation
                  (PAD), which, whilst computationally more intensive
                  than traditional approaches, offers several
                  advantages.  Here we provide methods for estimating
                  the uncertainty in the PAD-derived envelopes and
                  carriers, and for learning free-parameters like the
                  time-scale of the envelope. We show how the
                  probabilistic approach can naturally handle noisy
                  and missing data. Finally, we indicate how to extend
                  the model to signals which contain multiple
                  modulators and carriers.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2010a.html},
}

@incollection{TurSah11a,
  cat =		 {approx time review},
  author =	 {Richard E. Turner and Maneesh Sahani},
  title =	 {Two problems with variational expectation
                  maximisation for time-series models},
  booktitle =	 {Bayesian Time series models},
  pages =	 {109--130},
  publisher =	 {Cambridge University Press},
  year =	 2011,
  editor =	 {D. Barber and T. Cemgil and S. Chiappa},
  chapter =	 5,
  abstract =	 {Variational methods are a key component of the
                  approximate inference and learning toolbox. These
                  methods fill an important middle ground, retaining
                  distributional information about uncertainty in
                  latent variables, unlike maximum a posteriori
                  methods (MAP), and yet generally requiring less
                  computational time than Monte Carlo Markov Chain
                  methods. In particular the variational Expectation
                  Maximisation (vEM) and variational Bayes algorithms,
                  both involving variational optimisation of a
                  free-energy, are widely used in time-series
                  modelling. Here, we investigate the success of vEM
                  in simple probabilistic time-series models. First we
                  consider the inference step of vEM, and show that a
                  consequence of the well-known compactness property
                  of variational inference is a failure to propagate
                  uncertainty in time, thus limiting the usefulness of
                  the retained distributional information. In
                  particular, the uncertainty may appear to be
                  smallest precisely when the approximation is
                  poorest. Second, we consider parameter learning and
                  analytically reveal systematic biases in the
                  parameters found by vEM. Surprisingly, simpler
                  variational approximations (such a mean-field) can
                  lead to less bias than more complicated structured
                  approximations.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2011a.html},
}

@article{TurSah11b,
  cat =		 {gp time mhearing sigproc},
  author =	 {Richard E. Turner and Maneesh Sahani},
  title =	 {Demodulation as Probabilistic Inference},
  journal =	 {Transactions on Audio, Speech and Language
                  Processing},
  year =	 2011,
  volume =	 19,
  issue =	 8,
  pages =	 {2398--2411},
  abstract =	 {Demodulation is an ill-posed problem whenever both
                  carrier and envelope signals are broadband and
                  unknown. Here, we approach this problem using the
                  methods of probabilistic inference. The new
                  approach, called Probabilistic Amplitude
                  Demodulation (PAD), is computationally challenging
                  but improves on existing methods in a number of
                  ways. By contrast to previous approaches to
                  demodulation, it satisfies five key desiderata: PAD
                  has soft constraints because it is probabilistic;
                  PAD is able to automatically adjust to the signal
                  because it learns parameters; PAD is user-steerable
                  because the solution can be shaped by user-specific
                  prior information; PAD is robust to broad-band noise
                  because this is modelled explicitly; and PAD’s
                  solution is self-consistent, empirically satisfying
                  a Carrier Identity property.  Furthermore, the
                  probabilistic view naturally encompasses noise and
                  uncertainty, allowing PAD to cope with missing data
                  and return error bars on carrier and envelope
                  estimates. Finally, we show that when PAD is applied
                  to a bandpass-filtered signal, the stop-band energy
                  of the inferred carrier is minimal, making PAD
                  well-suited to sub-band demodulation.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2011b.html},
}

@incollection{TurSah11c,
  cat =		 {gp time mhearing sigproc},
  title =	 {Probabilistic amplitude and frequency demodulation},
  author =	 {Richard E. Turner and Maneesh Sahani},
  booktitle =	 nips24,
  publisher =	 mit,
  pages =	 {981--989},
  year =	 2011,
  abstract =	 {A number of recent scientific and engineering
                  problems require signals to be decomposed into a
                  product of a slowly varying positive envelope and a
                  quickly varying carrier whose instantaneous
                  frequency also varies slowly over time. Although
                  signal processing provides algorithms for so-called
                  amplitude- and frequency-demodulation (AFD), there
                  are well known problems with all of the existing
                  methods. Motivated by the fact that AFD is
                  ill-posed, we approach the problem using
                  probabilistic inference. The new approach, called
                  probabilistic amplitude and frequency demodulation
                  (PAFD), models instantaneous frequency using an
                  auto-regressive generalization of the von Mises
                  distribution, and the envelopes using Gaussian
                  auto-regressive dynamics with a positivity
                  constraint. A novel form of expectation propagation
                  is used for inference. We demonstrate that although
                  PAFD is computationally demanding, it outperforms
                  previous approaches on synthetic and real signals in
                  clean, noisy and missing data settings.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2011c.html},
}

@inproceedings{TurSah12a,
  cat =		 {sigproc mhearing sigproc},
  author =	 {Richard E. Turner and Maneesh Sahani},
  booktitle =	 {Acoustics, Speech and Signal Processing (ICASSP),
                  2012 IEEE International Conference on},
  abstract =	 {There are many methods for decomposing signals into
                  a sum of amplitude and frequency modulated
                  sinusoids. In this paper we take a new estimation
                  based approach. Identifying the problem as
                  ill-posed, we show how to regularize the solution by
                  imposing soft constraints on the amplitude and phase
                  variables of the sinusoids. Estimation proceeds
                  using a version of Kalman smoothing. We evaluate the
                  method on synthetic and natural, clean and noisy
                  signals, showing that it outperforms previous
                  decompositions, but at a higher computational cost.},
  title =	 {Decomposing signals into a sum of amplitude and
                  frequency modulated sinusoids using probabilistic
                  inference},
  year =	 2012,
  month =	 {march},
  pages =	 {2173--2176},
  doi =		 {10.1109/ICASSP.2012.6288343},
  ISSN =	 {1520-6149},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2012a.html},
}

@article{TurSah2007a,
  cat =		 {time mvision neuro},
  author =	 {Richard E. Turner and Maneesh Sahani},
  title =	 {A Maximum-Likelihood Interpretation for Slow Feature
                  Analysis},
  journal =	 {nc},
  year =	 2007,
  volume =	 19,
  pages =	 {1022--1038},
  number =	 4,
  address =	 {Cambridge, MA, USA},
  doi =		 {http://dx.doi.org/10.1162/neco.2007.19.4.1022},
  issn =	 {0899-7667},
  publisher =	 {MIT Press},
  abstract =	 {The brain extracts useful features from a maelstrom
                  of sensory information, and a fundamental goal of
                  theoretical neuroscience is to work out how it does
                  so. One proposed feature extraction strategy is
                  motivated by the observation that the meaning of
                  sensory data, such as the identity of a moving
                  visual object, is often more persistent than the
                  activation of any single sensory receptor. This
                  notion is embodied in the slow feature analysis
                  (SFA) algorithm, which uses “slowness” as an
                  heuristic by which to extract semantic information
                  from multi-dimensional time-series. Here, we develop
                  a probabilistic interpretation of this algorithm
                  showing that inference and learning in the limiting
                  case of a suitable probabilistic model yield exactly
                  the results of SFA. Similar equivalences have proved
                  useful in interpreting and extending comparable
                  algorithms such as independent component
                  analysis. For SFA, we use the equivalent
                  probabilistic model as a conceptual spring-board,
                  with which to motivate several novel extensions to
                  the algorithm.},
  url =
                  {http://www.gatsby.ucl.ac.uk/~turner/Publications/turner-and-sahani-2007a.html},
}

@article{UedGha02a,
  author =	 {Ueda, Naonori and Ghahramani, Zoubin},
  journal =	 {Neural Networks},
  number =	 10,
  pages =	 {1223-1241},
  title =	 {Bayesian model search for mixture models based on
                  optimizing variational bounds},
  url =		 {.},
  volume =	 15,
  year =	 2002,
  abstract =	 {When learning a mixture model, we suffer from the
                  local optima and model structure determination
                  problems. In this paper, we present a method for
                  simultaneously solving these problems based on the
                  variational Bayesian (VB) framework. First, in the
                  VB framework, we derive an objective function that
                  can simultaneously optimize both model parameter
                  distributions and model structure. Next, focusing on
                  mixture models, we present a deterministic algorithm
                  to approximately optimize the objective function by
                  using the idea of the split and merge operations
                  which we previously proposed within the maximum
                  likelihood framework. Then, we apply the method to
                  mixture of expers (MoE) models to experimentally
                  show that the proposed method can find the optimal
                  number of experts of a MoE while avoiding local
                  maxima. q 2002 Elsevier Science Ltd. All rights
                  reserved.  }
}

@article{UedNakGha00a,
  author =	 {Ueda, Naonori and Nakano, Ryohei and Ghahramani,
                  Zoubin and Hinton, Geoffrey E.},
  cat =		 {clust},
  journal =	 {Neural Computation},
  number =	 9,
  pages =	 {2109-2128},
  title =	 {{SMEM} Algorithm for Mixture Models},
  url =		 {.},
  volume =	 12,
  year =	 2000,
  abstract =	 {We present a split-and-merge
                  expectation-maximization (SMEM) algorithm to
                  overcome the local maxima problem in parameter
                  estimation of finite mixture models. In the case of
                  mixture models, local maxima often involve having
                  too many components of a mixture model in one part
                  of the space and too few in another, widely
                  separated part of the space. To escape from such
                  configurations, we repeatedly perform simultaneous
                  split-and-merge operations using a new criterion for
                  efficiently selecting the split-and-merge
                  candidates. We apply the proposed algorithm to the
                  training of gaussian mixtures and mixtures of factor
                  analyzers using synthetic and real data and show the
                  effectiveness of using the split-and-merge
                  operations to improve the likelihood of both the
                  training data and of held-out test data. We also
                  show the practical usefulness of the proposed
                  algorithm by applying it to image compression and
                  pattern recognition problems.  }
}

@article{UedNakGha00b,
  author =	 {Ueda, Naonori and Nakano, Ryohei and Ghahramani,
                  Zoubin and Hinton, Geoffrey E.},
  cat =		 {clust},
  journal =	 {VLSI Signal Processing},
  number =	 {1-2},
  pages =	 {133-140},
  title =	 {Split and Merge {EM} Algorithm for Improving
                  {G}aussian Mixture Density Estimates},
  url =		 {.},
  volume =	 26,
  year =	 2000,
  abstract =	 {We present a split and merge EM algorithm to
                  overcome the local maximum problem in Gaussian
                  mixture density estimation. Nonglobal maxims often
                  involve having too many Gaussians in one part of the
                  space and too few in another, widely separated part
                  of the space. To escape from such configurations we
                  repeatedly perform split and merge operations using
                  a new criterion for efficiently selecting the split
                  and merge candidates. Experimental results on
                  synthetic and real data show the effectiveness of
                  using the split and merge operations to improve the
                  likelihood of both the training data and of held-out
                  test data }
}

@inproceedings{UedNakGha98a,
  author =	 {Ueda, Naonori and Nakano, Ryohei and Ghahramani,
                  Zoubin and Hinton, Geoffrey E.},
  booktitle =	 {NIPS},
  cat =		 {clust},
  editor =	 {Kearns, Michael J. and Solla, Sara A. and Cohn,
                  David A.},
  isbn =	 {0-262-11245-0},
  pages =	 {599-605},
  publisher =	 {The MIT Press},
  title =	 {{SMEM} Algorithm for Mixture Models},
  url =		 {.},
  year =	 1998,
  abstract =	 {We present a split-and-merge
                  expectation-maximization (SMEM) algorithm to
                  overcome the local maxima problem in parameter
                  estimation of finite mixture models. In the case of
                  mixture models, local maxima often involve having
                  too many components of a mixture model in one part
                  of the space and too few in another, widely
                  separated part of the space. To escape from such
                  configurations, we repeatedly perform simultaneous
                  split-and-merge operations using a new criterion for
                  efficiently selecting the split-and-merge
                  candidates. We apply the proposed algorithm to the
                  training of gaussian mixtures and mixtures of factor
                  analyzers using synthetic and real data and show the
                  effectiveness of using the split- and-merge
                  operations to improve the likelihood of both the
                  training data and of held-out test data. We also
                  show the practical usefulness of the proposed
                  algorithm by applying it to image compression and
                  pattern recognition problems.  }
}

@inproceedings{VanSaaTehGha08,
  cat =		 {np mcmc time},
  abstract =	 {The infinite hidden Markov model is a non-parametric
                  extension of the widely used hidden Markov
                  model. Our paper introduces a new inference
                  algorithm for the infinite Hidden Markov model
                  called beam sampling. Beam sampling combines slice
                  sampling, which limits the number of states
                  considered at each time step to a finite number,
                  with dynamic programming, which samples whole state
                  trajectories efficiently. Our algorithm typically
                  outperforms the Gibbs sampler and is more robust. We
                  present applications of iHMM inference using the
                  beam sampler on changepoint detection and text
                  prediction problems.},
  address =	 {Helsinki, Finland},
  author =	 {Jurgen {Van Gael} and Yunus Saat\c{c}i and Yee-Whye
                  Teh and Zoubin Ghahramani},
  booktitle =	 icml25,
  pages =	 {1088--1095},
  publisher =	 acm,
  title =	 {Beam sampling for the infinite hidden {M}arkov
                  model},
  url =		 {.},
  volume =	 25,
  year =	 2008
}

@inproceedings{VanTehGha08,
  cat =		 {np time},
  author =	 {{Van Gael}, J. and Teh, Y.W. and Ghahramani, Z.},
  booktitle =	 nips21,
  editor =	 {Koller, D. and Schuurmans, D. and Bottou, L. and
                  Bengio, Y.},
  pages =	 {1697--1704},
  address =	 {Cambridge, MA, USA},
  publisher =	 mit,
  title =	 {The infinite factorial hidden {M}arkov model},
  url =		 {.},
  volume =	 21,
  year =	 2008,
  month =	 {December},
  abstract =	 {The infinite factorial hidden Markov model is a
                  non-parametric extension of the factorial hidden
                  Markov model. Our model defines a probability
                  distribution over an infinite number of independent
                  binary hidden Markov chains which together produce
                  an observable sequence of random variables. Central
                  to our model is a new type of non-parametric prior
                  distribution inspired by the Indian Buffet Process
                  which we call the \emph{Indian Buffet Markov
                  Process}.}
}

@inproceedings{VanVlaGha09,
  cat =		 {np,nlp},
  author =	 {{Van Gael}, J. and Vlachos, A. and Ghahramani, Z.},
  booktitle =	 {Proceedings of the 2009 Conference on Empirical
                  Methods in Natural Language Processing (EMNLP)},
  title =	 {The infinite {HMM} for unsupervised {PoS} tagging},
  pages =	 {678--687},
  isbn =	 {978-1-932432-62-6},
  url =		 {.},
  year =	 2009,
  month =	 {August},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Singapore},
  abstract =	 {We extend previous work on fully unsupervised
                  part-of-speech tagging. Using a non-parametric
                  version of the HMM, called the infinite HMM (iHMM),
                  we address the problem of choosing the number of
                  hidden states in unsupervised Markov models for PoS
                  tagging. We experiment with two non-parametric
                  priors, the Dirichlet and Pitman-Yor processes, on
                  the Wall Street Journal dataset using a parallelized
                  implementation of an iHMM inference algorithm. We
                  evaluate the results with a variety of clustering
                  evaluation metrics and achieve equivalent or better
                  performances than previously reported. Building on
                  this promising result we evaluate the output of the
                  unsupervised PoS tagger as a direct replacement for
                  the output of a fully supervised PoS tagger for the
                  task of shallow parsing and compare the two
                  evaluations.}
}

@inproceedings{VanZhu07,
  author =	 {{Van Gael}, J. and Zhu, X.},
  booktitle =	 ijcai,
  title =	 {Correlation clustering for crosslingual link
                  detection},
  url =		 {.},
  pages =	 {1744--1749},
  year =	 2007,
  month =	 {January},
  address =	 {Hyderabad, India},
  editor =	 {Manuela M. Veloso},
  abstract =	 {The crosslingual link detection problem calls for
                  identifying news articles in multiple languages that
                  report on the same news event. This paper presents a
                  novel approach based on constrained clustering.  We
                  discuss a general way for constrained clustering
                  using a recent, graph-based clustering framework
                  called correlation clustering. We introduce a
                  correlation clustering implementation that features
                  linear program chunking to allow processing larger
                  datasets. We show how to apply the correlation
                  clustering algorithm to the crosslingual link
                  detection problem and present experimental results
                  that show correlation clustering improves upon the
                  hierarchical clustering approaches commonly used in
                  link detection, and, hierarchical clustering
                  approaches that take constraints into account.}
}

@inproceedings{VerPehMau18,
  cat =		 {gm clust},
  title =	 {Sum-Product Autoencoding: Encoding and Decoding
                  Representations using Sum-Product Networks},
  author =	 {Antonio Vergari and Robert Peharz and Nicola Di
                  Mauro and Alejandro Molina and Kristian Kersting and
                  Floriana Esposito},
  abstract =	 {Abstract Sum-Product Networks (SPNs) are a deep
                  probabilistic architecture that up to now has been
                  successfully employed for tractable inference. Here,
                  we extend their scope towards unsupervised
                  representation learning: we encode samples into
                  continuous and categorical embeddings and show that
                  they can also be decoded back into the original
                  input space by leveraging MPE inference. We
                  characterize when this Sum-Product Autoencoding
                  (SPAE) leads to equivalent reconstructions and
                  extend it towards dealing with missing embedding
                  information. Our experimental results on several
                  multilabel classification problems demonstrate that
                  SPAE is competitive with state-of-the-art
                  autoencoder architectures, even if the SPNs were
                  never trained to reconstruct their inputs.},
  url =		 {.},
  booktitle =	 aaai32,
  year =	 2018,
  address =	 {New Orleans, USA},
  month =	 {February}
}

@inproceedings{VlaAndKor08a,
  title =	 {Dirichlet process mixture models for verb
                  clustering},
  author =	 {Vlachos, Andreas and Ghahramani, Zoubin and
                  Korhonen, Anna},
  booktitle =	 {Proceedings of the ICML workshop on Prior Knowledge
                  for Text and Language},
  year =	 2008,
  url =		 {.},
  abstract =	 {In this work we apply Dirichlet Process Mixture
                  Models to a learning task in natural language
                  processing (NLP): lexical-semantic verb
                  clustering. We assess the performance on a dataset
                  based on Levin’s (1993) verb classes using the
                  recently introduced V- measure metric. In, we
                  present a method to add human supervision to the
                  model in order to to influence the solution with
                  respect to some prior knowledge. The quantitative
                  evaluation performed highlights the benefits of the
                  chosen method compared to previously used clustering
                  approaches.}
}

@inproceedings{VlaAndKor09a,
  title =	 {Unsupervised and constrained {D}irichlet process
                  mixture models for verb clustering},
  author =	 {Vlachos, Andreas and Korhonen, Anna and Ghahramani,
                  Zoubin},
  booktitle =	 {Proceedings of the workshop on geometrical models of
                  natural language semantics},
  pages =	 {74--82},
  year =	 2009,
  cat =		 {nlp},
  organization = {Association for Computational Linguistics},
  abstract =	 {In this work, we apply Dirichlet Process Mixture
                  Models (DPMMs) to a learning task in natural
                  language processing (NLP): lexical-semantic verb
                  clustering. We thoroughly evaluate a method of
                  guiding DP- MMs towards a particular clustering
                  solution using pairwise constraints. The
                  quantitative and qualitative evaluation per- formed
                  highlights the benefits of both standard and
                  constrained DPMMs com- pared to previously used
                  approaches. In addition, it sheds light on the use
                  of evaluation measures and their practical
                  application.},
  url =		 {.}
}

@inproceedings{VlaGhaBri10,
  cat =		 {clust},
  author =	 {Andreas Vlachos and Zoubin Ghahramani and Ted
                  Briscoe},
  year =	 2010,
  title =	 {Active Learning for Constrained {Dirichlet} Process
                  Mixture Models},
  booktitle =	 {Proceedings of the 2010 Workshop on Geometrical
                  Models of Natural Language Semantics},
  pages =	 {57--61},
  address =	 {Uppsala, Sweden},
  url =		 {.},
  abstract =	 {Recent work applied Dirichlet Process Mixture Models
                  to the task of verb clustering, incorporating
                  supervision in the form of must-links and
                  cannot-links constraints between instances. In this
                  work, we introduce an active learning approach for
                  constraint selection employing uncertainty-based
                  sampling. We achieve substantial improvements over
                  random selection on two datasets.}
}

@inproceedings{VlaGhaKor08,
  cat =		 {clust},
  booktitle =	 {ICML Workshop on Prior Knowledge for Text and
                  Language Processing},
  title =	 {{D}irichlet process mixture models for verb
                  clustering},
  author =	 {A. Vlachos and Z. Ghahramani and A Korhonen},
  year =	 2008,
  month =	 {July},
  address =	 {Helsinki, Finland},
  pages =	 {43--48},
  editor =	 {Guillaume Bouchard and Hal {Daum\'{e} III} and Marc
                  Dymetman and Yee Whye Teh},
  url =		 {.},
  abstract =	 {In this work we apply Dirichlet Process Mixture
                  Models to a learning task in natural language
                  processing (NLP): lexical-semantic verb
                  clustering. We assess the performance on a dataset
                  based on Levin's (1993) verb classes using the
                  recently introduced V-measure metric. In, we present
                  a method to add human supervision to the model in
                  order to to influence the solution with respect to
                  some prior knowledge. The quantitative evaluation
                  performed highlights the benefits of the chosen
                  method compared to previously used clustering
                  approaches.}
}

@inproceedings{VlaKorGha09,
  cat =		 {clust np nlp},
  booktitle =	 {4th Workshop on Statistical Machine Translation,
                  EACL '09},
  title =	 {Unsupervised and constrained {Dirichlet} process
                  mixture models for verb clustering},
  author =	 {A. Vlachos and A Korhonen and Z. Ghahramani},
  year =	 2009,
  month =	 {March},
  address =	 {Athens, Greece},
  url =		 {.},
  abstract =	 {In this work, we apply Dirichlet Process Mixture
                  Models (DPMMs) to a learning task in natural
                  language processing (NLP): lexical-semantic verb
                  clustering. We thoroughly evaluate a method of
                  guiding DPMMs towards a particular clustering
                  solution using pairwise constraints. The
                  quantitative and qualitative evaluation performed
                  highlights the benefits of both standard and
                  constrained DPMMs compared to previously used
                  approaches. In addition, it sheds light on the use
                  of evaluation measures and their practical
                  application.}
}

@inproceedings{WebReyChe20,
  cat =		 {deep},
  author =	 {Andrew Webb and Charles Reynolds and Wenlin Chen and
                  Henry Reeve and Dan Iliescu and Mikel Luj{\'a}n and
                  Gavin Brown},
  booktitle =	 {European Conference on Machine Learning (ECML)},
  title =	 {To Ensemble or Not Ensemble: When Does End-to-End
                  Training Fail?},
  year =	 2020,
  url =
                  {https://link.springer.com/chapter/10.1007/978-3-030-67664-3_7},
  abstract =	 {End-to-End training (E2E) is becoming more and more
                  popular to train complex Deep Network
                  architectures. An interesting question is whether
                  this trend will continue---are there any clear
                  failure cases for E2E training? We study this
                  question in depth, for the specific case of E2E
                  training an ensemble of networks. Our strategy is to
                  blend the gradient smoothly in between two extremes:
                  from independent training of the networks, up to to
                  full E2E training.  We find clear failure cases,
                  where overparameterized models cannot be trained
                  E2E. A surprising result is that the optimum can
                  sometimes lie in between the two, neither an
                  ensemble or an E2E system. The work also uncovers
                  links to Dropout, and raises questions around the
                  nature of ensemble diversity and multi-branch
                  networks.},
  annote =	 {<a
                  href="https://arxiv.org/abs/1902.04422">arXiv</a>}
}

@inproceedings{Wel15,
  cat =		 {gm},
  title =	 {Revisiting the limits of {MAP} inference by {MWSS}
                  on perfect graphs},
  booktitle =	 aistats18,
  year =	 2015,
  month =	 {May},
  address =	 {San Diego, California},
  author =	 {Adrian Weller},
  pages =	 {1061--1069},
  abstract =	 {A recent, promising approach to identifying a
                  configuration of a discrete graphical model with
                  highest probability (termed MAP inference) is to
                  reduce the problem to finding a maximum weight
                  stable set (MWSS) in a derived weighted graph,
                  which, if perfect, allows a solution to be found in
                  polynomial time. Weller and Jebara (2013)
                  investigated the class of binary pairwise models
                  where this method may be applied. However, their
                  analysis made a seemingly innocuous assumption which
                  simplifies analysis but led to only a subset of
                  possible reparameterizations being considered. Here
                  we introduce novel techniques and consider all
                  cases, demonstrating that this greatly expands the
                  set of tractable models. We provide a simple, exact
                  characterization of the new, enlarged set and show
                  how such models may be efficiently identified, thus
                  settling the power of the approach on this class.},
  url =		 {http://jmlr.org/proceedings/papers/v38/weller15.pdf},
  annote =	 {Also accepted for presentation at the 21st
                  International Conference on Principles and Practice
                  of Constraint Programming (CP 2015)}
}

@inproceedings{Wel15b,
  cat =		 {approx gm},
  title =	 {{B}ethe and Related Pairwise Entropy Approximations},
  booktitle =	 uai31,
  year =	 2015,
  month =	 {July},
  address =	 {Amsterdam},
  author =	 {Adrian Weller},
  pages =	 {942--951},
  abstract =	 {For undirected graphical models, belief propagation
                  often performs remarkably well for approximate
                  marginal inference, and may be viewed as a heuristic
                  to minimize the Bethe free energy. Focusing on
                  binary pairwise models, we demonstrate that several
                  recent results on the Bethe approximation may be
                  generalized to a broad family of related pairwise
                  free energy approximations with arbitrary counting
                  numbers. We explore approximation error and shed
                  light on the empirical success of the Bethe
                  approximation.},
  url =		 {http://auai.org/uai2015/proceedings/papers/9.pdf},
  annote =	 {<a
                  href="http://auai.org/uai2015/proceedings/supp/9_supp.pdf">Supplementary
                  Material</a>}
}

@inproceedings{Wel16,
  cat =		 {gm},
  title =	 {Uprooting and Rerooting Graphical Models},
  booktitle =	 icml33,
  year =	 2016,
  month =	 {June},
  address =	 {New York, NY},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/Wel16_Uproot.pdf},
  author =	 {Adrian Weller},
  abstract =	 {We show how any binary pairwise model may be
                  ‘uprooted’ to a fully symmetric model, wherein
                  original singleton potentials are transformed to
                  potentials on edges to an added variable, and then
                  ‘rerooted’ to a new model on the original number of
                  variables. The new model is essentially equivalent
                  to the original model, with the same partition
                  function and allowing recovery of the original
                  marginals or a MAP configuration, yet may have very
                  different computational properties that allow much
                  more efficient inference. This meta-approach deepens
                  our understanding, may be applied to any existing
                  algorithm to yield improved methods in practice,
                  generalizes earlier theoretical results, and reveals
                  a remarkable interpretation of the
                  triplet-consistent polytope.}
}

@inproceedings{Wel16b,
  cat =		 {gm},
  title =	 {Characterizing Tightness of {LP} Relaxations by
                  Forbidding Signed Minors},
  booktitle =	 uai32,
  year =	 2016,
  month =	 {June},
  address =	 {Jersey City, NJ},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/minor_cam},
  author =	 {Adrian Weller},
  abstract =	 {We consider binary pairwise graphical models and
                  provide an exact characterization (necessary and
                  sufficient conditions observing signs of potentials)
                  of tightness for the LP relaxation on the
                  triplet-consistent polytope of the MAP inference
                  problem, by forbidding an odd-K5 (complete graph on
                  5 variables with all edges repulsive) as a signed
                  minor in the signed suspension graph. This captures
                  signs of both singleton and edge potentials in a
                  compact and efficiently testable condition, and
                  improves significantly on earlier results. We
                  provide other results on tightness of LP relaxations
                  by forbidding minors, draw connections and suggest
                  paths for future research.}
}

@inproceedings{WelDom16,
  cat =		 {gm},
  title =	 {Clamping Improves {TRW} and Mean Field
                  Approximations},
  booktitle =	 aistats19,
  year =	 2016,
  month =	 {May},
  address =	 {Cadiz, Spain},
  author =	 {Adrian Weller and Justin Domke},
  abstract =	 {We examine the effect of clamping variables for
                  approximate inference in undirected graphical models
                  with pairwise relationships and discrete
                  variables. For any number of variable labels, we
                  demonstrate that clamping and summing approximate
                  sub-partition functions can lead only to a decrease
                  in the partition function estimate for TRW, and an
                  increase for the naive mean field method, in each
                  case guaranteeing an improvement in the
                  approximation and bound. We next focus on binary
                  variables, add the Bethe approximation to
                  consideration and examine ways to choose good
                  variables to clamp, introducing new methods. We show
                  the importance of identifying highly frustrated
                  cycles, and of checking the singleton entropy of a
                  variable. We explore the value of our methods by
                  empirical analysis and draw lessons to guide
                  practitioners.},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/clamp_aistats_final.pdf}
}

@incollection{WelJeb14,
  cat =		 {approx gm},
  title =	 {Clamping Variables and Approximate Inference},
  author =	 {Weller, Adrian and Jebara, Tony},
  booktitle =	 {Advances in Neural Information Processing Systems
                  28},
  editor =	 {Z. Ghahramani and M. Welling and C. Cortes and
                  N.D. Lawrence and K.Q. Weinberger},
  pages =	 {909--917},
  year =	 2014,
  publisher =	 {Curran Associates, Inc.},
  url =
                  {http://papers.nips.cc/paper/5529-clamping-variables-and-approximate-inference.pdf},
  annote =	 {<a
                  href="http://papers.nips.cc/paper/5529-clamping-variables-and-approximate-inference-supplemental.zip">Supplementary
                  Material</a>},
  abstract =	 { It was recently proved using graph covers (Ruozzi,
                  2012) that the Bethe partition function is upper
                  bounded by the true partition function for a binary
                  pairwise model that is attractive. Here we provide a
                  new, arguably simpler proof from first
                  principles. We make use of the idea of clamping a
                  variable to a particular value. For an attractive
                  model, we show that summing over the Bethe partition
                  functions for each sub-model obtained after clamping
                  any variable can only raise (and hence improve) the
                  approximation. In fact, we derive a stronger result
                  that may have other useful implications. Repeatedly
                  clamping until we obtain a model with no cycles,
                  where the Bethe approximation is exact, yields the
                  result. We also provide a related lower bound on a
                  broad class of approximate partition functions of
                  general pairwise multi-label models that depends
                  only on the topology. We demonstrate that clamping a
                  few wisely chosen variables can be of practical
                  value by dramatically reducing approximation error.
                  }
}

@inproceedings{WelRowSon16,
  cat =		 {gm},
  title =	 {Tightness of {LP} Relaxations for Almost Balanced
                  Models},
  booktitle =	 aistats19,
  year =	 2016,
  month =	 {May},
  address =	 {Cadiz, Spain},
  url =		 {http://mlg.eng.cam.ac.uk/adrian/tricam.pdf},
  author =	 {Adrian Weller and Mark Rowland and David Sontag},
  abstract =	 {Linear programming (LP) relaxations are widely used
                  to attempt to identify a most likely configuration
                  of a discrete graphical model. In some cases, the LP
                  relaxation attains an optimum vertex at an integral
                  location and thus guarantees an exact solution to
                  the original optimization problem. When this occurs,
                  we say that the LP relaxation is tight. Here we
                  consider binary pairwise models and derive sufﬁcient
                  conditions for guaranteed tightness of (i) the
                  standard LP relaxation on the local polytope LP+LOC,
                  and (ii) the LP relaxation on the triplet-consistent
                  polytope LP+TRI (the next level in the Sherali-Adams
                  hierarchy). We provide simple new proofs of earlier
                  results and derive signiﬁcant novel results
                  including that LP+TRI is tight for any model where
                  each block is balanced or almost balanced, and a
                  decomposition theorem that may be used to break
                  apart complex models into smaller pieces.  An almost
                  balanced (sub-)model is one that contains no
                  frustrated cycles except through one privileged
                  variable.}
}

@phdthesis{Wil14,
  cat =		 {gp np time},
  author =	 {Andrew Gordon Wilson},
  title =	 {Covariance Kernels for Fast Automatic Pattern
                  Discovery and Extrapolation with {G}aussian
                  Processes},
  year =	 2014,
  url =		 {http://mlg.eng.cam.ac.uk/andrew/andrewgwthesis.pdf},
  school =	 {University of Cambridge},
  address =	 {Cambridge, UK},
  abstract =	 {Truly intelligent systems are capable of pattern
                  discovery and extrapolation without human
                  intervention.  Bayesian nonparametric models, which
                  can uniquely represent expressive prior information
                  and detailed inductive biases, provide a distinct
                  opportunity to develop intelligent systems, with
                  applications in essentially any learning and
                  prediction task.<br> Gaussian processes are rich
                  distributions over functions, which provide a
                  Bayesian nonparametric approach to smoothing and
                  interpolation.  A covariance kernel determines the
                  support and inductive biases of a Gaussian process.
                  In this thesis, we introduce new covariance kernels
                  to enable fast automatic pattern discovery and
                  extrapolation with Gaussian processes.<br> In the
                  introductory chapter, we discuss the high level
                  principles behind all of the models in this thesis:
                  1) we can typically improve the predictive
                  performance of a model by accounting for additional
                  structure in data; 2) to automatically discover rich
                  structure in data, a model must have large support
                  and the appropriate inductive biases; 3) we most
                  need expressive models for large datasets, which
                  typically provide more information for learning
                  structure, and 4) we can often exploit the existing
                  inductive biases (assumptions) or structure of a
                  model for scalable inference, without the need for
                  simplifying assumptions.<br> In the context of this
                  introduction, we then discuss, in chapter 2,
                  Gaussian processes as kernel machines, and my views
                  on the future of Gaussian process research.<br> In
                  chapter 3 we introduce the Gaussian process
                  regression network (GPRN) framework, a multi-output
                  Gaussian process method which scales to many output
                  variables, and accounts for input-dependent
                  correlations between the outputs.  Underlying the
                  GPRN is a highly expressive kernel, formed using an
                  adaptive mixture of latent basis functions in a
                  neural network like architecture.  The GPRN is
                  capable of discovering expressive structure in data.
                  We use the GPRN to model the time-varying expression
                  levels of 1000 genes, the spatially varying
                  concentrations of several distinct heavy metals, and
                  multivariate volatility (input dependent noise
                  covariances) between returns on equity indices and
                  currency exchanges, which is particularly valuable
                  for portfolio allocation.  We generalise the GPRN to
                  an adaptive network framework, which does not depend
                  on Gaussian processes or Bayesian nonparametrics;
                  and we outline applications for the adaptive network
                  in nuclear magnetic resonance (NMR) spectroscopy,
                  ensemble learning, and change-point modelling.<br>
                  In chapter 4 we introduce simple closed form kernel
                  for automatic pattern discovery and extrapolation.
                  These spectral mixture (SM) kernels are derived by
                  modelling the spectral densiy of a kernel (its
                  Fourier transform) using a scale-location Gaussian
                  mixture.  SM kernels form a basis for all stationary
                  covariances, and can be used as a drop-in
                  replacement for standard kernels, as they retain
                  simple and exact learning and inference procedures.
                  We use the SM kernel to discover patterns and
                  perform long range extrapolation on atmospheric CO2
                  trends and airline passenger data, as well as on
                  synthetic examples.  We also show that the SM kernel
                  can be used to automatically reconstruct several
                  standard covariances.  The SM kernel and the GPRN
                  are highly complementary; we show that using the SM
                  kernel with adaptive basis functions in a GPRN
                  induces an expressive prior over non-stationary
                  kernels.<br> In chapter 5 we introduce GPatt, a
                  method for fast multidimensional pattern
                  extrapolation, particularly suited to imge and movie
                  data.  Without human intervention -- no hand
                  crafting of kernel features, and no sophisticated
                  initialisation procedures -- we show that GPatt can
                  solve large scale pattern extrapolation, inpainting
                  and kernel discovery problems, including a problem
                  with 383,400 training points.  GPatt exploits the
                  structure of a spectral mixture product (SMP)
                  kernel, for fast yet exact inference procedures.  We
                  find that GPatt significantly outperforms popular
                  alternative scalable gaussian process methods in
                  speed and accuracy.  Moreover, we discover profound
                  differences between each of these methods,
                  suggesting expressive kernels, nonparametric
                  representations, and scalable inference which
                  exploits existing model structure are useful in
                  combination for modelling large scale
                  multidimensional patterns.<br> The models in this
                  dissertation have proven to be scalable and with
                  greatly enhanced predictive performance over the
                  alternatives: the extra structure being modelled is
                  an important part of a wide variety of real data --
                  including problems in econometrics, gene expression,
                  geostatistics, nuclear magnetic resonance
                  spectroscopy, ensemble learning, multi-output
                  regression, change point modelling, time series,
                  multivariate volatility, image inpainting, texture
                  extrapolation, video extrapolation, acoustic
                  modelling, and kernel discovery.}
}

@inproceedings{WilAda13,
  cat =		 {gp np time},
  author =	 {Andrew Gordon Wilson and R<!>yan Prescott Adams},
  title =	 {Gaussian Process Kernels for Pattern Discovery and
                  Extrapolation},
  booktitle =	 icml30,
  abstract =	 {Gaussian processes are rich distributions over
                  functions, which provide a Bayesian nonparametric
                  approach to smoothing and interpolation.  We
                  introduce simple closed form kernels that can be
                  used with Gaussian processes to discover patterns
                  and enable extrapolation.  These kernels are derived
                  by modelling a spectral density -- the Fourier
                  transform of a kernel -- with a Gaussian mixture.
                  The proposed kernels support a broad class of
                  stationary covariances, but Gaussian process
                  inference remains simple and analytic.  We
                  demonstrate the proposed kernels by discovering
                  patterns and performing long range extrapolation on
                  synthetic examples, as well as atmospheric CO2
                  trends and airline passenger data.  We also show
                  that we can reconstruct standard covariances within
                  our framework.},
  month =	 {February 18},
  year =	 2013,
  url =		 {http://jmlr.org/proceedings/papers/v28/wilson13.pdf},
  annote =	 {arXiv:<a
                  href="http://arxiv.org/abs/1302.4245">1302.4245</a>}
}

@inproceedings{WilGha08,
  author =	 {Sinead Williamson and Zoubin Ghahramani},
  year =	 2008,
  title =	 {Probabilistic Models for Data Combination in
                  Recommender Systems},
  booktitle =	 {Learning from Multiple Sources Workshop, NIPS
                  Conference},
  address =	 {Whistler Canada},
  url =		 {.}
}

@inproceedings{WilGha10,
  cat =		 {np gp time},
  author =	 {Andrew Gordon Wilson and Zoubin Ghahramani},
  title =	 {Copula Processes},
  booktitle =	 nips23,
  year =	 2010,
  abstract =	 {We define a copula process which describes the
                  dependencies between arbitrarily many random
                  variables independently of their marginal
                  distributions. As an example, we develop a
                  stochastic volatility model, Gaussian Copula Process
                  Volatility (GCPV), to predict the latent standard
                  deviations of a sequence of random variables. To
                  make predictions we use Bayesian inference, with the
                  Laplace approximation, and with Markov chain Monte
                  Carlo as an alternative. We find our model can
                  outperform GARCH on simulated and financial
                  data. And unlike GARCH, GCPV can easily handle
                  missing data, incorporate covariates other than
                  time, and model a rich class of covariance
                  structures.  },
  url =
                  {http://books.nips.cc/papers/files/nips23/NIPS2010_0784.pdf},
  annote =	 {<a
                  href="http://books.nips.cc/papers/files/nips23/NIPS2010_0784.extra.zip">Supplementary
                  Material</a>, <a
                  href="http://books.nips.cc/papers/files/nips23/NIPS2010_0784_slide.pdf">slides</a>.},
  note =	 {Spotlight}
}

@inproceedings{WilGha11,
  cat =		 {np gp time},
  author =	 {Andrew Gordon Wilson and Zoubin Ghahramani},
  title =	 {Generalised {W}ishart Processes},
  booktitle =	 uai27,
  year =	 2011,
  abstract =	 {We introduce a new stochastic process called the
                  generalised Wishart process (GWP). It is a
                  collection of positive semi-definite random matrices
                  indexed by any arbitrary input variable. We use this
                  process as a prior over dynamic (e.g. time varying)
                  covariance matrices. The GWP captures a diverse
                  class of covariance dynamics, naturally hanles
                  missing data, scales nicely with dimension, has
                  easily interpretable parameters, and can use input
                  variables that include covariates other than
                  time. We describe how to construct the GWP,
                  introduce general procedures for inference and
                  prediction, and show that it outperforms its main
                  competitor, multivariate GARCH, even on financial
                  data that especially suits GARCH.  },
  url =		 {http://uai.sis.pitt.edu/papers/11/p736-wilson.pdf},
  annote =	 {<a
                  href="http://mlg.eng.cam.ac.uk/andrew/gwpsupp.pdf">Supplementary
                  Material</a>, Best Student Paper Award}
}

@inproceedings{WilGha12a,
  author =	 {Wilson, Andrew Gordon and Ghahramani, Zoubin},
  booktitle =	 {ECML/PKDD},
  cat =		 {gp},
  editor =	 {Flach, Peter A. and Bie, Tijl De and Cristianini,
                  Nello},
  isbn =	 {978-3-642-33485-6},
  pages =	 {858-861},
  publisher =	 {Springer},
  series =	 {Lecture Notes in Computer Science},
  title =	 {Modelling Input Varying Correlations between
                  Multiple Responses},
  url =		 {.},
  volume =	 7524,
  year =	 2012,
  abstract =	 {We introduced a generalised Wishart process (GWP)
                  for modelling input dependent covariance matrices
                  Σ(x), allowing one to model input varying
                  correlations and uncertainties between multiple
                  response variables. The GWP can naturally scale to
                  thousands of response variables, as opposed to
                  competing multivariate volatility models which are
                  typically intractable for greater than 5 response
                  variables. The GWP can also naturally capture a rich
                  class of covariance dynamics -- periodicity,
                  Brownian motion, smoothness, {\ldots}-- through a
                  covariance kernel.}
}

@article{WilGilNehetal13,
  cat =		 {gp np time},
  author =	 {Andrew Gordon Wilson and Elad Gilboa and Arye
                  Nehorai and John P Cunningham},
  title =	 {GPatt: {F}ast Multidimensional Pattern Extrapolation
                  with {G}aussian Processes},
  abstract =	 {Gaussian processes are typically used for smoothing
                  and interpolation on small datasets.  We introduce a
                  new Bayesian nonparametric framework -- GPatt --
                  enabling automatic pattern extrapolation with
                  Gaussian processes on large multidimensional
                  datasets.  GPatt unifies and extends highly
                  expressive kernels and fast exact inference
                  techniques.  Without human intervention -- no hand
                  crafting of kernel features, and no sophisticated
                  initialisation procedures -- we show that GPatt can
                  solve large scale pattern extrapolation, inpainting,
                  and kernel discovery problems, including a problem
                  with 383,400 training points.  We find that GPatt
                  significantly outperforms popular alternative
                  scalable Gaussian process methods in speed and
                  accuracy.  Moreover, we discover profound
                  differences between each of these methods,
                  suggesting expressive kernels, nonparametric
                  representations, and scalable inference which
                  exploits model structure are useful in combination
                  for modelling large scale multidimensional
                  patterns.},
  journal =	 {arXiv preprint arXiv:1310.5288},
  year =	 2013,
  url =		 {http://arxiv.org/pdf/1310.5288v3}
}

@techreport{WilKnoGha11,
  cat =		 {np gp time deep},
  author =	 {Andrew Gordon Wilson and David A Knowles and Zoubin
                  Ghahramani},
  title =	 {Gaussian Process Regression Networks},
  number =	 {arXiv:1110.4411 [stat.ML]},
  institution =	 {Department of Engineering, University of Cambridge},
  address =	 {Cambridge, UK},
  abstract =	 {We introduce a new regression framework, Gaussian
                  process regression networks (GPRN), which combines
                  the structural properties of Bayesian neural
                  networks with the non-parametric flexibility of
                  Gaussian processes.  This model accommodates input
                  dependent signal and noise correlations between
                  multiple response variables, input dependent
                  length-scales and amplitudes, and heavy-tailed
                  predictive distributions. We derive both efficient
                  Markov chain Monte Carlo and variational Bayes
                  inference procedures for this model. We apply GPRN
                  as a multiple output regression and multivariate
                  volatility model, demonstrating substantially
                  improved performance over eight popular multiple
                  output (multi-task) Gaussian process models and
                  three multivariate volatility models on benchmark
                  datasets, including a 1000 dimensional gene
                  expression dataset. },
  month =	 {October 19},
  year =	 2011,
  url =		 {.},
  annote =	 {arXiv:<a
                  href="http://arxiv.org/abs/1110.4411">1110.4411</a>}
}

@inproceedings{WilKnoGha12,
  cat =		 {gp},
  author =	 {Andrew Gordon Wilson and David A. Knowles and Zoubin
                  Ghahramani},
  title =	 {Gaussian Process Regression Networks},
  booktitle =	 icml29,
  year =	 2012,
  month =	 {June},
  address =	 {Edinburgh, Scotland},
  url =		 {http://icml.cc/2012/papers/329.pdf},
  abstract =	 {We introduce a new regression framework, Gaussian
                  process regression networks (GPRN), which combines
                  the structural properties of Bayesian neural
                  networks with the nonparametric flexibility of
                  Gaussian processes.  GPRN accommodates input
                  (predictor) dependent signal and noise correlations
                  between multiple output (response) variables, input
                  dependent length-scales and amplitudes, and
                  heavy-tailed predictive distributions.  We derive
                  both elliptical slice sampling and variational Bayes
                  inference procedures for GPRN.  We apply GPRN as a
                  multiple output regression and multivariate
                  volatility model, demonstrating substantially
                  improved performance over eight popular multiple
                  output (multi-task) Gaussian process models and
                  three multivariate volatility models on real
                  datasets, including a 1000 dimensional gene
                  expression dataset.}
}

@inproceedings{WilOrbGha10,
  cat =		 {np},
  author =	 {Sinead Williamson and Peter Orbanz and Zoubin
                  Ghahramani},
  title =	 {Dependent {I}ndian buffet processes},
  booktitle =	 aistats13,
  year =	 2010,
  volume =	 9,
  series =	 {W \& CP},
  address =	 {Chia Laguna, Sardinia, Italy},
  month =	 {May},
  pages =	 {924--931},
  url =		 {.},
  abstract =	 {Latent variable models represent hidden structure in
                  observational data. To account for the distribution
                  of the observational data changing over time, space
                  or some other covariate, we need generalizations of
                  latent variable models that explicitly capture this
                  dependency on the covariate. A variety of such
                  generalizations has been proposed for latent
                  variable models based on the Dirichlet process. We
                  address dependency on covariates in binary latent
                  feature models, by introducing a dependent Indian
                  Buffet Process. The model generates a binary random
                  matrix with an unbounded number of columns for each
                  value of the covariate. Evolution of the binary
                  matrices over the covariate set is controlled by a
                  hierarchical Gaussian process model. The choice of
                  covariance functions controls the dependence
                  structure and exchangeability properties of the
                  model. We derive a Markov Chain Monte Carlo sampling
                  algorithm for Bayesian inference, and provide
                  experiments on both synthetic and real-world
                  data. The experimental results show that explicit
                  modeling of dependencies significantly improves
                  accuracy of predictions.}
}

@inproceedings{WilRas96,
  cat =		 {gp},
  author =	 {Chris K.~I.~Williams and Carl Edward Rasmussen},
  title =	 {Gaussian processes for regression},
  booktitle =	 nips8,
  editors =	 {D.~S.~Touretzky and M.~C.~Mozer and M.~E.~Hasselmo},
  pages =	 {514--520},
  publisher =	 mit,
  address =	 {Cambridge, MA., USA},
  abstract =	 {The Bayesian analysis of neural networks is
                  difficult because a simple prior over weights
                  implies a complex prior over functions. We
                  investigate the use of a Gaussian process prior over
                  functions, which permits the predictive Bayesian
                  analysis for fixed values of hyperparameters to be
                  carried out exactly using matrix operations. Two
                  methods, using optimization and averaging (via
                  Hybrid Monte Carlo) over hyperparameters have been
                  tested on a number of challenging problems and have
                  produced excellent results.},
  year =	 1996,
  url =		 {.}
}

@inproceedings{WilRasHen17,
  cat =		 {gp},
  author =	 {Mark van der Wilk and Carl Edward Rasmussen and
                  James Hensman},
  title =	 {Convolutional {G}aussian Processes},
  abstract =	 {We present a practical way of introducing
                  convolutional structure into Gaussian processes,
                  making them more suited to high-dimensional inputs
                  like images. The main contribution of our work is
                  the construction of an inter-domain inducing point
                  approximation that is well-tailored to the
                  convolutional kernel. This allows us to gain the
                  generalisation benefit of a convolutional kernel,
                  together with fast but accurate posterior
                  inference. We investigate several variations of the
                  convolutional kernel, and apply it to MNIST and
                  CIFAR-10, which have both been known to be
                  challenging for Gaussian processes. We also show how
                  the marginal likelihood can be used to find an
                  optimal weighting between convolutional and RBF
                  kernels to further improve performance. We hope that
                  this illustration of the usefulness of a marginal
                  likelihood will help automate discovering
                  architectures in larger models.},
  year =	 2017,
  url =
                  {https://papers.nips.cc/paper/6877-convolutional-gaussian-processes},
  annote =	 {<a
                  href="https://arxiv.org/abs/1709.01894">arXiv</a>},
  booktitle =	 nips31
}

@techreport{WilRasSchTre02,
  cat =		 {gp},
  author =	 {Christopher K.~I.~Williams and Carl Edward Rasmussen
                  and Anton Schwaighofer and Volker Tresp},
  title =	 {Observations on the {N}ystr{\"o}m Method for
                  {G}aussian Process Prediction},
  url =		 {.},
  year =	 2002,
  institution =	 {University of Edinburgh},
  abstract =	 {A number of methods for speeding up Gaussian Process
                  (GP) prediction have been proposed, including the
                  Nystr{\"o}m method of Williams and Seeger (2001). In
                  this paper we focus on two issues (1) the
                  relationship of the Nystr{\"o}m method to the Subset
                  of Regressors method (Poggio and Girosi 1990; Luo
                  and Wahba, 1997) and (2) understanding in what
                  circumstances the Nystr{\"o}m approximation would be
                  expected to provide a good approximation to exact GP
                  regression.}
}

@inproceedings{WilWanHelBle10,
  cat =		 {np},
  author =	 {Sinead Williamson and Katherine A. Heller and
                  C. Wang and D. M. Blei},
  title =	 {The {IBP} compound {D}irichlet process and its
                  application to focused topic modeling},
  booktitle =	 icml27,
  pages =	 {1151--1158},
  year =	 2010,
  month =	 {June},
  address =	 {Haifa, Israel},
  url =		 {.},
  abstract =	 {The hierarchical Dirichlet process (HDP) is a
                  Bayesian nonparametric mixed membership model ---
                  each data point is modeled with a collection of
                  components of different proportions. Though
                  powerful, the HDP makes an assumption that the
                  probability of a component being exhibited by a data
                  point is positively correlated with its proportion
                  within that data point. This might be an undesirable
                  assumption. For example, in topic modeling, a topic
                  (component) might be rare throughout the corpus but
                  dominant within those documents (data points) where
                  it occurs. We develop the IBP compound Dirichlet
                  process (ICD), a Bayesian nonparametric prior that
                  decouples across-data prevalence and within-data
                  proportion in a mixed membership model. The ICD
                  combines properties from the HDP and the Indian
                  buffet process (IBP), a Bayesian nonparametric prior
                  on binary matrices. The ICD assigns a subset of the
                  shared mixture components to each data point. This
                  subset, the data point's "focus", is determined
                  independently from the amount that each of its
                  components contribute. We develop an ICD mixture
                  model for text, the focused topic model (FTM), and
                  show superior performance over the HDP-based topic
                  model.}
}

@article{WilYuHoletal14,
  cat =		 {gp sigproc time},
  author =	 {Andrew Gordon Wilson and Yuting Wu and Daniel
                  J. Holland and S<!>ebastian Nowozin and Mick
                  D. Mantle and Lynn F. Gladden and Andrew Blake},
  title =	 {Bayesian Inference for {NMR} Spectroscopy with
                  Applications to Chemical Quantification},
  abstract =	 {Nuclear magnetic resonance (NMR) spectroscopy
                  exploits the magnetic properties of atomic nuclei to
                  discover the structure, reaction state and chemical
                  environment of molecules.  We propose a
                  probabilistic generative model and inference
                  procedures for NMR spectroscopy.  Specifically, we
                  use a weighted sum of trigonometric functions
                  undergoing exponential decay to model free induction
                  decay (FID) signals.  We discuss the challenges in
                  estimating the components of this general model --
                  amplitudes, phase shifts, frequencies, decay rates,
                  and noise variances -- and offer practical
                  solutions.  We compare with conventional Fourier
                  transform spectroscopy for estimating the relative
                  concentrations of chemicals in a mixture, using
                  synthetic and experimentally acquired FID signals.
                  We find the proposed model is particularly robust to
                  low signal to noise ratios (SNR), and overlapping
                  peaks in the Fourier transform of the FID, enabling
                  accurate predictions (e.g., 1\% error at low SNR)
                  which are not possible with conventional
                  spectroscopy (5\% error).},
  year =	 2014,
  journal =	 {arXiv preprint arXiv 1402.3580},
  url =		 {http://arxiv.org/pdf/1402.3580v2}
}

@inproceedings{WolGhaJor94a,
  author =	 {Wolpert, Daniel M. and Ghahramani, Zoubin and
                  Jordan, Michael I.},
  booktitle =	 nips7,
  cat =		 {neuro},
  editor =	 {Tesauro, Gerald and Touretzky, David S. and Leen,
                  Todd K.},
  pages =	 {43-50},
  publisher =	 {MIT Press},
  title =	 {Forward dynamic models in human motor control:
                  Psychophysical evidence},
  url =		 {.},
  year =	 1994,
  abstract =	 {Based on computational principles, with as yet no
                  direct experimental validation, it has been proposed
                  that the central nervous system (CNS) uses an
                  internal model to simulate the dynamic behavior of
                  the motor system in planning, control and learning.
                  We present experimental results and simulations
                  based on a novel approach that investigates the
                  temporal propagation of errors in the sensorimotor
                  integration process.  Our results provide direct
                  support for the existence of an internal model.}
}

@inproceedings{WooGriGha06a,
  author =	 {Wood, Frank and Griffiths, Thomas L. and Ghahramani,
                  Zoubin},
  booktitle =	 {UAI},
  cat =		 {np},
  isbn =	 {0-9749039-2-2},
  publisher =	 {AUAI Press},
  title =	 {A Non-Parametric {B}ayesian Method for Inferring
                  Hidden Causes},
  url =		 {.},
  year =	 2006,
  abstract =	 {We present a non-parametric Bayesian approach to
                  structure learning with hidden causes. Previous
                  Bayesian treatments of this problem define a prior
                  over the number of hidden causes and use algorithms
                  such as reversible jump Markov chain Monte Carlo to
                  move between solutions. In contrast, we assume that
                  the number of hidden causes is unbounded, but only a
                  finite number influence observable variables. This
                  makes it possible to use a Gibbs sampler to
                  approximate the distribution over causal
                  structures. We evaluate the performance of both
                  approaches in discovering hidden causes in simulated
                  data, and use our non-parametric approach to
                  discover hidden causes in a real medical dataset.}
}

@inproceedings{WuHerGha13,
  cat =		 {mcmc time},
  title =	 {Dynamic Covariance Models for Multivariate Financial
                  Time Series},
  author =	 {Yue Wu and Jos\'e Miguel Hern\'andez-Lobato and
                  Zoubin Ghahramani},
  booktitle =	 icml30,
  year =	 2013,
  month =	 {June},
  address =	 {Atlanta, Georgia, USA},
  url =		 {http://jmlr.org/proceedings/papers/v28/wu13.pdf},
  abstract =	 {The accurate prediction of time-changing covariances
                  is an important problem in the modeling of
                  multivariate financial data. However, some of the
                  most popular models suffer from a) overfitting
                  problems and multiple local optima, b) failure to
                  capture shifts in market conditions and c) large
                  computational costs. To address these problems we
                  introduce a novel dynamic model for time-changing
                  covariances. Over-fitting and local optima are
                  avoided by following a Bayesian approach instead of
                  computing point estimates. Changes in market
                  conditions are captured by assuming a diffusion
                  process in parameter values, and finally
                  computationally efficient and scalable inference is
                  performed using particle filters.  Experiments with
                  financial data show excellent performance of the
                  proposed method with respect to current standard
                  models.}
}

@inproceedings{XuHelGha09,
  cat =		 {np approx},
  volume =	 5,
  author =	 {Yang Xu and Katherine A. Heller and Zoubin
                  Ghahramani},
  note =	 {ISSN 1938-7228},
  booktitle =	 aistats12,
  editor =	 {D.~van Dyk and M.~Welling},
  title =	 {Tree-based inference for {D}irichlet process
                  mixtures},
  publisher =	 {Microtome Publishing (paper), Journal of Machine
                  Learning Research (online)},
  year =	 2009,
  month =	 {April},
  address =	 {Clearwater Beach, FL, USA},
  pages =	 {623--630},
  url =		 {.},
  abstract =	 {The Dirichlet process mixture (DPM) is a widely used
                  model for clustering and for general nonparametric
                  Bayesian density estimation. Unfortunately, like in
                  many statistical models, exact inference in a DPM is
                  intractable, and approximate methods are needed to
                  perform efficient inference.  While most attention
                  in the literature has been placed on Markov chain
                  Monte Carlo (MCMC) [1, 2, 3], variational Bayesian
                  (VB) [4] and collapsed variational methods [5], [6]
                  recently introduced a novel class of approximation
                  for DPMs based on Bayesian hierarchical clustering
                  (BHC). These tree-based combinatorial approximations
                  efficiently sum over exponentially many ways of
                  partitioning the data and offer a novel lower bound
                  on the marginal likelihood of the DPM [6]. In this
                  paper we make the following contributions: (1) We
                  show empirically that the BHC lower bounds are
                  substantially tighter than the bounds given by VB
                  [4] and by collapsed variational methods [5] on
                  synthetic and real datasets. (2) We also show that
                  BHC offers a more accurate predictive performance on
                  these datasets. (3) We further improve the
                  tree-based lower bounds with an algorithm that
                  efficiently sums contributions from alternative
                  trees. (4) We present a fast approximate method for
                  BHC. Our results suggest that our combinatorial
                  approximate inference methods and lower bounds may
                  be useful not only in DPMs but in other models as
                  well.}
}

@article{YuCunSanetal09a,
  cat =		 {gp},
  author =	 {B. M. Yu and J. P. Cunningham and G. Santhanam and
                  S. I. Ryu and K. V. Shenoy and M. Sahani},
  title =	 {{G}aussian-process factor analysis for
                  low-dimensional single-trial analysis of neural
                  population activity},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/YuJNP2009.pdf},
  journal =	 {Journal of Neurophysiology},
  volume =	 102,
  pages =	 {614-635},
  year =	 2009,
  abstract =	 {We consider the problem of extracting smooth,
                  low-dimensional neural trajectories that summarize
                  the activity recorded simultaneously from many
                  neurons on individual experimental trials. Beyond
                  the benefit of visualizing the high-dimensional,
                  noisy spiking activity in a compact form, such
                  trajectories can offer insight into the dynamics of
                  the neural circuitry underlying the recorded
                  activity. Current methods for extracting neural
                  trajectories involve a two-stage process: the spike
                  trains are first smoothed over time, then a static
                  dimensionality- reduction technique is applied. We
                  first describe extensions of the two-stage methods
                  that allow the degree of smoothing to be chosen in a
                  principled way and that account for spiking
                  variability, which may vary both across neurons and
                  across time. We then present a novel method for
                  extracting neural trajectories -- Gaussian-process
                  factor analysis (GPFA) -- which unifies the
                  smoothing and dimensionality- reduction operations
                  in a common probabilistic framework. We applied
                  these methods to the activity of 61 neurons recorded
                  simultaneously in macaque premotor and motor
                  cortices during reach planning and execution. By
                  adopting a goodness-of-fit metric that measures how
                  well the activity of each neuron can be predicted by
                  all other recorded neurons, we found that the
                  proposed extensions improved the predictive ability
                  of the two-stage methods. The predictive ability was
                  further improved by going to GPFA. From the
                  extracted trajectories, we directly observed a
                  convergence in neural state during motor planning,
                  an effect that was shown indirectly by previous
                  studies. We then show how such methods can be a
                  powerful tool for relating the spiking activity
                  across a neural population to the subject's behavior
                  on a single-trial basis. Finally, to assess how well
                  the proposed methods characterize neural population
                  activity when the underlying time course is known,
                  we performed simulations that revealed that GPFA
                  performed tens of percent better than the best
                  two-stage method.}
}

@inproceedings{YuCunSanetal09b,
  cat =		 {gp},
  booktitle =	 nips21,
  author =	 {B. M. Yu and J. P. Cunningham and G. Santhanam and
                  S. I. Ryu and K. V. Shenoy and M. Sahani},
  title =	 {{G}aussian-process factor analysis for
                  low-dimensional single-trial analysis of neural
                  population activity},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/YuNIPS2009.pdf},
  year =	 2009,
  address =	 {Vancouver, BC},
  month =	 {December},
  pages =	 {1--8},
  abstract =	 {We consider the problem of extracting smooth,
                  low-dimensional neural trajectories that summarize
                  the activity recorded simultaneously from many
                  neurons on individual experimental trials. Beyond
                  the benefit of visualizing the high-dimensional,
                  noisy spiking activity in a compact form, such
                  trajectories can offer insight into the dynamics of
                  the neural circuitry underlying the recorded
                  activity. Current methods for extracting neural
                  trajectories involve a two-stage process: the spike
                  trains are first smoothed over time, then a static
                  dimensionality- reduction technique is applied. We
                  first describe extensions of the two-stage methods
                  that allow the degree of smoothing to be chosen in a
                  principled way and that account for spiking
                  variability, which may vary both across neurons and
                  across time. We then present a novel method for
                  extracting neural trajectories -- Gaussian-process
                  factor analysis (GPFA) -- which unifies the
                  smoothing and dimensionality- reduction operations
                  in a common probabilistic framework. We applied
                  these methods to the activity of 61 neurons recorded
                  simultaneously in macaque premotor and motor
                  cortices during reach planning and execution. By
                  adopting a goodness-of-fit metric that measures how
                  well the activity of each neuron can be predicted by
                  all other recorded neurons, we found that the
                  proposed extensions improved the predictive ability
                  of the two-stage methods. The predictive ability was
                  further improved by going to GPFA. From the
                  extracted trajectories, we directly observed a
                  convergence in neural state during motor planning,
                  an effect that was shown indirectly by previous
                  studies. We then show how such methods can be a
                  powerful tool for relating the spiking activity
                  across a neural population to the subject's behavior
                  on a single-trial basis. Finally, to assess how well
                  the proposed methods characterize neural population
                  activity when the underlying time course is known,
                  we performed simulations that revealed that GPFA
                  performed tens of percent better than the best
                  two-stage method.}
}

@inproceedings{Zafetal17,
  cat =		 {fairness},
  title =	 {From parity to preference: Learning with
                  cost-effective notions of fairness},
  booktitle =	 nips31,
  year =	 2017,
  month =	 {December},
  address =	 {Long Beach, California},
  author =	 {M. B. Zafar and Isabel Valera and Manuel Rodriguez
                  and Krishna P. Gummadi and Adrian Weller},
  url =
                  {http://mlg.eng.cam.ac.uk/adrian/NIPS17-from-parity-to-preference.pdf},
  abstract =	 {The adoption of automated, data-driven decision
                  making in an ever expanding range of applications
                  has raised concerns about its potential unfairness
                  towards certain social groups. In this context, a
                  number of recent studies have focused on defining,
                  detecting, and removing unfairness from data-driven
                  decision systems. However, the existing notions of
                  fairness, based on parity (equality) in treatment or
                  outcomes for different social groups, tend to be
                  needlessly stringent, limiting the overall decision
                  making accuracy. In this paper, we draw inspiration
                  from the fair-division and envy-freeness literature
                  in economics and game theory and propose
                  preference-based notions of fairness —- given the
                  choice between various sets of decision treatments
                  or outcomes, any group of users would collectively
                  prefer its treatment or outcomes, regardless of the
                  (dis)parity as compared to the other groups. Then,
                  we introduce tractable proxies to design convex
                  margin-based classifiers that satisfy these
                  preference-based notions of fairness. Finally, we
                  experiment with a variety of synthetic and
                  real-world datasets and show that preference-based
                  fairness allows for greater decision accuracy than
                  parity-based fairness.}
}

@article{ZhaBatCunetal11,
  cat =		 {time},
  author =	 {M. Zhao and A. P. Batista and J. P. Cunningham and
                  C. A. Chestek and Z. Rivera-Alvidrez and R. Kalmar
                  and S. I. Ryu and K. V. Shenoy and S. Iyengar},
  title =	 {An {L}1-regularized logistic model for detecting
                  short-term neuronal interactions.},
  url =
                  {http://mlg.eng.cam.ac.uk/john/pubs/pdf/ZhaoJCNS2011.pdf},
  doi =		 {10.1007/s10827-011-0365-5},
  journal =	 {Journal of Computational Neuroscience},
  note =	 {In Press.},
  year =	 2011,
  abstract =	 {Interactions among neurons are a key com- ponent of
                  neural signal processing. Rich neural data sets
                  potentially containing evidence of interactions can
                  now be collected readily in the laboratory, but
                  existing analysis methods are often not sufficiently
                  sensitive and specific to reveal these
                  interactions. Generalized linear models offer a
                  platform for analyzing multi-electrode recordings of
                  neuronal spike train data. Here we suggest an
                  L1-regularized logistic regression model (L1L
                  method) to detect short-term (order of 3ms) neuronal
                  interactions. We estimate the parameters in this
                  model using a coordinate descent algorithm, and
                  determine the optimal tuning parameter using a
                  Bayesian Information Criterion. Simulation studies
                  show that in general the L1L method has better
                  sensitivities and specificities than those of the
                  traditional shuffle-corrected cross-correlogram
                  (covariogram) method. The L1L method is able to
                  detect excitatory interactions with both high
                  sensitivity and specificity with reasonably large
                  recordings, even when the magnitude of the
                  interactions is small; similar results hold for
                  inhibition given sufficiently high baseline firing
                  rates. Our study also suggests that the false
                  positives can be further removed by thresholding,
                  because their magnitudes are typically smaller than
                  true interactions. Simulations also show that the
                  L1L method is somewhat robust to partially observed
                  networks. We apply the method to multi-electrode
                  recordings collected in the monkey dorsal premotor
                  cortex (PMd) while the animal prepares to make
                  reaching arm movements. The results show that some
                  neurons interact differently depending on task
                  conditions. The stronger interactions detected with
                  our L1L method were also visible using the
                  covariogram method.}
}

@inproceedings{ZhaGhaYan04a,
  author =	 {Zhang, Jian and Ghahramani, Zoubin and Yang, Yiming},
  booktitle =	 {NIPS},
  cat =		 {nlp},
  editor =	 {Thrun, S<!>ebastian and Saul, Lawrence K. and
                  Sch{\"o}lkopf, Bernhard},
  isbn =	 {0-262-20152-6},
  publisher =	 {MIT Press},
  title =	 {A Probabilistic Model for Online Document Clustering
                  with Application to Novelty Detection},
  url =		 {.},
  year =	 2004,
  abstract =	 {In this paper we propose a probabilistic model for
                  online document clustering. We use non-parametric
                  Dirichlet process prior to model the growing number
                  of clusters, and use a prior of general English
                  language model as the base distribution to handle
                  the generation of novel clusters. Furthermore,
                  cluster uncertainty is modeled with a Bayesian
                  Dirichletmultinomial distribution. We use empirical
                  Bayes method to estimate hyperparameters based on a
                  historical dataset. Our probabilistic model is
                  applied to the novelty detection task in Topic
                  Detection and Tracking (TDT) and compared with
                  existing approaches in the literature.}
}

@inproceedings{ZhaGhaYan05a,
  author =	 {Zhang, Jian and Ghahramani, Zoubin and Yang, Yiming},
  booktitle =	 {NIPS},
  cat =		 {nlp},
  title =	 {Learning Multiple Related Tasks using Latent
                  Independent Component Analysis},
  url =		 {.},
  year =	 2005,
  abstract =	 {We propose a probabilistic model based on
                  Independent Component Analysis for learning multiple
                  related tasks. In our model the task parameters are
                  assumed to be generated from independent sources
                  which account for the relatedness of the tasks. We
                  use Laplace distributions to model hidden sources
                  which makes it possible to identify the hidden,
                  independent components instead of just modeling
                  correlations. Furthermore, our model enjoys a
                  sparsity property which makes it both parsimonious
                  and robust. We also propose efficient algorithms for
                  both empirical Bayes method and point
                  estimation. Our experimental results on two
                  multi-label text classification data sets show that
                  the proposed approach is promising.  }
}

@article{ZhaGhaYan08,
  cat =		 {gm},
  volume =	 73,
  number =	 3,
  month =	 {December},
  author =	 {J. Zhang and Z. Ghahramani and Y. Yang},
  title =	 {Flexible latent variable models for multi-task
                  learning},
  publisher =	 {Springer Netherlands},
  year =	 2008,
  journal =	 {Machine Learning},
  pages =	 {221--242},
  url =		 {.},
  abstract =	 {Given multiple prediction problems such as
                  regression and classification, we are interested in
                  a joint inference framework which can effectively
                  borrow information among tasks to improve the
                  prediction accuracy, especially when the number of
                  training examples per problem is small. In this
                  paper we propose a probabilistic framework which can
                  support a set of latent variable models for
                  different multi-task learning scenarios. We show
                  that the framework is a generalization of standard
                  learning methods for single prediction problems and
                  it can effectively model the shared structure among
                  different prediction tasks. Furthermore, we present
                  efficient algorithms for the empirical Bayes method
                  as well as point estimation. Our experiments on both
                  simulated datasets and real world classification
                  datasets show the effectiveness of the proposed
                  models in two evaluation settings: standard
                  multi-task learning setting and transfer learning
                  setting.}
}

@inproceedings{ZhaSutSto12a,
  author =	 {Zhang, Yichuan and Sutton, Charles A. and Storkey,
                  Amos J. and Ghahramani, Zoubin},
  booktitle =	 {NIPS},
  cat =		 {approx, mcmc},
  editor =	 {Bartlett, Peter L. and Pereira, Fernando C. N. and
                  Burges, Christopher J. C. and Bottou, L{\'e}on and
                  Weinberger, Kilian Q.},
  pages =	 {3203-3211},
  title =	 {Continuous Relaxations for Discrete {H}amiltonian
                  {M}onte {C}arlo},
  url =		 {.},
  year =	 2012,
  abstract =	 {Continuous relaxations play an important role in
                  discrete optimization, but have not seen much use in
                  approximate probabilistic inference. Here we show
                  that a general form of the Gaussian Integral Trick
                  makes it possible to transform a wide class of
                  discrete variable undirected models into fully
                  continuous systems. The continuous representation
                  allows the use of gradient-based Hamiltonian Monte
                  Carlo for inference, results in new ways of
                  estimating normalization constants (partition
                  functions), and in general opens up a number of new
                  avenues for inference in difficult discrete
                  systems. We demonstrate some of these continuous
                  relaxation inference algorithms on a number of
                  illustrative problems.}
}

@inproceedings{ZhuGhaLaf03a,
  author =	 {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty,
                  John D.},
  booktitle =	 {ICML},
  cat =		 {ssl},
  editor =	 {Fawcett, Tom and Mishra, Nina},
  isbn =	 {1-57735-189-4},
  pages =	 {912-919},
  publisher =	 {AAAI Press},
  title =	 {Semi-Supervised Learning Using {G}aussian Fields and
                  Harmonic Functions},
  url =		 {.},
  year =	 2003,
  abstract =	 {An approach to semi-supervised learning is proposed
                  that is based on a Gaussian random field
                  model. Labeled and unlabeled data are represented as
                  vertices in a weighted graph, with edge weights
                  encoding the similarity between instances. The
                  learning problem is then formulated in terms of a
                  Gaussian random field on this graph, where the mean
                  of the field is characterized in terms of harmonic
                  functions, and is efficiently obtained using matrix
                  methods or belief propagation. The resulting
                  learning algorithms have intimate connections with
                  random walks, electric networks, and spectral graph
                  theory. We discuss methods to incorporate class
                  priors and the predictions of classifiers obtained
                  by supervised learning. We also propose a method of
                  parameter learning by entropy minimization, and show
                  the algorithm's ability to perform feature
                  selection. Promising experimental results are
                  presented for synthetic data, digit classification,
                  and text classification tasks.}
}

@inproceedings{ZhuKanGha04a,
  author =	 {Zhu, Xiaojin and Kandola, Jaz S. and Ghahramani,
                  Zoubin and Lafferty, John D.},
  booktitle =	 {NIPS},
  cat =		 {ssl},
  editor =	 {Thrun, S<!>ebastian and Saul, Lawrence K. and
                  Sch{\"o}lkopf, Bernhard},
  isbn =	 {0-262-20152-6},
  publisher =	 {MIT Press},
  title =	 {Nonparametric Transforms of Graph Kernels for
                  Semi-Supervised Learning},
  url =		 {.},
  year =	 2004,
  abstract =	 {We present an algorithm based on convex optimization
                  for constructing kernels for semi-supervised
                  learning. The kernel matrices are derived from the
                  spectral decomposition of graph Laplacians, and
                  combine labeled and unlabeled data in a systematic
                  fashion. Unlike previous work using diffusion
                  kernels and Gaussian random field kernels, a
                  nonparametric kernel approach is presented that
                  incorporates order constraints during
                  optimization. This results in flexible kernels and
                  avoids the need to choose among different parametric
                  forms. Our approach relies on a quadratically
                  constrained quadratic program (QCQP), and is
                  computationally feasible for large datasets. We
                  evaluate the kernels on real datasets using support
                  vector machines, with encouraging results.  }
}

@article{ZilButWel22,
  cat =		 {fairness},
  author =	 {Zilka, Miri and Butcher, Bradley and Weller, Adrian},
  title =	 {A Survey and Datasheet Repository of Publicly
                  Available US Criminal Justice Datasets},
  journal =	 {Thirty-sixth Conference on Neural Information
                  Processing Systems Datasets and Benchmarks Track},
  year =	 2022,
  url =		 {https://openreview.net/pdf?id=FA9jVbCIgBh},
  abstract =	 {Criminal justice is an increasingly important
                  application domain for machine learning and
                  algorithmic fairness, as predictive tools are
                  becoming widely used in police, courts, and prison
                  systems worldwide. A few relevant benchmarks have
                  received significant attention, e.g., the COMPAS
                  dataset, often without proper consideration of the
                  domain context. To raise awareness of publicly
                  available criminal justice datasets and encourage
                  their responsible use, we conduct a survey, consider
                  contexts, highlight potential uses, and identify
                  gaps and limitations. We provide datasheets for 15
                  datasets and upload them to a public repository. We
                  compare the datasets across several dimensions,
                  including size, coverage of the population, and
                  potential use, highlighting concerns. We hope that
                  this work can provide a useful starting point for
                  researchers looking for appropriate datasets related
                  to criminal justice, and that the repository will
                  continue to grow as a community effort.}
}

@article{ZilSarWel22,
  cat =		 {fairness},
  author =	 {Zilka, Miri and Sargeant, Holli and Weller, Adrian},
  title =	 {Transparency, Governance and Regulation of
                  Algorithmic Tools Deployed in the Criminal Justice
                  System: A UK Case Study},
  journal =	 {Proceedings of the 2022 AAAI/ACM Conference on AI,
                  Ethics, and Society},
  year =	 2022,
  url =		 {https://doi.org/10.1145/3514094.3534200},
  abstract =	 {We present a survey of tools used in the criminal
                  justice system in the UK in three categories: data
                  infrastructure, data analysis, and risk
                  prediction. Many tools are currently in deployment,
                  offering potential benefits, including improved
                  efficiency and consistency. However, there are also
                  important concerns. Transparent information about
                  these tools, their purpose, how they are used, and
                  by whom is difficult to obtain. Even when
                  information is available, it is often insufficient
                  to enable a satisfactory evaluation. More work is
                  needed to establish governance mechanisms to ensure
                  that tools are deployed in a transparent, safe and
                  ethical way. We call for more engagement with
                  stakeholders and greater documentation of the
                  intended goal of a tool, how it will achieve this
                  goal compared to other options, and how it will be
                  monitored in deployment. We highlight additional
                  points to consider when evaluating the
                  trustworthiness of deployed tools and make concrete
                  proposals for policy.}
}

@article{brauner2020infer,
  title =	 "{Inferring the effectiveness of government
                  interventions against {COVID-19}}",
  author =	 "Brauner, Jan M and Mindermann, S{\"o}ren and Sharma,
                  Mrinank and Johnston, David and Salvatier, John and
                  Gaven{\v c}iak, Tom{\'a}{\v s} and Stephenson, Anna
                  B and Leech, Gavin and Altman, George and Mikulik,
                  Vladimir and Norman, Alexander John and Monrad,
                  Joshua Teperowski and Besiroglu, Tamay and Ge, Hong
                  and Hartwick, Meghan A and Teh, Yee Whye and
                  Chindelevitch, Leonid and Gal, Yarin and Kulveit,
                  Jan",
  journal =	 "Science",
  cat =		 {bioinf},
  url =		 {https://www.science.org/doi/10.1126/science.abd9338},
  month =	 dec,
  year =	 2020,
  language =	 "en"
}

@inproceedings{cao2022provable,
  title =	 {Provable lifelong learning of representations},
  author =	 {Cao, Xinyuan and Liu, Weiyang and Vempala, Santosh},
  booktitle =	 {International Conference on Artificial Intelligence
                  and Statistics},
  pages =	 {6334--6356},
  year =	 2022,
  organization = {PMLR}
}

@inproceedings{chen2022scalable,
  cat =		 {deep fairness approx},
  title =	 {Scalable Infomin Learning},
  author =	 {Yanzhi Chen and Weihao Sun and Yingzhen Li and
                  Adrian Weller},
  booktitle =	 {Advances in Neural Information Processing Systems},
  year =	 2022,
  url =		 {https://openreview.net/forum?id=Ojakr9ofova},
  abstract =	 {The task of infomin learning aims to learn a
                  representation with high utility while being
                  uninformative about a specified target, with the
                  latter achieved by minimising the mutual information
                  between the representation and the target. It has
                  broad applications, ranging from training fair
                  prediction models against protected attributes, to
                  unsupervised learning with disentangled
                  representations. Recent works on infomin learning
                  mainly use adversarial training, which involves
                  training a neural network to estimate mutual
                  information or its proxy and thus is slow and
                  difficult to optimise. Drawing on recent advances in
                  slicing techniques, we propose a new infomin
                  learning approach, which uses a novel proxy metric
                  to mutual information. We further derive an accurate
                  and analytically computable approximation to this
                  proxy metric, thereby removing the need of
                  constructing neural network-based mutual information
                  estimators. Compared to baselines, experiments on
                  algorithmic fairness, disentangled representation
                  learning and domain adaptation verify that our
                  method can more effectively remove unwanted
                  information with limited time budget.}
}

@inproceedings{diaz2022identifying,
  title =	 {Identifying causes of Pyrocumulonimbus (PyroCb)},
  author =	 {Diaz, Emiliano and Tazi, Kenza and Braude, Ashwin S
                  and Okoh, Daniel and Lamb, Kara and Watson-Parris,
                  Duncan and Harder, Paula and Meinert, Nis},
  booktitle =	 {NeurIPS Workshop on Causality for Real-world Impact},
  year =	 2022,
  cat =		 {interpretability},
  url =		 {https://arxiv.org/abs/2211.08883},
  abstract =	 {A first causal discovery analysis from observational
                  data of pyroCb (storm clouds generated from extreme
                  wildfires) is presented. Invariant Causal Prediction
                  was used to develop tools to understand the causal
                  drivers of pyroCb formation. This includes a
                  conditional independence test for testing Y
                  conditionally independent of E given X for binary
                  variable Y and multivariate, continuous variables X
                  and E, and a greedy-ICP search algorithm that relies
                  on fewer conditional independence tests to obtain a
                  smaller more manageable set of causal
                  predictors. With these tools, we identified a subset
                  of seven causal predictors which are plausible when
                  contrasted with domain knowledge: surface sensible
                  heat flux, relative humidity at 850 hPa, a component
                  of wind at 250 hPa, 13.3 micro-meters, thermal
                  emissions, convective available potential energy,
                  and altitude},
}

@article{ge2015dirichlet,
  title =	 {{Distributed Inference for {D}irichlet Process
                  Mixture Models}},
  author =	 {Ge, Hong and Chen, Yutian and Wan, Moquan and
                  Ghahramani, Zoubin},
  booktitle =	 {Proceedings of the 32nd International Conference on
                  Machine Learning},
  cat =		 {mcmc approx clust},
  pages =	 {2276--2284},
  year =	 2015,
  editor =	 {Bach, Francis and Blei, David},
  volume =	 37,
  series =	 {Proceedings of Machine Learning Research},
  address =	 {Lille, France},
  month =	 {07--09 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v37/gea15.pdf},
  url =		 {https://proceedings.mlr.press/v37/gea15.html},
  abstract =	 {Bayesian nonparametric mixture models based on the
                  Dirichlet process (DP) have been widely used for
                  solving problems like clustering, density estimation
                  and topic modelling. These models make weak
                  assumptions about the underlying process that
                  generated the observed data. Thus, when more data
                  are collected, the complexity of these models can
                  change accordingly. These theoretical properties
                  often lead to superior predictive performance when
                  compared to traditional finite mixture
                  models. However, despite the increasing amount of
                  data available, the application of Bayesian
                  nonparametric mixture models is so far limited to
                  relatively small data sets. In this paper, we
                  propose an efficient distributed inference algorithm
                  for the DP and the HDP mixture model. The proposed
                  method is based on a variant of the slice sampler
                  for DPs. Since this sampler does not involve a
                  pre-determined truncation, the stationary
                  distribution of the sampling algorithm is
                  unbiased. We provide both local thread-level and
                  distributed machine-level parallel implementations
                  and study the performance of this sampler through an
                  extensive set of experiments on image and text
                  data. When compared to existing inference
                  algorithms, the proposed method exhibits
                  state-of-the-art accuracy and strong scalability
                  with up to 512 cores.}
}

@article{ge2018turing,
  title =	 {Turing: A Language for Flexible Probabilistic
                  Inference},
  author =	 {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  booktitle =	 {Proceedings of the Twenty-First International
                  Conference on Artificial Intelligence and
                  Statistics},
  pages =	 {1682--1690},
  cat =		 {mcmc approx},
  year =	 2018,
  editor =	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume =	 84,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {09--11 Apr},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v84/ge18b/ge18b.pdf},
  url =		 {https://proceedings.mlr.press/v84/ge18b.html},
  abstract =	 {Probabilistic programming promises to simplify and
                  democratize probabilistic machine learning, but
                  successful probabilistic programming systems require
                  flexible, generic and efficient inference
                  engines. In this work, we present a system called
                  Turing for building MCMC algorithms for
                  probabilistic programming inference. Turing has a
                  very simple syntax and makes full use of the
                  numerical capabilities in the Julia programming
                  language, including all implemented probability
                  distributions, and automatic differentiation. Turing
                  supports a wide range of popular Monte Carlo
                  algorithms, including Hamiltonian Monte Carlo (HMC),
                  HMC with No-U-Turns (NUTS), Gibbs sampling,
                  sequential Monte Carlo (SMC), and several particle
                  MCMC (PMCMC) samplers. Most importantly, Turing
                  inference is composable: it combines MCMC operations
                  on subsets of variables, for example using a
                  combination of an HMC engine and a particle Gibbs
                  (PG) engine.  We explore several combinations of
                  inference methods with the aim of finding approaches
                  that are both efficient and universal,
                  i.e. applicable to arbitrary probabilistic
                  models. NUTS—a popular variant of HMC that adapts
                  Hamiltonian simulation path length automatically,
                  although quite powerful for exploring differentiable
                  target distributions, is however not universal. We
                  identify some failure modes for the NUTS engine, and
                  demonstrate that composition of PG (for discrete
                  variables) and NUTS (for continuous variables) can
                  be useful when the NUTS engine is either not
                  applicable, or simply does not work well. Our aim is
                  to present Turing and its composable inference
                  engines to the world and encourage other researchers
                  to build on this system to help advance the field of
                  probabilistic machine learning. }
}

@article{iorio2013drug,
  title =	 {{Transcriptional data: a new gateway to drug
                  repositioning?}},
  author =	 {Iorio, Francesco and Rittman, Timothy and Ge, Hong
                  and Menden, Michael and Saez-Rodriguez, Julio},
  journal =	 {Drug Discovery Today},
  cat =		 {review},
  volume =	 18,
  number =	 {7-8},
  pages =	 {350--357},
  month =	 apr,
  year =	 2013,
  url =		 {https://pubmed.ncbi.nlm.nih.gov/22897878/},
  language =	 {en}
}

@article{liu2021iterative,
  title =	 {Iterative teaching by label synthesis},
  author =	 {Liu, Weiyang and Liu, Zhen and Wang, Hanchen and
                  Paull, Liam and Sch{\"o}lkopf, Bernhard and Weller,
                  Adrian},
  journal =	 {Advances in Neural Information Processing Systems},
  volume =	 34,
  pages =	 {21681--21695},
  year =	 2021
}

@inproceedings{liu2021learning,
  title =	 {Learning with hyperspherical uniformity},
  author =	 {Liu, Weiyang and Lin, Rongmei and Liu, Zhen and
                  Xiong, Li and Sch{\"o}lkopf, Bernhard and Weller,
                  Adrian},
  booktitle =	 {International Conference On Artificial Intelligence
                  and Statistics},
  pages =	 {1180--1188},
  year =	 2021,
  organization = {PMLR}
}

@inproceedings{liu2021orthogonal,
  title =	 {Orthogonal over-parameterized training},
  author =	 {Liu, Weiyang and Lin, Rongmei and Liu, Zhen and
                  Rehg, James M and Paull, Liam and Xiong, Li and
                  Song, Le and Weller, Adrian},
  booktitle =	 {Proceedings of the IEEE/CVF Conference on Computer
                  Vision and Pattern Recognition},
  pages =	 {7251--7260},
  year =	 2021
}

@inproceedings{liu2022pre,
  title =	 {Pre-training Molecular Graph Representation with 3D
                  Geometry},
  author =	 {Liu, Shengchao and Wang, Hanchen and Liu, Weiyang
                  and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  booktitle =	 {International Conference on Learning
                  Representations},
  year =	 2022
}

@article{liu2022sphereface,
  title =	 {SphereFace Revived: Unifying Hyperspherical Face
                  Recognition},
  author =	 {Liu, Weiyang and Wen, Yandong and Raj, Bhiksha and
                  Singh, Rita and Weller, Adrian},
  journal =	 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =	 2022,
  publisher =	 {IEEE}
}

@inproceedings{liu2022structural,
  title =	 {Structural Causal 3D Reconstruction},
  author =	 {Liu, Weiyang and Liu, Zhen and Paull, Liam and
                  Weller, Adrian and Sch{\"o}lkopf, Bernhard},
  booktitle =	 {European Conference on Computer Vision},
  pages =	 {140--159},
  year =	 2022,
  organization = {Springer}
}

@article{nicholson2022interoperability,
  title =	 "{Interoperability of statistical models in pandemic
                  preparedness: principles and reality}",
  author =	 "Nicholson, George and Blangiardo, Marta and Briers,
                  Mark and Diggle, Peter J and Fjelde, Tor Erlend and
                  Ge, Hong and Goudie, Robert J B and Jersakova, Radka
                  and King, Ruairidh E and Lehmann, Brieuc C L and
                  Mallon, Ann-Marie and Padellini, Tullia and Teh, Yee
                  Whye and Holmes, Chris and Richardson, Sylvia",
  journal =	 "Stat. Sci.",
  cat =		 {mcmc review bioinf},
  volume =	 37,
  number =	 2,
  pages =	 "183--206",
  month =	 may,
  year =	 2022,
  language =	 "en",
  url =		 {https://pubmed.ncbi.nlm.nih.gov/35664221/},
  abstract =	 {We present interoperability as a guiding framework
                  for statistical modelling to assist policy makers
                  asking multiple questions using diverse datasets in
                  the face of an evolving pandemic
                  response. Interoperability provides an important set
                  of principles for future pandemic preparedness,
                  through the joint design and deployment of adaptable
                  systems of statistical models for disease
                  surveillance using probabilistic reasoning. We
                  illustrate this through case studies for inferring
                  and characterising spatial-temporal prevalence and
                  reproduction numbers of SARS-CoV-2 infections in
                  England.}
}

@inproceedings{tazi2022pyrocast,
  title =	 {Pyrocast: a machine learning pipeline to forecast
                  pyrocumulonimbus (pyrocb) clouds},
  author =	 {Tazi, Kenza and D{\'\i}az Salas-Porras, Emiliano and
                  Braude, Ashwin and Okoh, Daniel and Lamb, Kara D and
                  Watson-Parris, Duncan and Harder, Paula and Meinert,
                  Nis},
  journal =	 {NeurIPS Workshop on Tackling Climate Change with
                  Machine Learning},
  year =	 2022,
  url =		 {https://arxiv.org/abs/2211.13052},
  cat =		 {interpretability},
  abstract =	 {Pyrocumulonimbus (pyroCb) clouds are storm clouds
                  generated by extreme wildfires.  PyroCbs are
                  associated with unpredictable, and therefore
                  dangerous, wildfire spread.  They can also inject
                  smoke particles and trace gases into the upper
                  troposphere and lower stratosphere, affecting the
                  Earth's climate. As global temperatures increase,
                  these previously rare events are becoming more
                  common. Being able to predict which fires are likely
                  to generate pyroCb is therefore key to climate
                  adaptation in wildfire-prone areas.  This paper
                  introduces Pyrocast, a pipeline for pyroCb analysis
                  and forecasting. The pipeline's first two
                  components, a pyroCb database and a pyroCb forecast
                  model, are presented. The database brings together
                  geostationary imagery and environmental data for
                  over 148 pyroCb events across North America,
                  Australia, and Russia between 2018 and 2022.  Random
                  Forests, Convolutional Neural Networks (CNNs), and
                  CNNs pretrained with Auto-Encoders were tested to
                  predict the generation of pyroCb for a given fire
                  six hours in advance.  The best model predicted
                  pyroCb with an AUC of 0.90±0.04.},
}

@inproceedings{wen2021self,
  title =	 {Self-supervised 3d face reconstruction via
                  conditional estimation},
  author =	 {Wen, Yandong and Liu, Weiyang and Raj, Bhiksha and
                  Singh, Rita},
  booktitle =	 {Proceedings of the IEEE/CVF International Conference
                  on Computer Vision},
  pages =	 {13289--13298},
  year =	 2021
}

@inproceedings{wen2022sphereface2,
  title =	 {SphereFace2: Binary Classification is All You Need
                  for Deep Face Recognition},
  author =	 {Wen, Yandong and Liu, Weiyang and Weller, Adrian and
                  Raj, Bhiksha and Singh, Rita},
  booktitle =	 {International Conference on Learning
                  Representations},
  year =	 2022
}

@article{xu2021hmc,
  title =	 {Couplings for Multinomial {H}amiltonian {M}onte
                  {C}arlo},
  author =	 {Xu, Kai and Fjelde, Tor Erlend and Sutton, Charles
                  and Ge, Hong},
  cat =		 {mcmc approx},
  booktitle =	 {Proceedings of The 24th International Conference on
                  Artificial Intelligence and Statistics},
  pages =	 {3646--3654},
  year =	 2021,
  editor =	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume =	 130,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--15 Apr},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v130/xu21i/xu21i.pdf},
  url =		 {https://proceedings.mlr.press/v130/xu21i.html},
  abstract =	 { Hamiltonian Monte Carlo (HMC) is a popular sampling
                  method in Bayesian inference. Recently, Heng &amp;
                  Jacob (2019) studied Metropolis HMC with couplings
                  for unbiased Monte Carlo estimation, establishing a
                  generic parallelizable scheme for HMC. However, in
                  practice a different HMC method, multinomial HMC, is
                  considered as the go-to method, e.g. as part of the
                  no-U-turn sampler. In multinomial HMC, proposed
                  states are not limited to end-points as in
                  Metropolis HMC; instead points along the entire
                  trajectory can be proposed. In this paper, we
                  establish couplings for multinomial HMC, based on
                  optimal transport for multinomial sampling in its
                  transition. We prove an upper bound for the meeting
                  time – the time it takes for the coupled chains to
                  meet – based on the notion of local
                  contractivity. We evaluate our methods using three
                  targets: 1,000 dimensional Gaussians, logistic
                  regression and log-Gaussian Cox point
                  processes. Compared to Heng &amp; Jacob (2019),
                  coupled multinomial HMC generally attains a smaller
                  meeting time, and is more robust to choices of step
                  sizes and trajectory lengths, which allows re-use of
                  existing adaptation methods for HMC. These
                  improvements together paves the way for a wider and
                  more practical use of coupled HMC methods. }
}

@article{xu2021locality,
  title =	 {Locality sensitive teaching},
  author =	 {Xu, Zhaozhuo and Chen, Beidi and Li, Chaojian and
                  Liu, Weiyang and Song, Le and Lin, Yingyan and
                  Shrivastava, Anshumali},
  journal =	 {Advances in Neural Information Processing Systems},
  volume =	 34,
  pages =	 {18049--18062},
  year =	 2021
}

@inproceedings{zhang2022towards,
  title =	 {Towards principled disentanglement for domain
                  generalization},
  author =	 {Zhang, Hanlin and Zhang, Yi-Fan and Liu, Weiyang and
                  Weller, Adrian and Sch{\"o}lkopf, Bernhard and Xing,
                  Eric P},
  booktitle =	 {Proceedings of the IEEE/CVF Conference on Computer
                  Vision and Pattern Recognition},
  pages =	 {8024--8034},
  year =	 2022
}
